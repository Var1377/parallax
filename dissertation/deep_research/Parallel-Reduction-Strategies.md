Comparative Performance Analysis of Runtime Strategies for Parallel Interaction Network Reduction
1. Introduction
1.1 Purpose
This report provides a comparative performance analysis of two distinct runtime implementation strategies proposed for parallel interaction network reduction. The objective is to evaluate the likely performance characteristics and scalability limitations of each approach—one based on a global node buffer with atomic locking, and the other utilizing thread-owned graph partitions with ownership transfers—across different hardware architectures. This analysis draws upon high-performance computing principles and relevant research to inform the implementation decision.
1.2 Problem Context
Interaction networks, often represented as graphs, require reduction operations that involve modifying node states based on interactions with neighbors. Efficiently parallelizing these reductions on modern multi-core and many-core processors presents significant challenges. Key difficulties include managing concurrent access to shared node data, ensuring data locality to leverage complex memory hierarchies (including caches and Non-Uniform Memory Access (NUMA) architectures), and dynamically balancing the computational load across available processing units.1 The choice of runtime strategy for synchronization, data management, and load balancing critically impacts overall performance and scalability.
1.3 Options Overview
Two primary implementation options are considered:
* Option 1 (Global Atomics): This approach utilizes a single, globally accessible buffer containing all network nodes. Concurrency control for accessing and modifying nodes is managed using atomic operations, specifically atomic swaps, acting as fine-grained locks on individual nodes. Load balancing is achieved through per-thread task queues employing a work-stealing mechanism, where idle threads pull tasks (nodes to process) from busy threads.3
* Option 2 (Partition Ownership): This strategy partitions the interaction network graph, assigning distinct partitions (subgraphs) to specific threads. Each thread has uncontested access to nodes within its owned partition, eliminating the need for atomic operations during local reductions. Interactions involving nodes in different partitions (cross-partition reductions) are handled via explicit ownership transfers, effectively passing control or data between threads. Load balancing can be implemented by dynamically transferring ownership of entire partitions or by splitting partitions and transferring subsections.5
1.4 Roadmap
This report proceeds as follows: Section 2 provides a comparative analysis of the core mechanisms underlying each option, focusing on scalability limitations of synchronization primitives versus communication, performance strategies (fine-grained locking vs. partitioning), memory system impacts (cache coherence and data locality), and load balancing techniques. Section 3 evaluates the likely performance of each option on specific hardware architectures, including multi-core x86 (with NUMA considerations), high-core-count ARM, and GPUs. Section 4 synthesizes these findings, presents a comparative summary, and offers recommendations regarding the most suitable approach under different conditions.
2. Comparative Analysis: Core Mechanisms
2.1 Scalability: Atomic Operations vs. Partition Communication
The fundamental difference in scalability between the two options lies in how they handle concurrent access and interaction: Option 1 relies on hardware atomic operations for synchronization on shared data, while Option 2 replaces local synchronization with explicit communication for non-local interactions.
2.1.1 Atomic Operations (Option 1)
* Fundamental Behavior: Atomic operations, such as compare-and-swap (CAS) or atomic swap (used in Option 1 for node locking), provide a hardware guarantee that a read-modify-write sequence on a memory location executes indivisibly with respect to other threads.8 They are often initially favored over traditional mutexes or locks in parallel runtimes because they can avoid blocking threads and potentially offer lower overhead under low contention.3
* Uncontended Performance: When threads rarely attempt to access the same atomic variable (i.e., lock the same node) simultaneously, the performance cost is relatively low and constant. Measurements show uncontended atomic increment latencies around 5 ns on AMD EPYC Rome (x86) and between 20-38 ns on IBM Power9.3 While faster than contended operations, this is still demonstrably slower than non-atomic memory accesses.3 This uncontended latency represents the baseline overhead for Option 1 when interactions are sparse or well-distributed.
* Contended Performance & Scalability Limits: The performance of atomic operations degrades drastically under high contention. When multiple threads repeatedly attempt to perform atomic operations on the same shared variable (a single node lock in Option 1), the hardware effectively serializes these accesses.3 Even though modern CPUs might lock individual cache lines, the atomic operation itself must occur sequentially for each contending thread. This serialization dramatically increases the average time per operation. Measurements show increases to approximately 530 ns per operation with 64 threads contending on AMD EPYC, and 1200 ns with only 22 threads contending on Power9.3 This serialization becomes a critical performance bottleneck, fundamentally limiting the application's ability to scale with increasing core counts.3 This bottleneck is not unique to CPUs; GPUs implementing MapReduce frameworks also face severe contention issues when using global atomic operations for result collection 10, and high contention on specific memory locations (global or shared) significantly impacts GPU atomic performance.11 Therefore, Option 1's scalability is inherently capped by the degree of contention on its shared node locks.
* Architectural Variations: The exact performance characteristics and scaling limits of atomic operations vary significantly across different processor architectures and even generations within the same architecture.3 For instance, the observed contended latency differs substantially between AMD EPYC and IBM Power9.3 ARM architectures (v8.1-A onwards) introduce the concept of "near" versus "far" atomics.9 Near atomics execute within the local CPU core/cluster's cache hierarchy, while far atomics are processed by the coherent interconnect logic. Far atomics can suffer significant performance degradation in large, high-core-count systems due to latency and contention in the interconnect.9 Consequently, system firmware (like Arm Trusted Firmware-A) often defaults to forcing cacheable atomics to execute as "near" atomics by bringing the cache line exclusively into the local cache first.9 This highlights that the underlying hardware implementation and system configuration heavily influence how atomic contention manifests as a bottleneck.
* Implications for Option 1: The scalability of Option 1 is directly dependent on the rate of contention for node locks. If the interaction network reduction process frequently involves multiple threads attempting to modify the same node concurrently, Option 1 will likely encounter severe performance limitations, particularly on systems with many cores. The specific hardware platform (e.g., x86 generation, ARM Neoverse variant) will determine the precise core count and contention level at which this bottleneck becomes critical.3
2.1.2 Partition Communication (Option 2)
* Mechanism: Option 2 avoids atomic contention for local reductions by design, as each thread has exclusive access to its partition. The scalability challenge shifts to the cost of handling cross-partition interactions, which necessitate explicit communication, specifically ownership transfers in this proposal. The overhead associated with this communication becomes the primary factor limiting scalability for non-local interactions.
* Communication Costs: The cost of partition communication (ownership transfers) is influenced by several factors. Latency and bandwidth of the underlying communication fabric are critical; this includes the on-chip interconnect, memory bus, and potentially network links in distributed settings or across NUMA nodes.5 Intra-node communication (e.g., within the same CPU socket or NUMA node) is generally significantly faster than inter-node communication.5 The volume of data transferred per interaction (e.g., the size of the node state being transferred or potentially the size of a partition being migrated for load balancing) also contributes to overhead.5 Furthermore, research on distributed graph processing highlights that the communication pattern can be as important, or even more important, than the raw communication volume, especially at scale.6 Frequent, small messages or complex all-to-all patterns can incur higher overhead than structured, bulk transfers, even if the total data volume is similar.
* Partitioning Impact: The quality of the initial graph partitioning strategy is paramount for Option 2's performance.5 A good partitioning algorithm aims to minimize the number of edges crossing partition boundaries (edge-cut), thereby reducing the frequency of required inter-partition communication.5 Simultaneously, it must balance the computational load across partitions to ensure efficient parallel execution.6 Poor partitioning leads to increased communication overhead and potential load imbalance.5 Different strategies exist, such as edge-cut (partitioning vertices) and vertex-cut (partitioning edges), each with trade-offs regarding replication factor and communication patterns.15 Advanced strategies like Cartesian Vertex-Cut (CVC) demonstrate that optimizing the communication pattern can yield better scalability than simply minimizing communication volume.6 Hybrid approaches, like PowerLyra's hybrid-cut, adapt the strategy based on vertex degree in skewed graphs to optimize both load balance and communication.15
* Implications for Option 2: Option 2 fundamentally trades the potential bottleneck of atomic contention for the overhead associated with partition management and inter-partition communication. Its ability to scale hinges on the effectiveness of the partitioning strategy in minimizing cross-partition interactions and the efficiency of the hardware's communication pathways. If the cost of serialized atomic operations under high contention (Option 1) exceeds the cost of managing partitions and executing ownership transfers (Option 2), then Option 2 offers a more scalable path forward.
2.1.3 Scalability Trade-offs
The choice between these two approaches pivots on the interplay between contention levels, core counts, and the relative costs of atomic operations versus communication on the target hardware. In scenarios with low core counts or workloads exhibiting low contention for shared nodes, the simplicity and potentially lower baseline overhead of uncontended atomic operations might make Option 1 preferable. However, as core counts increase and/or contention rises, the cost of serialized atomics escalates rapidly.3 At some point, this cost will likely surpass the overhead incurred by Option 2's partition management and communication mechanisms. Option 2 becomes more advantageous when the workload demands high interaction rates that would saturate the atomic locking mechanism of Option 1, provided that the partitioning scheme effectively localizes most work and the communication fabric can handle the required ownership transfers efficiently.5
Furthermore, the performance characteristics suggest that Option 1, relying heavily on atomic operations, is likely more sensitive to the intricate details of a specific CPU's microarchitecture and cache coherence protocol implementation, especially under contention. The significant performance differences observed between AMD and Power9 for contended atomics 3, and the near/far atomic distinction on ARM 9, underscore this sensitivity. Option 2, while still sensitive to communication bandwidth and latency, relies on a more architecturally consistent mechanism (data movement). While the speed of data movement varies, the fundamental process is less subject to the kind of implementation-specific performance cliffs seen with highly contended atomics across diverse CPU designs.
2.2 Performance Strategies: Fine-Grained Locking vs. Graph Partitioning
The two options represent different high-level strategies for managing parallel execution in graph processing: Option 1 implicitly uses fine-grained locking via atomics, while Option 2 explicitly employs graph partitioning.
2.2.1 Fine-Grained Locking (Implicit in Option 1)
* Concept: Option 1's use of atomic swaps to protect individual nodes constitutes a fine-grained locking strategy. The intent behind fine-grained locking is generally to allow more parallelism than coarse-grained approaches (e.g., locking the entire graph or large sections), as only the specific data being accessed is locked.17 Synchronization coherence mechanisms have been proposed to integrate fine-grained synchronization directly with cache coherence protocols.17
* Performance: While potentially unlocking more concurrency than coarse locks, fine-grained locking implemented with atomics, as in Option 1, still faces the severe scalability limitations under contention discussed previously.3 Its effectiveness hinges on minimizing the probability of multiple threads attempting to lock the same node simultaneously.18 Systems like LOCKHASH, which use fine-grained locks (one per hash table partition), exemplify this approach and its performance characteristics.19
* Overhead: Beyond the serialization cost under contention, atomic operations introduce overhead even when uncontended, being slower than standard memory operations.3 They also inherently increase cache coherence traffic. When a thread acquires a "lock" (the cache line containing the node state and its lock status) via an atomic operation, that cache line must often be moved from another core's cache, incurring latency and consuming interconnect bandwidth. This movement is necessary for every lock acquisition if different threads access the same node.2
2.2.2 Graph Partitioning (Explicit in Option 2)
* Concept: Option 2 explicitly partitions the graph, assigning subgraphs (partitions) to specific threads. This strategy is widely adopted in parallel and distributed graph processing systems.5 The primary goal is to localize computation and data access, thereby eliminating the need for fine-grained locking or atomics for operations within a partition.20
* Performance Factors: The performance of a partitioning-based approach like Option 2 depends critically on several factors:
   * Partition Quality: As mentioned, achieving both computational load balance and minimizing inter-partition communication (edge cuts) is crucial.5 This is particularly challenging for real-world graphs which often exhibit skewed degree distributions (power-law graphs).15 Strategies like vertex-cut or hybrid-cut are specifically designed to handle such skew by partitioning edges associated with high-degree vertices, rather than just the vertices themselves.15 NUMA-aware partitioning also explicitly considers load balance across NUMA nodes.20
   * Communication Efficiency: The cost associated with handling interactions that cross partition boundaries (ownership transfers in Option 2) must be low.6 This depends on the implementation of the transfer mechanism and the underlying hardware communication performance.
   * Dynamic Rebalancing: If the workload changes significantly over time, dynamic rebalancing might be necessary. This involves migrating vertices or transferring/splitting partitions, which introduces additional overhead.5 Lightweight repartitioning algorithms aim to minimize this cost.5
* Comparisons: Direct comparisons between fine-grained locking and partitioning with message passing (conceptually similar to Option 2's ownership transfer) exist. For instance, the CPHASH study compared a hash table using partitioning and message passing (CPHASH) against one using fine-grained locking (LOCKHASH).19 CPHASH demonstrated significantly higher throughput (1.6x) on an 80-core machine, primarily because it experienced fewer expensive L3 cache misses and reduced contention on the interconnect and DRAM, despite the overhead of message passing. This suggests that the locality benefits of partitioning can outweigh the costs of explicit communication compared to the contention and coherence costs of fine-grained locking on many-core systems.19 Graph partitioning inherently aims to improve data locality, a known challenge in irregular graph problems.1
2.2.3 Impact of Graph Structure
The structure and properties of the interaction network graph significantly influence the relative performance of the two options:
* Dense Graphs: Dense graphs imply a high number of interactions relative to the number of nodes. This could lead to high contention rates in Option 1, quickly hitting the atomic serialization bottleneck. For Option 2, partitioning dense graphs effectively to minimize edge cuts can be difficult, potentially leading to high communication overhead due to frequent cross-partition interactions.
* Sparse Graphs: Sparse graphs are common in many real-world applications. Option 1 might perform adequately if the interactions remain localized or contention is naturally low. However, sparse graphs often exhibit irregular access patterns, leading to poor locality.1 Option 2 generally handles sparse graphs well, as partitioning algorithms are often effective. However, the inherent irregularity can still lead to poor cache utilization if not managed carefully.1
* Skewed Graphs (Power-Law): These graphs, characterized by a few nodes having extremely high degrees (connections), pose a significant challenge. In Option 1, these high-degree nodes become immediate and severe contention hotspots, as many threads will likely attempt to interact with them concurrently.15 Option 2 requires specialized partitioning strategies to handle skew effectively. Simple vertex partitioning (edge-cut) can lead to severe load imbalance.15 Vertex-cut or hybrid-cut approaches, which distribute the edges of high-degree vertices, are necessary to balance the computational load and manage communication.15 NUMA-aware systems like Polymer also incorporate edge-oriented partitioning for skewed graphs.20
2.2.4 Locality, Contention, and Complexity Considerations
The two options represent a fundamental trade-off. Option 1 offers simpler global access to any node, potentially simplifying the programming model initially. However, this comes at the cost of potentially poor data locality (leading to cache misses and NUMA penalties) and vulnerability to atomic contention bottlenecks.3 Option 2 explicitly prioritizes data locality by assigning partitions to threads, which eliminates local contention and improves cache behavior.19 The trade-off is the introduction of explicit communication overhead (ownership transfers) for non-local interactions and increased implementation complexity.6 The optimal choice depends on the balance between local and non-local interactions within the specific workload and the relative costs of atomic contention versus explicit communication on the target platform.
Regarding implementation, Option 1 might seem simpler at first glance, leveraging standard primitives like atomic operations and work-stealing queues.3 However, diagnosing and mitigating performance issues arising from high contention can become complex and require deep system knowledge.3 Option 2 necessitates a more complex initial design, involving the implementation of partitioning logic, an ownership transfer protocol, and a partition-based load balancing mechanism.5 While more involved upfront, this explicit management provides more direct control levers for performance tuning, such as selecting appropriate partitioning strategies or optimizing communication patterns.6 This potentially leads to a more predictable, albeit complex, path towards achieving scalability.
2.3 Memory System Impact: Cache Coherence and Data Locality
The way each option interacts with the memory hierarchy—caches, coherence protocols, and main memory (potentially NUMA)—is a critical differentiator.
2.3.1 Option 1 (Global Buffer & Atomics)
* Cache Coherence: This approach is highly susceptible to generating significant cache coherence traffic. When multiple threads contend for the same node lock (atomic swap), the cache line holding that node's state must be transferred between the caches of the contending cores.2 This movement, often referred to as cache line "ping-ponging," incurs latency, consumes interconnect bandwidth, and directly contributes to the serialization observed under high atomic contention.19 Even accesses to different nodes can cause coherence traffic if those nodes happen to reside on the same cache line (false sharing), although this is less likely with node-level granularity compared to finer-grained data. The LOCKHASH example illustrates how lock acquisitions (even fine-grained ones) necessitate cache line movement.19 Cache coherence protocols themselves add overhead even for read-mostly workloads.2
* Data Locality: Option 1 generally exhibits poor data locality, both temporal (reuse of data over time) and spatial (use of nearby data in memory). Threads fetch tasks (nodes) from work-stealing queues, leading to potentially random accesses across the entire global node buffer.1 There is little guarantee that a thread will process the same node or adjacent nodes in consecutive operations. This irregular access pattern results in frequent cache misses at all levels (L1, L2, L3), forcing reliance on slower main memory.19 This problem is severely exacerbated in NUMA systems. Accessing data residing in memory attached to a remote CPU socket incurs significantly higher latency and lower bandwidth compared to accessing local memory.20 NUMA-oblivious approaches, which treat memory as uniform, perform poorly in graph analytics due to these costly remote accesses.20 Option 1's global buffer model, without specific NUMA-aware placement and access strategies, falls into this category.
2.3.2 Option 2 (Partitioned Data & Local Access)
* Cache Coherence: Option 2 significantly reduces cache coherence traffic for local operations. Since a thread operates primarily on its owned partition, the data associated with that partition ideally remains resident in the thread's local cache hierarchy (L1/L2).19 Coherence traffic is primarily generated during the explicit communication phases—when ownership of a node or partition is transferred between threads/cores. While coherence protocols still manage the consistency of this shared data during transfer, the traffic is localized to these specific events rather than being potentially continuous due to lock contention or data bouncing between cores.19
* Data Locality: This approach offers substantially better potential for data locality. By design, threads work predominantly on their assigned partition, promoting both temporal locality (repeated access to nodes within the partition) and spatial locality (accessing potentially contiguous data within the partition's memory layout).19 Crucially, Option 2 naturally lends itself to NUMA-aware implementations. Partitions can be allocated in the memory local to the NUMA node where the owning thread is running.20 This co-location of computation and data minimizes expensive cross-socket memory accesses.20 Systems like Polymer explicitly implement this NUMA-aware partitioning and data placement.20 Similarly, CPHASH demonstrates partitioning data across the L1/L2 caches of specific cores to maximize locality.19
* Transfer Costs: The primary memory system cost in Option 2 shifts from managing implicit coherence traffic under contention to managing the explicit data movement during ownership transfers.6 While these transfers incur latency and consume bandwidth, the costs are generally more predictable and controllable compared to the potentially chaotic effects of high contention and cache thrashing in Option 1. Optimizing the transfer mechanism (e.g., bulk transfers, asynchronous operations) and communication patterns becomes a key performance consideration.6
2.3.3 Memory System Implications
NUMA architectures amplify the performance differences stemming from locality. Option 1's reliance on global, potentially random access patterns is fundamentally at odds with the NUMA philosophy of localized access. It is likely to suffer significant performance degradation due to frequent, high-latency remote memory accesses.20 Conversely, Option 2's inherent focus on data locality aligns perfectly with NUMA principles. By enabling explicit data placement (allocating partitions to local NUMA nodes), Option 2 provides a mechanism to minimize costly remote accesses and leverage the memory hierarchy more effectively.20 Research consistently shows that NUMA-aware graph processing systems significantly outperform NUMA-oblivious ones.20
Consequently, Option 2 offers better potential to utilize the entire cache hierarchy. Local partitions are more likely to remain "hot" in the L1 and L2 caches of the owning core due to repeated local access. Option 1's global access pattern is more prone to cache thrashing, where useful data is constantly evicted from upper cache levels due to accesses to unrelated parts of the global buffer, leading to higher reliance on the slower L3 cache and main memory.1
2.4 Load Balancing: Work Stealing vs. Partition Transfer
Effective load balancing is crucial for maintaining high utilization of parallel resources, especially in graph algorithms where work distribution can be irregular or change dynamically.
2.4.1 Task-Based Work Stealing (Option 1)
* Mechanism: This is a widely used dynamic load balancing technique.26 Each thread maintains a collection of tasks (e.g., nodes needing processing in Option 1), typically in a double-ended queue (deque). Threads primarily work on their own tasks. When a thread runs out of work (becomes idle), it acts as a "thief" and attempts to "steal" a task from the queue of another, randomly chosen "victim" thread.26 Lock-free deques are often used to minimize synchronization overhead during stealing.26
* Pros: Work stealing has proven effective and scalable for a variety of parallel applications, particularly those generating tasks dynamically or having unpredictable execution times.4 It possesses good theoretical properties, such as near-optimal load balancing for certain classes of computations (e.g., fully strict, fork-join) under specific assumptions.7 It is relatively straightforward to implement using standard concurrent queue structures.
* Cons in Graph Context:
   * Granularity Sensitivity: The performance of work stealing is highly sensitive to task granularity. If the amount of computation per task is very small (fine-grained), the overhead associated with the stealing mechanism itself (checking queues, communication for stealing, synchronization) can dominate the useful work, leading to poor performance and scalability.7 Interaction network reduction might involve such fine-grained operations on nodes.
   * Locality Obliviousness: Standard work stealing is generally locality-unaware. A thief steals a task without regard to where the data associated with that task resides. This can lead to stolen tasks requiring access to data that is not in the thief's local cache or NUMA node, incurring significant access latency penalties.26
   * Contention: While lock-free deques mitigate some issues, high rates of stealing attempts, especially with many idle threads, can still lead to contention on the underlying atomic operations used by the queues.7
   * Communication Overhead: The act of stealing involves communication between thief and victim (request/response), which introduces latency, particularly in distributed memory or large NUMA systems.26
2.4.2 Partition Transfer/Splitting (Option 2)
* Mechanism: In this approach, load balancing is achieved by transferring larger units of work—entire graph partitions or sub-partitions resulting from splitting—from overloaded threads to underloaded ones. This involves transferring not just the computational task but also the ownership and potentially the associated data.
* Pros:
   * Locality Preservation: A key potential advantage is improved data locality compared to task stealing. Since the data (the partition) is transferred along with the work, the receiving thread might achieve better performance by having the necessary data readily available, potentially already in its cache or local NUMA node.27
   * Coarser Granularity: Balancing operations occur at the level of partitions, which are typically much larger than individual tasks/nodes. This can reduce the frequency of load balancing interventions and associated overhead compared to fine-grained task stealing.
   * Topology Awareness: If the initial partitioning is topology-aware 5, transferring partitions might help maintain some of that structural locality, although this is not guaranteed.
* Cons:
   * Transfer Overhead: Moving entire partitions can be costly, involving potentially large data transfers, especially if partitions are substantial.5 The overhead of serialization/deserialization and communication can be significant.26 Efficient mechanisms for splitting partitions are necessary if transferring whole partitions leads to excessive overhead or overshoots the balancing target.
   * Complexity: Implementing partition transfer logic is considerably more complex than standard task stealing. It requires mechanisms to decide when to balance, which partition (or part) to transfer, to whom to transfer it, and how to manage the actual data movement and ownership update efficiently.7 Finding effective heuristics to trigger rebalancing can be difficult.7
   * Potential Imbalance/Oscillation: Transferring large, indivisible partitions might not achieve fine-grained balance or could even create new imbalances. Splitting partitions adds further complexity to the management logic.
2.4.3 Hybrid Approaches and Performance Comparisons
Some systems attempt to combine the benefits of both approaches. For example, work stealing might be used within a partition that is assigned to multiple threads, or entire partitions themselves might be stealable units.27 The Chaos system uses simple streaming partitions but allows multiple machines to work on the same partition via work stealing.30
Direct comparisons indicate that while work stealing generally performs well, its performance degrades sharply for very fine-grained tasks where scheduling overhead becomes dominant.7 In these specific scenarios, graph partitioning can sometimes match or outperform work stealing by reducing inter-processor communication.7 However, partitioning-based balancing can suffer from high variability and unpredictable performance, and the cost of repartitioning or migration can be substantial.7 For iterative applications with predictable workload changes, persistence-based load balancing (adjusting partitioning based on previous iterations' performance) can outperform work stealing but is less suitable for highly dynamic workloads.28
2.4.4 Granularity and Locality Considerations
The optimal load balancing strategy appears heavily dependent on the characteristic granularity of the work units involved in the interaction network reduction. If reductions are naturally coarse-grained or can be easily bundled into larger tasks, Option 1's simpler task-based work stealing might be sufficient and easier to implement. However, if the fundamental operations are very fine-grained and occur frequently, the overhead of task stealing could become prohibitive.7 In such cases, Option 2's partition-based approach, despite its complexity, might offer better performance by amortizing balancing overhead over larger work units, provided the cost of transferring/splitting partitions is manageable.
A fundamental tension exists between aggressive load balancing and maintaining data locality, especially on NUMA systems. Option 1's standard work stealing prioritizes keeping all cores busy, often at the expense of locality, as tasks are moved without consideration for data placement.26 Option 2's partition transfer mechanism inherently attempts to address this by moving data along with the work, aiming to preserve locality.27 However, the effectiveness of this depends critically on the cost and implementation of the partition transfer mechanism. This reflects a core design decision: is it better to prioritize maximal core utilization even if it means frequent remote data access, or to prioritize data locality even if it requires more complex and potentially coarser-grained load balancing?
3. Architecture-Specific Performance Evaluation
The choice between Option 1 and Option 2 is further influenced by the specific characteristics of the target hardware architecture.
3.1 x86 Architectures
Modern x86 server architectures typically feature high core counts and NUMA memory systems, both of which significantly impact the performance trade-offs.
* Multi-core/Many-core Scaling:
   * Option 1: As established, Option 1's reliance on global atomics makes it highly susceptible to contention bottlenecks as the number of x86 cores increases.3 The serialization effect means performance is likely to plateau or even degrade beyond a relatively modest core count (potentially 16-64 cores, depending on contention).3 Cache coherence traffic generated by moving locked cache lines between cores becomes a dominant overhead.2
   * Option 2: This approach offers better theoretical scaling potential on many-core x86 systems. By localizing work within partitions, it avoids the atomic contention bottleneck for local reductions.19 Scalability becomes limited by the efficiency of the partitioning algorithm in minimizing cross-partition communication, the performance of the on-chip interconnect and memory subsystem for handling ownership transfers, and the overhead of the partition management and load balancing logic.5
* NUMA Considerations:
   * Impact: NUMA is a defining characteristic of multi-socket x86 servers and significantly impacts memory access performance.20 Accessing memory attached to a remote socket is considerably slower than accessing local memory.20 Graph processing workloads, often involving irregular memory accesses, are particularly sensitive to NUMA effects.20
   * Option 1: The global buffer and random access pattern inherent in Option 1 make it perform poorly on NUMA systems without extensive, complex NUMA-aware optimizations that contradict its basic design.20 The default behavior would lead to frequent high-latency remote memory accesses, severely hindering performance.
   * Option 2: This strategy aligns much more naturally with NUMA principles. Partitions can be explicitly allocated on the memory local to the NUMA node where the owning thread resides.20 This co-location of data and computation minimizes costly remote memory accesses. Ownership transfers become explicit communication events between NUMA nodes, which can potentially be optimized. Implementing Option 2 effectively on NUMA requires NUMA-aware design choices, such as those demonstrated by the Polymer system: NUMA-aware data allocation, edge-oriented partitioning for load balance across nodes, and hierarchical synchronization mechanisms.20 GraphIt DSL extensions also target NUMA optimizations.25 Techniques focus on maximizing local accesses and ensuring that necessary remote accesses (like sequential reads during partition transfers) utilize higher-bandwidth modes if available.20
* Relevant Frameworks/Benchmarks: Performance studies of shared-memory graph processing frameworks like Ligra 20, Galois 20, X-Stream 20, GraphIt 25, and Polymer 20 on x86 platforms provide valuable data. They demonstrate the effectiveness of techniques like direction optimization (push/pull), hybrid approaches, NUMA-aware partitioning and scheduling, and sophisticated task scheduling. Comparisons like CPHASH vs. LOCKHASH 19 specifically highlight the benefits of partitioning over fine-grained locking for cache/coherence performance on multi-core x86.
* Likely Winner on x86: For contemporary and future high-core-count x86 server systems, particularly those employing NUMA, Option 2 (Partition Ownership) is significantly more likely to achieve higher performance and better scalability than Option 1. This assumes that the partitioning, communication (ownership transfer), and load balancing mechanisms in Option 2 are implemented competently and with NUMA awareness. Option 1 might only be competitive on older systems with very few cores, UMA architectures (rare in modern servers), or workloads exhibiting exceptionally low contention. The critical impact of NUMA on graph processing performance 20 strongly favors Option 2's locality-centric design.
3.2 Other CPU Architectures (e.g., ARM)
High-performance ARM processors are increasingly prevalent in servers and HPC, presenting an alternative to x86.
* High-Core Count ARM (Neoverse): Modern server-grade ARM CPUs, such as those based on Arm Neoverse V-series (like NVIDIA Grace 33) or N-series (like Ampere Altra 35 or AWS Graviton 36), offer high core counts (e.g., 72-144 cores per socket 33) and sophisticated interconnects (e.g., mesh 38, NVLink-C2C 34). These designs often incorporate NUMA principles or NUMA-like characteristics due to their scale.40 They are optimized for scalable performance and power efficiency.38
* Atomic Operations on ARM:
   * ARMv8.1-A introduced the Large System Extensions (LSE), providing dedicated atomic instructions (e.g., CAS, SWP, LDADD) designed to improve scalability on large server systems.9
   * As previously noted, performance critically depends on whether an atomic operation executes as "near" (within the local core/cluster cache) or "far" (handled by the interconnect).9 High contention on "far" atomics can lead to significant performance degradation due to interconnect latency and contention.9
   * System configurations often default to forcing cacheable atomics to execute as "near" atomics to mitigate this.9 This involves bringing the target cache line into the local cache exclusively before performing the atomic operation.
   * Implication: For Option 1 on ARM, performance under contention will be heavily influenced by this near/far behavior. Forcing "near" atomics might alleviate the interconnect bottleneck associated with "far" atomics but concentrates the contention handling onto the local core's cache and atomic execution logic. While potentially better than unmitigated "far" atomic contention, high contention levels will still likely lead to serialization at the cache line level, limiting scalability. This mechanism introduces a subtle difference compared to typical x86 atomic handling. On x86, atomics might be resolved further out in the coherence domain, potentially distributing the handling slightly differently. On ARM with forced "near" atomics, the bottleneck becomes intensely localized to the core attempting the operation and its L1/L2 cache.
* Parallel Graph Processing on ARM:
   * Similar to x86, NUMA effects are a major consideration on large ARM systems, and NUMA-aware optimization techniques are crucial for performance.40 Strategies involve co-locating computation and data across NUMA domains and employing cache-aware parallelization within nodes.40
   * Performance relative to x86 is workload-dependent. ARM server CPUs often demonstrate competitive or superior performance-per-watt 38, and sometimes better price-performance.36 Architectural differences (RISC vs. CISC, different cache hierarchies, memory models) exist, but high-end ARM and x86 server capabilities are converging.41 The presence of powerful vector units (like SVE/SVE2 33) is important for computationally intensive phases.
* Likely Winner on ARM: Similar to the conclusion for x86, Option 2 (Partition Ownership) appears more promising for high-core-count ARM server CPUs. The presence of NUMA or NUMA-like characteristics, coupled with the potential for atomic contention bottlenecks (even with LSE and "near" atomics), favors the locality-focused partitioning approach.40 The emphasis on NUMA awareness in ARM HPC optimization literature supports this preference.40
3.3 GPU Architectures
GPUs offer massive parallelism but have a distinct execution model and memory hierarchy compared to CPUs.
* GPU Parallelism Model: GPUs execute thousands of threads concurrently, organized into groups of 32 threads called warps. Warps are scheduled onto streaming multiprocessors (SMs). All threads within a warp execute the same instruction at any given time (SIMT - Single Instruction, Multiple Thread).44 Control flow divergence within a warp (threads taking different paths in an if-else) leads to serialization and performance degradation.45 Threads are further grouped into Cooperative Thread Arrays (CTAs) or thread blocks, which execute on a single SM and can cooperate using fast on-chip shared memory.44
* Atomic Operations on GPUs:
   * GPUs provide atomic operations for both global memory (off-chip DRAM) and shared memory (on-chip scratchpad).11
   * Global memory atomics are notoriously prone to becoming bottlenecks due to the sheer number of threads potentially contending for the same location.10 Performance is also highly sensitive to memory access patterns; uncoalesced atomic accesses (where threads in a warp access scattered locations) are particularly slow.12 While necessary for some coordination tasks, their use should be minimized.13
   * Shared memory atomics provide faster, on-chip synchronization but can still suffer from contention and bank conflicts.11 Older GPU architectures implemented shared memory atomics using costly lock/unlock sequences, while newer architectures (Maxwell and later) have native instructions, improving performance.11
   * Libraries like libcu++ provide C++ atomic types with finer control over memory ordering and scope (block, grid, system).48
   * Implication for Option 1: A direct implementation of Option 1, relying on atomic swaps on a global node buffer for locking, is highly likely to perform poorly on GPUs. The massive parallelism would lead to extreme contention on node locks located in global memory, creating a severe bottleneck.10
* Partitioning and Locality on GPUs:
   * Shared Memory: This fast, on-chip memory, accessible only to threads within the same CTA/block, is critical for high-performance GPU computing.45 It is used extensively in graph algorithms to cache frequently accessed graph data (e.g., vertex properties, parts of the adjacency list), facilitate efficient communication and synchronization among threads within a block, and perform local reductions to minimize expensive global memory atomics.10 Careful management is needed to avoid bank conflicts, where multiple threads in a warp access the same memory bank simultaneously.47
   * Partitioning Concepts: The concept of partitioning is central to efficient GPU graph processing, though implemented differently than on CPUs. Graph data is often divided into chunks or partitions that can be processed by individual thread blocks, often loading relevant data into shared memory for fast access.50 Techniques like warp specialization assign different roles (e.g., data loading vs. computation) to different warps within a block.45 GraphIt's ETWC optimization dynamically partitions the edges of a vertex among threads, warps, and CTAs based on neighbor list size to improve load balancing.53 For multi-GPU processing, techniques like vertex refinement selectively communicate only necessary boundary vertex data between GPUs.51
   * Implication for Option 2: The underlying principle of Option 2—partitioning work and data to maximize local computation—aligns well with the GPU execution model. However, a direct translation of thread-owned partitions is not typical. Instead, the implementation would likely involve partitioning the graph such that thread blocks process distinct subgraphs. Within a block, threads would collaborate using shared memory. "Ownership transfer" might manifest as writing results to global memory for other blocks to read, or using more advanced inter-block communication or multi-GPU communication primitives.51 Load balancing must consider the granularity of warps and blocks.53
* Common Strategies & Performance:
   * Successful GPU graph algorithms leverage massive parallelism by assigning vertices or edges to threads.54
   * Compact graph representations like Compressed Sparse Row (CSR) are used to maximize the amount of graph data that fits into the limited GPU global memory.51
   * Optimizing for coalesced global memory accesses (where threads in a warp access contiguous memory locations) is crucial for bandwidth.13
   * Maximizing the use of shared memory for data reuse and performing local reductions before writing results to global memory (often with a single atomic operation per block, if needed) is a key optimization pattern.10
   * Minimizing warp divergence by structuring code carefully is important.46
   * Efficient parallel primitives like prefix sum (scan) and reduction are often used as building blocks.55
   * For iterative algorithms, CUDA Graphs can amortize kernel launch overhead by defining the computation graph once and replaying it.44
   * Performance can differ between NVIDIA and AMD GPUs due to architectural choices like warp size (32 vs 64) and shared memory organization, with lower branching complexity potentially favoring AMD's larger warps.46
* Likely Winner on GPU: An implementation strategy inspired by the principles of Option 2 (Partitioning), but adapted specifically for the GPU's architecture and execution model, is overwhelmingly more likely to achieve good performance than a direct translation of Option 1. High-performance GPU graph processing relies heavily on maximizing shared memory utilization, minimizing slow global memory accesses and atomics, and ensuring coalesced memory patterns—all facilitated by partitioning work effectively at the block and warp level.13 Option 1's reliance on global atomics is fundamentally incompatible with efficient GPU execution at scale.10 The GPU's distinct memory hierarchy (registers, fast shared memory, slower global memory) and massive parallelism demand an approach that prioritizes keeping computation close to data in the on-chip shared memory, making Option 2's philosophy the better fit.
4. Synthesis and Recommendations
4.1 Summary of Trade-offs
The analysis reveals fundamental trade-offs between the two proposed runtime strategies:
* Synchronization: Option 1 uses hardware atomics on a global buffer, risking scalability bottlenecks due to contention. Option 2 uses explicit communication (ownership transfers) for non-local work, shifting the bottleneck to communication overhead and partition management.
* Locality: Option 1 provides simple global access but suffers from poor data locality, especially on NUMA systems. Option 2 enforces locality through partitioning, aligning better with modern memory hierarchies but requiring explicit management of non-local access.
* Load Balancing: Option 1 employs standard task-based work stealing, which is simpler but locality-agnostic and sensitive to task granularity. Option 2 uses coarser-grained partition transfer/splitting, potentially preserving locality but introducing higher complexity and transfer costs.
* Complexity: Option 1 appears simpler initially but faces potentially complex tuning challenges related to contention. Option 2 requires a more complex design upfront but offers more explicit control over performance factors.
4.2 Comparative Table
The following table summarizes the key characteristics and trade-offs of each option across several dimensions:


Feature
	Option 1: Global Atomics
	Option 2: Partition Ownership
	Notes
	Synchronization
	Atomics (e.g., swap) on global buffer
	Ownership transfer (message/protocol)
	Option 1 prone to contention 3; Option 2 overhead depends on comms.6
	Scalability Limit
	Atomic Contention 3
	Communication Bandwidth/Latency 5
	Option 1 limits scale on many-core; Option 2 depends on partitioning quality.
	Data Locality
	Poor (Global Access) 1
	Good (Local Partitions) 19
	Option 2 better suited for cache/NUMA optimization.20
	Cache Coherence
	High Traffic (Lock/Data Movement) 19
	Lower Traffic (Explicit Transfers) 19
	Option 1 suffers more from coherence bottlenecks.19
	NUMA Suitability
	Poor 20
	Good (Requires NUMA-aware impl.) 20
	Option 2 aligns naturally with NUMA locality principles.
	Load Balancing
	Task Work Stealing 4
	Partition Transfer/Splitting
	Option 1 simpler but locality-unaware 26; Option 2 complex.7
	LB Granularity
	Fine (Task/Node)
	Coarse (Partition)
	Option 1 suffers if tasks too fine 7; Option 2 has transfer cost.
	Impl. Complexity
	Lower initial complexity, harder tuning
	Higher initial complexity, more control
	Option 1 hides issues in atomics; Option 2 requires explicit protocols.
	Suitability: x86
	Low core count / UMA / Low contention
	High core count / NUMA / High contention
	NUMA strongly favors Option 2.20
	Suitability: ARM Svr
	Low core count / Low contention
	High core count / NUMA / High contention
	Near/Far atomics nuance 9, but NUMA likely favors Option 2.40
	Suitability: GPU
	Poor (Global atomics bottleneck) 10
	Good (If adapted to GPU model) 13
	Option 2 principles map better to shared mem/block parallelism.
	4.3 Factors Favoring Each Approach
Based on the analysis, the suitability of each option depends heavily on the target hardware and workload characteristics:
* Factors Favoring Option 1 (Global Atomics):
   * Targeting systems with low core counts (e.g., fewer than 8-16 cores), where atomic contention is less likely to become a dominant bottleneck.
   * Workloads exhibiting inherently low contention for node locks, meaning simultaneous access attempts to the same node are rare.
   * Prioritizing simpler initial implementation over achieving maximum scalability on high-end hardware.
   * Targeting Uniform Memory Access (UMA) architectures where data locality optimizations are less critical than on NUMA systems.
* Factors Favoring Option 2 (Partition Ownership):
   * Targeting high-core count systems, including modern x86 servers, ARM servers, and potentially GPUs (with adaptation).
   * Targeting NUMA architectures, where leveraging data locality is crucial for performance.
   * Workloads characterized by high interaction rates or patterns that lead to significant contention under Option 1's locking scheme.
   * Requiring predictable scalability beyond the limits imposed by atomic operation serialization.
   * Willingness to invest in a more complex implementation to gain fine-grained control over locality, communication, and load balancing.
   * The interaction network graph possesses properties that allow for effective partitioning (balancing load while minimizing cross-partition communication).
4.4 Recommendations
Considering the goal of achieving the fastest possible runtime for interaction network reduction, particularly on contemporary and future hardware:
1. For x86 Servers (Multi-Socket, NUMA): Strongly recommend pursuing Option 2. The prevalence of NUMA architectures and high core counts in this domain makes Option 1's global access model highly disadvantageous. Success with Option 2 requires a focus on NUMA-aware partition allocation (placing partitions on the owner thread's local node) and designing efficient, low-latency ownership transfer mechanisms. Drawing inspiration from NUMA-aware graph systems like Polymer 20 would be beneficial.
2. For High-Core Count ARM Servers: Also recommend Option 2. These architectures share many characteristics with x86 servers (high core counts, NUMA/NUMA-like interconnects) that favor locality-based approaches. While ARM's LSE atomics and near/far execution 9 offer nuances, the fundamental scaling limitations of contention and the importance of NUMA awareness 40 still point towards partitioning as the more robust strategy for scalability.
3. For GPUs: Recommend an approach based on the principles of Option 2, but heavily adapted to the GPU execution model. A direct translation of Option 1 is ill-advised due to the severe performance penalties associated with global atomic contention on GPUs.10 Instead, focus on partitioning the graph or workload among thread blocks (CTAs), maximizing the use of fast on-chip shared memory for local computation and reduction within each block, minimizing global memory traffic and atomic operations, and ensuring coalesced memory accesses. Study existing high-performance GPU graph libraries and techniques 46 for implementation patterns.
4. General Considerations:
   * Workload Profiling: Before committing, profile the specific interaction patterns of the target reduction workload. If analysis reveals extremely low contention rates across representative graphs and scales, Option 1 might remain a simpler, viable alternative, but this should be verified carefully.
   * Implementation Effort vs. Scalability: Option 2 offers a higher potential performance ceiling and better scalability characteristics on modern hardware but requires significantly more implementation effort and careful design (partitioning strategy, communication protocol, load balancing). Option 1 is simpler initially but risks hitting insurmountable scaling walls. The choice depends on the project's performance requirements and development resources.
   * Hybrid Models: While complex, exploring hybrid approaches (e.g., partitioning combined with localized work stealing) could be a future optimization path, but should likely be considered only after establishing a baseline with one of the core strategies.
In conclusion, for achieving high performance and scalability on modern multi-core/many-core architectures (x86, ARM, GPU), the Partition Ownership strategy (Option 2), when implemented with careful attention to locality (especially NUMA), communication efficiency, and appropriate load balancing, presents a more promising path than the Global Atomics strategy (Option 1). Option 1's reliance on global atomic operations introduces fundamental scalability bottlenecks related to contention and cache coherence that are difficult to overcome on systems with high core counts.
Works cited
1. (PDF) Challenges in Parallel Graph Processing. - ResearchGate, accessed on April 17, 2025, https://www.researchgate.net/publication/220439595_Challenges_in_Parallel_Graph_Processing
2. CHALLENGES IN PARALLEL GRAPH PROCESSING 1. Introduction Graphs provide a very flexible abstraction for describing relationships - CiteSeerX, accessed on April 17, 2025, https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=87ee99d4cc4e0601cbb519f6ddbac85772bdc49e
3. icl.utk.edu, accessed on April 17, 2025, https://icl.utk.edu/~herault/papers/108%20-%20Pushing%20the%20boundaries%20of%20Small%20Tasks:%20Scalable%20Low-Overhead%20Data-Flow%20Programming%20in%20TTG%20-%20CLUSTER%20(2022).pdf
4. Enabling Extremely Fine-grained Parallelism via Scalable Concurrent Queues on Modern Many-core Architectures - Data-Intensive Distributed Systems Laboratory - Illinois Institute of Technology, accessed on April 17, 2025, http://datasys.cs.iit.edu/publications/2021_MASCOTS21-XQueue.pdf
5. PARAGON: Parallel Architecture-Aware Graph Partition Refinement Algorithm - OpenProceedings.org, accessed on April 17, 2025, https://openproceedings.org/2016/conf/edbt/paper-110.pdf
6. A Study of Partitioning Policies for Graph Analytics on Large-scale Distributed Platforms - VLDB Endowment, accessed on April 17, 2025, http://www.vldb.org/pvldb/vol12/p321-gill.pdf
7. (PDF) Limits of Work-Stealing Scheduling - ResearchGate, accessed on April 17, 2025, https://www.researchgate.net/publication/221475366_Limits_of_Work-Stealing_Scheduling
8. SynCron: Efficient Synchronization Support for Near-Data-Processing Architectures - arXiv, accessed on April 17, 2025, https://arxiv.org/pdf/2101.07557
9. Do near or far atomics give the best performance on Neoverse systems? - Arm Developer, accessed on April 17, 2025, https://developer.arm.com/documentation/ka004706/latest/
10. Using Shared Memory to Accelerate MapReduce on Graphics Processing Units, accessed on April 17, 2025, https://www.researchgate.net/publication/224257759_Using_Shared_Memory_to_Accelerate_MapReduce_on_Graphics_Processing_Units
11. CUDA atomic operation performance in different scenarios - Stack Overflow, accessed on April 17, 2025, https://stackoverflow.com/questions/22367238/cuda-atomic-operation-performance-in-different-scenarios
12. Atomic Functions Performance - CUDA - NVIDIA Developer Forums, accessed on April 17, 2025, https://forums.developer.nvidia.com/t/atomic-functions-performance/5114
13. Lecture 13: Atomic operations in CUDA. GPU ode optimization rules of thumb. | Rui's Blog, accessed on April 17, 2025, https://blog.ruipan.xyz/earlier-readings-and-notes/cs759-hpc-course-notes/lecture-12-cuda-shared-memory-issues.-atomic-operations-in-cuda.
14. More Recent Advances in (Hyper)Graph Partitioning - arXiv, accessed on April 17, 2025, https://arxiv.org/pdf/2205.13202
15. ipads.se.sjtu.edu.cn, accessed on April 17, 2025, https://ipads.se.sjtu.edu.cn/projects/powerlyra/powerlyra-eurosys-final.pdf
16. 13 PowerLyra: Differentiated Graph Computation and Partitioning on Skewed Graphs - Papers on Big Data Meets New Hardware, accessed on April 17, 2025, https://readingxtra.github.io/docs/cpu-graph/ChenTOPC2019.pdf
17. A Transparent Hardware Mechanism for Cache Coherence and Fine-Grained Synchronization - UMass Amherst, accessed on April 17, 2025, https://www.umass.edu/nanofabrics/sites/default/files/sc07.pdf
18. Low-Overhead Software Transactional Memory with Progress Guarantees and Strong Semantics, accessed on April 17, 2025, https://mdbond.github.io/larktm-ppopp-2015.pdf
19. people.csail.mit.edu, accessed on April 17, 2025, https://people.csail.mit.edu/nickolai/papers/metreveli-cphash-tr.pdf
20. (PDF) NUMA-aware graph-structured analytics - ResearchGate, accessed on April 17, 2025, https://www.researchgate.net/publication/282890041_NUMA-aware_graph-structured_analytics
21. ipads.se.sjtu.edu.cn, accessed on April 17, 2025, https://ipads.se.sjtu.edu.cn/_media/publications/polymer-ppopp15.pdf
22. Scalable Synchronization in Shared-Memory Systems: Extrapolating, Adapting, Tuning - Infoscience, accessed on April 17, 2025, https://infoscience.epfl.ch/bitstreams/c1c56db4-d113-42bd-8f8e-307cb52735d5/download
23. Intro to Computer Architecture Unit 7 – Parallel and Multiprocessing Systems - Fiveable, accessed on April 17, 2025, https://library.fiveable.me/introduction-computer-architecture/unit-7
24. NUMA-Aware Graph Mining Techniques for Performance and Energy Efficiency - School of Electrical Engineering and Computer Science, accessed on April 17, 2025, https://www.cse.psu.edu/~kxm85/papers/NUMABC-SC12.pdf
25. Cache and NUMA Optimizations in A Domain-Specific Language for Graph Processing Mengjiao Yang - DSpace@MIT, accessed on April 17, 2025, https://dspace.mit.edu/bitstream/handle/1721.1/119915/1080643331-MIT.pdf?sequence=1
26. Fine-grained Dynamic Load Balancing in Spatial Join by Work Stealing on Distributed Memory - NSF Public Access Repository, accessed on April 17, 2025, https://par.nsf.gov/servlets/purl/10412122
27. Scalable Load Distribution and Load Balancing for Dynamic Parallel Programs - Manning College of Information & Computer Sciences, accessed on April 17, 2025, https://people.cs.umass.edu/~emery/pubs/wcbc-99-beb.pdf
28. Work Stealing and Persistence-based Load Balancers for Iterative Overdecomposed Applications - Parallel Programming Laboratory - University of Illinois Urbana-Champaign, accessed on April 17, 2025, http://charm.cs.illinois.edu/newPapers/12-11/main.pdf
29. PathGraph: A Path Centric Graph Processing System - College of Computing, accessed on April 17, 2025, https://faculty.cc.gatech.edu/~lingliu/papers/2016/PathGraph-TPDS.pdf
30. Chaos: Scale-out Graph Processing from Secondary Storage, accessed on April 17, 2025, https://www.cl.cam.ac.uk/~ey204/teaching/ACS/R244_2024_2025/papers/roy_sosp_2015.pdf
31. Chaos: Scale-out Graph Processing from Secondary Storage - acm sigops, accessed on April 17, 2025, https://sigops.org/s/conferences/sosp/2015/current/2015-Monterey/089-roy-online.pdf
32. Provably Efficient and Scalable Shared-Memory Graph Processing - UMD Department of Computer Science, accessed on April 17, 2025, https://www.cs.umd.edu/~laxman/papers/thesis.pdf
33. NVIDIA Grace Performance Tuning Guide, accessed on April 17, 2025, https://docs.nvidia.com/grace-perf-tuning-guide/index.html
34. NVIDIA Grace CPU Superchip Architecture In Depth | NVIDIA Technical Blog, accessed on April 17, 2025, https://developer.nvidia.com/blog/nvidia-grace-cpu-superchip-architecture-in-depth/
35. Scaling Azure Arm64 VMs with Microsoft's Build of OpenJDK: A Performance Testing Journey, accessed on April 17, 2025, https://techcommunity.microsoft.com/blog/azurecompute/scaling-azure-arm64-vms-with-microsoft%E2%80%99s-build-of-openjdk-a-performance-testing-/3820435
36. Hunting a NUMA Performance Bug - P99 CONF, accessed on April 17, 2025, https://www.p99conf.io/2021/09/28/hunting-a-numa-performance-bug/
37. Multi-level Memory-Centric Profiling on ARM Processors with ARM SPE pre-print submitted for publication - arXiv, accessed on April 17, 2025, https://arxiv.org/html/2410.01514v1
38. The Arm Neoverse N1 Platform: Building Blocks for the Next-Gen Cloud-to-Edge Infrastructure SoC, accessed on April 17, 2025, https://www.arm.com/-/media/global/solutions/infrastructure/arm-neoverse-n1-platform.pdf
39. Arm Announces Neoverse V3 and N3 CPU Cores: Building Bigger and Moving Faster with CSS - AnandTech, accessed on April 17, 2025, https://www.anandtech.com/show/21270/arm-announces-neoverse-v3-and-n3-cpu-cores
40. Optimizing massively parallel sparse matrix computing on ARM many-core processor, accessed on April 17, 2025, https://openreview.net/forum?id=8rkyEw0CK4
41. ARM vs x86: What makes them different? - C&T Solution Inc., accessed on April 17, 2025, https://www.candtsolution.com/news_events-detail/what-is-the-difference-between-arm-and-x86/
42. Why is ARM more efficient than X86 architecture? - Reddit, accessed on April 17, 2025, https://www.reddit.com/r/arm/comments/1bfne6w/why_is_arm_more_efficient_than_x86_architecture/
43. How does the ARM architecture differ from x86? [closed] - Stack Overflow, accessed on April 17, 2025, https://stackoverflow.com/questions/14794460/how-does-the-arm-architecture-differ-from-x86
44. Evaluating the performance of CUDA Graphs in common GPGPU programming patterns - DiVA portal, accessed on April 17, 2025, https://www.diva-portal.org/smash/get/diva2:1779194/FULLTEXT01.pdf
45. CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization - Michael E. Bauer, Ph.D., accessed on April 17, 2025, https://lightsighter.org/pdfs/cudadma-sc11.pdf
46. GPU Architectures in Graph Analytics:A Comparative Experimental Study - OpenProceedings.org, accessed on April 17, 2025, https://openproceedings.org/2025/conf/edbt/paper-175.pdf
47. Shared memory: Optimizing vectorized accesses vs bank conflicts, accessed on April 17, 2025, https://forums.developer.nvidia.com/t/shared-memory-optimizing-vectorized-accesses-vs-bank-conflicts/300265
48. Performance Impact of Removing Data Races from GPU Graph Analytics Programs - Computer Science : Texas State University, accessed on April 17, 2025, https://userweb.cs.txstate.edu/~burtscher/papers/iiswc24a.pdf
49. What's the point of warp-level gemm : r/CUDA - Reddit, accessed on April 17, 2025, https://www.reddit.com/r/CUDA/comments/1hk4410/whats_the_point_of_warplevel_gemm/
50. An Efficient Parallel Algorithm for Graph Isomorphism on GPU using CUDA - ResearchGate, accessed on April 17, 2025, https://www.researchgate.net/publication/287222374_An_Efficient_Parallel_Algorithm_for_Graph_Isomorphism_on_GPU_using_CUDA
51. Scalable SIMD-Efficient Graph Processing on GPUs - Computer Science and Engineering, accessed on April 17, 2025, https://www.cs.ucr.edu/~gupta/research/Publications/Comp/wsvr.pdf
52. Enabling advanced GPU features in PyTorch - Warp Specialization, accessed on April 17, 2025, https://pytorch.org/blog/warp-specialization/
53. Compiling Graph Applications for GPUs with GraphIt - Julian Shun, accessed on April 17, 2025, https://jshun.csail.mit.edu/gpu-graphit.pdf
54. Performance Improvement in Large Graph Algorithms on GPU using CUDA: An Overview - International Journal of Computer Applications, accessed on April 17, 2025, https://www.ijcaonline.org/volume10/number10/pxc3871992.pdf
55. Chapter 39. Parallel Prefix Sum (Scan) with CUDA - NVIDIA Developer, accessed on April 17, 2025, https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda
56. Time complexity of Parallel Reduction Algorithm - Stack Overflow, accessed on April 17, 2025, https://stackoverflow.com/questions/53372549/time-complexity-of-parallel-reduction-algorithm