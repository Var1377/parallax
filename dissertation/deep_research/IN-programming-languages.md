Interaction Networks as a Foundation for Programming Languages: Design, Implementation, and Analysis
1. Introduction
1.1 Context: The Parallelism Challenge
Modern computing hardware is characterized by massive parallelism, with multi-core CPUs and general-purpose GPUs offering computational power far exceeding that of traditional sequential processors. However, harnessing this power effectively remains a significant challenge for software development. Conventional models of computation, such as the Turing machine and the lambda calculus, while foundational, are inherently sequential. Adapting programs written with these models in mind to execute efficiently in parallel often requires complex manual effort, involving explicit management of threads, synchronization primitives (locks, mutexes, atomics), and careful consideration of data dependencies and race conditions.1 This complexity hinders productivity and limits the scalability of software on contemporary hardware. There is a clear need for computational models and programming paradigms that intrinsically support concurrency and parallelism, allowing developers to express parallel computations more naturally and enabling runtimes to exploit hardware capabilities automatically.2
1.2 Interaction Nets as a Solution
Interaction Nets (INs), introduced by Yves Lafont in 1990, offer a compelling alternative model of computation specifically designed with parallelism in mind.5 INs are a graphical formalism based on graph rewriting, originating as a generalization of the proof structures found in linear logic.5 Computation in an interaction net proceeds through local graph transformations governed by simple, predefined rules. The key promise of INs lies in their inherent potential for massive parallelism, which arises directly from fundamental properties of the model: locality (computations affect only a small, specific part of the graph) and strong confluence (the order of independent computations does not affect the final result).5 These properties eliminate the need for global synchronization, making INs a natural language for describing concurrent processes.5
1.3 Scope of the Report
This report provides an in-depth technical analysis of interaction nets as a foundation for programming languages. It explores the theoretical underpinnings of the IN model, including its core components, computational mechanism, and key mathematical properties. The report then delves into specific programming languages built upon this foundation, with a primary focus on Bend and Vine, examining their design philosophies, language features, syntax, runtime environments, and how they map high-level constructs onto the interaction net formalism. Furthermore, it discusses related formalisms like Interaction Combinators and the Interaction Calculus, surveys common implementation strategies (including runtime systems, reduction algorithms, memory management, and compilation techniques), and provides both a theoretical and practical evaluation of the strengths and weaknesses of IN-based approaches compared to traditional models like the lambda calculus. Finally, the report summarizes the current state of research and development in this area, identifying key challenges, potential future directions, and notable applications. This document is intended for computer science researchers, graduate students, and knowledgeable software engineers with an interest in advanced programming language theory, parallel computation models, and graph rewriting systems.
1.4 Roadmap
The report is structured as follows:
* Section 2 details the interaction net model of computation.
* Section 3 provides an in-depth look at the Bend programming language.
* Section 4 examines the Vine programming language and contrasts it with Bend.
* Section 5 discusses related formalisms, including Interaction Combinators and the Interaction Calculus.
* Section 6 surveys common implementation strategies for interaction net languages.
* Section 7 analyzes the theoretical advantages and disadvantages of the interaction net model.
* Section 8 evaluates the practical aspects of languages like Bend and Vine, including performance and usability.
* Section 9 summarizes the current research landscape, challenges, and future directions.
* Section 10 concludes the report with a synthesis of the findings.
2. Interaction Nets: A Graphical Model of Computation
Interaction nets represent a departure from traditional sequential computation models, offering a visual and inherently parallel paradigm based on graph rewriting.11 Understanding their structure and dynamics is crucial before examining specific language implementations.
2.1 Core Concepts
An interaction net system is defined by a signature (a set of symbols) and a set of interaction rules.5
* Agents: The fundamental computational units or nodes within the graph are called agents (or cells).5 Each agent is an instance of a symbol defined in the system's signature (Σ).
* Ports: Agents connect to each other via ports. Each agent type has a fixed number of ports.5 Crucially, one port is designated as the principal port, often depicted with an arrow (↑), while the others are auxiliary ports.5 The number of auxiliary ports defines the agent's arity.5 Ports that are not connected to another port are termed free ports; the set of all free ports constitutes the net's interface.5 Some formulations also include port types or polarities (e.g., input/output) to constrain connections.11
* Nets: A net is an undirected graph composed of agents connected by wires (edges) such that each port is connected to at most one wire.5 A wire may connect two ports of the same agent. Special cases include a wiring (a net with no agents, just connections between free ports) and the empty net.5
* Interaction Rules: Computation is driven by interaction rules (R), which define graph transformations.5 These rules are subject to strict constraints:
   * They only apply to active pairs: two agents connected via their principal ports.5 This is the interaction net analogue of a redex in term rewriting or lambda calculus. The restriction to principal port interaction simplifies the matching process compared to general subgraph isomorphism.36
   * Rules must preserve the interface: the set of free ports on the right-hand side of the rule must exactly match the set of auxiliary ports of the agents on the left-hand side.5 This ensures locality – reductions don't create or destroy external connections.
   * There is at most one rule defined for each pair of agent symbols (α,β).5 This removes ambiguity and ensures determinism.
2.2 Computation via Graph Rewriting
Computation in interaction nets is synonymous with graph reduction.17 The process involves repeatedly finding active pairs within the net and applying the corresponding interaction rule:
1. Identify Active Pair: Scan the net for two agents, say of type α and β, connected by their principal ports.5
2. Apply Rule: If a rule ((α,β)⇒N) exists for this pair, replace the active pair (α,β) with the net N specified by the rule's right-hand side.5
3. Rewire Connections: The auxiliary ports of the original α and β agents are reconnected to the corresponding free ports of the newly introduced net N. The interface preservation constraint ensures this rewiring is always possible and well-defined.13
4. Repeat: Continue this process until no active pairs remain in the net.
A net containing no active pairs is said to be in normal form.5 This normal form represents the result of the computation.
Illustration: Unary Addition
Consider representing natural numbers using a Z agent (zero, arity 0) and an S agent (successor, arity 1).18 The number 2 would be S(S(Z)). Addition can be implemented with an Add agent (arity 2) and two rules:
1. Add ↔ Z ⇒ wire (connecting Add's 1st aux to its 2nd aux)
2. Add ↔ S ⇒ S connected to a new Add (rewiring appropriately)
Graphically (26):
Rule 1: Add(r, y) >< Z => r ~ y (Textual representation 17)
Rule 2: Add(r, y) >< S(x) => r ~ S(z), x ~ Add(z, y) (Textual representation 17)
Computing 1 + 1 (S(Z) + S(Z)), represented initially as Add connected to two S(Z) structures via its auxiliary ports and its principal port connected to some output wire res, would proceed via rule applications, eventually resulting in a net representing S(S(Z)) connected to res.25
2.3 Key Theoretical Properties
The strict constraints on interaction rules bestow INs with several powerful theoretical properties:
* Locality: Reductions are strictly local, modifying only the two interacting agents and their connections.5 This isolation is fundamental to enabling parallel execution without interference.
* Linearity: Rule application is a constant-time operation in terms of graph manipulation.5 Furthermore, rules are linear in the sense that they preserve connections – auxiliary ports cannot be implicitly duplicated or erased during a rewrite.12 Any required duplication or erasure must be explicitly encoded using dedicated agents and rules.25
* Strong Confluence (One-Step Diamond Property): This is perhaps the most crucial property. If a net N can reduce to two different nets N1​ and N2​ in a single step (i.e., by reducing two different active pairs), then both N1​ and N2​ can reduce to a common net N3​ in a single step.5 This guarantees that the final result (normal form) is unique and independent of the order in which reductions are performed. A direct consequence is that all reduction sequences leading to a normal form have the same length and are permutations of each other.17
* Determinism: The combination of strong confluence and the constraint of having at most one rule per active pair ensures that the computation is fully deterministic.5 Pure interaction nets cannot inherently model non-deterministic choice; extensions are required for this purpose, such as introducing agents with multiple principal ports or specific choice operators.5
* Parallelism: The confluence, locality, and linearity properties collectively enable massive, asynchronous parallelism. Since reductions are local and cannot interfere, any and all active pairs present in a net at a given time can be reduced concurrently without requiring any synchronization mechanism.5
The power of these properties stems directly from the restricted nature of the interaction rules. Allowing only binary interactions via principal ports ensures locality and simplifies matching. Preserving the interface (linearity) maintains consistency at the boundaries of a reduction. Uniqueness of rules guarantees determinism. This carefully crafted set of constraints is precisely what enables the strong confluence and inherent parallelism that distinguish interaction nets from more general graph rewriting systems.16 However, this strictness introduces a fundamental tension: achieving these desirable theoretical properties comes at the cost of practical programming expressiveness. Simple computational patterns, especially those involving non-linear use of data (copying or discarding) or complex conditional logic, often require intricate encodings involving auxiliary agents and multiple interaction steps, as the basic rules lack powerful pattern matching capabilities.18 Explicit agents for duplication and erasure are needed where other models might handle this implicitly.25
2.4 Theoretical Roots
Interaction nets are deeply rooted in theoretical computer science, particularly in proof theory and logic.
* Linear Logic (LL): INs originated from Lafont's work on implementing the proof nets of Girard's Linear Logic.5 Proof nets provide a graphical representation of logical proofs that abstracts away irrelevant syntactic details present in traditional sequent calculus derivations, focusing instead on the essential connections between logical formulas.57 The reduction process in INs corresponds directly to the cut-elimination procedure in proof nets, a fundamental operation in proof theory that simplifies proofs.8 LL's emphasis on resources (formulas used exactly once unless explicitly marked otherwise) maps naturally onto the linearity constraints of IN rules.58 This connection provides a strong logical foundation for understanding IN properties and for developing type systems.
* Geometry of Interaction (GoI): GoI, another concept introduced by Girard, provides a semantic interpretation of logic and computation based on the paths or flows of information (often visualized as tokens) through a program's structure.5 Context semantics, derived from GoI, has been applied to INs to study program evaluation and complexity.24 While less central to the basic definition of INs than LL, GoI offers a complementary perspective on their dynamics.
* Curry-Howard Correspondence: The relationship between logic and computation, often summarized by the Curry-Howard correspondence ("propositions as types, proofs as programs"), finds an expression in INs. Research has established correspondences between typed interaction nets (specifically, typed sharing graphs) and systems of linear logic, particularly using deep inference formalisms.10 This provides a formal type system for INs, enhancing guarantees of correctness and potentially termination, while IN reduction offers a computational interpretation for proof normalization in these logical systems.10
The link to Linear Logic is thus foundational. It not only inspired the creation of INs but also provides the theoretical tools to analyze their behavior and ensure their correctness. The resource sensitivity inherent in LL is mirrored in the linearity constraints of INs, and the dynamics of cut-elimination in proof nets provide a logical semantics for IN reduction. This connection underpins much of the theoretical work on INs, including efforts to develop type systems 10 and understand their computational power.
3. The Bend Programming Language
Bend emerges as a high-level language explicitly designed to leverage the inherent parallelism of interaction nets, aiming to simplify the development of massively parallel applications for modern hardware like multi-core CPUs and GPUs.1
3.1 Design Philosophy
The core philosophy of Bend revolves around automatic parallelism.1 Its fundamental pledge is that "Everything that can run in parallel, will run in parallel".64 This means developers write code using high-level constructs, and the Bend runtime system is responsible for identifying and exploiting parallelism without requiring explicit directives like thread creation, locks, or atomics.1
To achieve this, Bend aims for an accessible syntax, often described as "Python-like," intended to lower the barrier to entry for parallel programming.1 The language abstracts away the underlying interaction net machinery, allowing developers to focus on the application logic.3 Bend is developed by the Higher Order Company, founded by Victor Taelin.2
3.2 Runtime Foundation: HVM/HVM2
Bend's execution model is entirely dependent on the Higher-order Virtual Machine (HVM), specifically its successor HVM2.1 HVM2 is a runtime environment designed as a massively parallel evaluator for Interaction Combinators 63, a universal subset of interaction nets.29
Bend programs are compiled into HVM's low-level intermediate representation (IR), which specifies the interaction net structure.2 HVM2 then provides multiple backends for executing these nets:
* A reference interpreter written in Rust (sequential execution, primarily for correctness checking or debugging).2
* A C interpreter/runtime (parallel execution on multi-core CPUs).2
* A CUDA interpreter/runtime (massively parallel execution on NVIDIA GPUs).2
A key feature of HVM is its approach to memory management. It aims to be garbage-collection-free, leveraging the linearity inherent in interaction nets.43 This is achieved through mechanisms like affine variables (used at most once) and potentially lazy copying, where data structures are only physically duplicated when necessary.62 This avoids the need for global stop-the-world garbage collection pauses, contributing to predictable performance and potentially better scalability.70 HVM also aims for efficient, potentially optimal, reduction strategies.70
3.3 Language Design
Bend's design attempts to blend familiarity with features conducive to parallel execution.
* Syntax: The syntax borrows heavily from Python, using def for functions and significant indentation for blocks.1 However, it diverges significantly by requiring static type annotations for function parameters and return values (-> Type) and incorporating functional programming concepts.62 This "Python-like" claim primarily refers to the surface syntax rather than the underlying programming paradigm, which is functional and statically typed, potentially creating a learning curve for developers expecting Python's dynamic typing and imperative style.4
* Core Features:
   * Purity: Functions are pure, meaning they lack side effects and always produce the same output for the same input.65 This is crucial for enabling safe automatic parallelization, as function calls do not interfere with each other.
   * Immutability: Data structures are generally immutable.69 Operations that appear to modify data typically create new versions. Bend employs lazy copying to mitigate the performance cost of duplication.62
   * Algebraic Datatypes (ADTs): Bend uses ADTs, defined with the type keyword, to represent structured data. These are similar to tagged unions or enums with associated data, allowing for variants like type Shape: Circle { radius } Rectangle { width, height }.4 Lists are implemented as an ADT: type List: Nil Cons { head, ~tail }, although syntactic sugar `` is provided.65 Strings are similarly lists of u24 characters.65 The reliance on linked lists (List) instead of contiguous arrays is a significant departure from typical imperative languages and can impact performance for array-centric algorithms.4
   * Pattern Matching: match/case expressions are used to deconstruct ADTs based on their variant, binding variables to the fields within each case.2 A switch statement is available for matching on native numeric types.65
   * Higher-Order Functions & Recursion: Functions are first-class citizens, supporting closures and lambda expressions (lambda x: x * 2). Unrestricted recursion is the primary mechanism for iteration.2
   * Folds and Bends: To provide parallelizable alternatives to sequential loops, Bend offers fold and bend operations.3 fold consumes a recursive data structure by applying functions corresponding to its constructors (e.g., Nil and Cons for lists). bend generates a recursive data structure based on an initial state and a step function. These constructs are designed to be decomposable for parallel execution by the HVM runtime.
* Type System: Bend employs a static type system, requiring explicit type annotations.62 A notable current limitation is the restricted set of built-in numeric types: u24 (unsigned 24-bit integer), i24 (signed 24-bit integer), and f24 (24-bit float).4 This choice is likely driven by optimizations for GPU hardware (where lower precision arithmetic can be faster) or simplification of the HVM implementation, but it imposes significant constraints on general-purpose programming, increasing the risk of overflows.72 Larger types are planned for the future.65 Input/Output (IO) capabilities are currently experimental and limited.65
3.4 Mapping High-Level Constructs to Interaction Nets
The translation from Bend's high-level code to the interaction nets executed by HVM involves several key mappings:
* Compilation Target: Bend code is compiled into HVM's Interaction Combinator representation.2 This means high-level constructs like functions, datatypes, and pattern matching are ultimately expressed using a small set of universal interaction net agents (constructors, duplicators, erasers) and their interaction rules.
* Parallelism Strategy: The automatic parallelism arises from HVM's ability to identify and concurrently reduce independent active pairs within the generated interaction net.76 Algorithms structured using divide-and-conquer approaches naturally expose this independence and thus parallelize well in Bend/HVM.2 The runtime likely employs strategies to distribute these reduction tasks across available CPU cores or GPU threads.76
* Data Structures: Bend's ADTs are likely translated into specific configurations of interaction combinators. For example, a Cons cell might map to a constructor node, while accessing shared data might involve duplicator nodes. The fold and bend operations are designed to generate net structures that the HVM can efficiently reduce in parallel, likely by creating balanced tree-like structures or enabling pipeline parallelism.
Bend's core innovation appears to be the HVM runtime and its promise of transparently parallelizing code written in a relatively high-level, functional style. The language itself provides features (purity, immutability, ADTs, folds/bends) that facilitate this automatic parallelization by ensuring computations are decomposable into independent tasks suitable for the interaction net model.
4. The Vine Programming Language
Vine is another recent entrant in the space of interaction net-based programming languages, presenting a different philosophy and approach compared to Bend.62
4.1 Design Philosophy
Vine is explicitly positioned as an experimental language.87 Its primary goal is not necessarily ease of use or immediate high performance, but rather to explore and expose the fundamental capabilities of the interaction net model.62 Started by developer "T6" in June 2024 62, Vine aims to provide a platform for realizing the potential perceived in interaction nets, particularly for parallel and distributed computing.87
A key aspect of Vine's philosophy is its multi-paradigm nature, specifically designed to allow seamless interoperability between functional and imperative programming patterns.88 This suggests an attempt to bridge the gap between the purely functional nature of interaction nets and the stateful, sequential operations common in imperative programming. Influences are drawn from languages like Rust, Haskell, Erlang, and OCaml, emphasizing safety, performance, and concurrency.87
4.2 Runtime and Compilation
Vine employs its own distinct runtime system and compilation pipeline:
* Ivy Intermediate Language: Vine code is compiled into an intermediate language called Ivy.62 Ivy is described as a low-level interaction-combinator programming language, serving a similar role to HVM's IR but specific to the Vine ecosystem. Ivy programs consist of named global nets specified using a syntax based on the interaction calculus.98
* Interaction Virtual Machine (IVM): Ivy code is executed on the IVM, a dedicated runtime environment for interaction combinators developed for Vine.62 Currently, the IVM appears to be a CPU-based virtual machine, as GPU support is explicitly mentioned as lacking compared to Bend.62 The choice of a custom VM (IVM) and intermediate language (Ivy) grants Vine greater control over its specific execution model and features, potentially enabling unique optimizations or language constructs tailored to its interaction net interpretation. However, this comes at the cost of not immediately leveraging existing, highly optimized backends like C/CUDA compilers or potentially HVM itself, which might impact portability and raw performance on standard hardware.
* Compilation Pipeline: Vine utilizes a multi-stage compilation process to transform source code into executable Ivy nets.87 This involves:
   1. Parsing source files into an Abstract Syntax Tree (AST).
   2. Resolving modules and disambiguating AST nodes.
   3. Type checking expressions and forms.
   4. Distilling the AST into Vine Intermediate Representation (VIR), simplifying expressions and converting control flow into a Stacked Flow Graph (SFG).
   5. Normalizing the VIR to remove divergence.
   6. Analyzing the VIR for reachability and dataflow properties.
   7. Emitting the final Ivy net code from the processed VIR. This structured pipeline suggests a design focused on enabling sophisticated analysis and optimization passes, potentially aiming for greater correctness guarantees or performance tuning in the future, even if the current implementation is experimental.
4.3 Language Design
Vine's design reflects its goal of exploring interaction nets while providing a multi-paradigm interface.
* Syntax: Vine is statically typed and compiled.87 Its syntax is influenced by Rust, featuring an expression-oriented style and a module system (e.g., use std::IO; pub fn main(...) {... }).62 It includes standard features like integer and float operations, boolean logic, string manipulation, tuples, if-else conditionals, and while loops.87 for loops and break statements are noted as currently absent.97
* Key Features: Beyond the standard constructs, Vine aims for functional/imperative interoperability.88 Examples demonstrate basic variable assignment (let), arithmetic, conditionals, and IO operations via a standard library module.97
* Unique Concepts:
   * Borrowed References: Mentioned as a feature to help programmers avoid unnecessary data copies 62, hinting at memory management optimizations controllable by the developer, possibly akin to Rust's borrowing system but adapted to the interaction net context.
   * Inverse Operator / Types: Vine introduces a concept related to "inverse" operations or types, often associated with the ~ operator.62 This appears to be a high-level abstraction over the bidirectional nature of connections in interaction nets. The sub_min example illustrates its use: a function calculates the minimum of a list and subtracts it from every element. Imperatively, this requires two passes. Vine, using the inverse operator, can express this in a way that the underlying interaction net evaluation effectively performs the second pass's logic (subtraction using the computed minimum) as the result of the first pass (the minimum value) becomes available and propagates "backwards" through the net structure, without requiring a second explicit iteration over the data structure.96 This leverages the graph reduction mechanism to achieve complex data flow patterns efficiently.
4.4 Comparison with Bend
While both Bend and Vine are built on interaction nets, they represent different points in the design space:


Feature
	Bend
	Vine
	Primary Goal
	Automatic parallelism, ease of use (Python-like) 1
	Exposing IN model power, functional/imperative interop 62
	Runtime
	HVM/HVM2 2
	IVM 87
	Targets
	CPU (C), GPU (CUDA) 2
	CPU (via IVM) 62
	Syntax Influence
	Python, Haskell 62
	Rust 87
	Paradigm Focus
	Functional (for parallelism) 65
	Multi-paradigm (Functional/Imperative) 88
	Key Abstraction
	Automatic parallelism, Folds/Bends 2
	Inverse operator/types 62
	Current Status
	Demonstrates GPU scaling, performance needs optimization 62
	Experimental, focus on features/tooling, performance TBD 62
	Bend prioritizes making parallelism accessible through a familiar-feeling (if functionally oriented) language, relying on the sophisticated HVM runtime for automatic scaling. Vine, conversely, seems more focused on exploring the theoretical possibilities of interaction nets directly, providing potentially more powerful but less abstracted language features like the inverse operator, while building its own specific runtime infrastructure.
5. Related Formalisms and Systems
Interaction nets do not exist in isolation but are part of a broader landscape of computational models, particularly those related to graph rewriting, linear logic, and functional computation. Understanding these related systems provides context for the design and capabilities of IN-based languages.
5.1 Interaction Combinators (ICs)
Interaction Combinators represent a minimal, universal core of the interaction net model.1
* Definition: An IC system is a specific interaction net system defined with a very small, fixed set of agent types (combinators) and interaction rules. Lafont's influential system uses just three agents: a constructor (γ), a duplicator (δ), and an eraser (ϵ), along with six interaction rules.25 Symmetric variants also exist, simplifying the rules further.38 Constructor and duplicator agents typically have one principal port and two auxiliary ports.25
* Universality: Despite their simplicity, ICs are computationally universal in two senses: they are Turing complete 43, and more importantly, they are universal for interaction nets. Any computation expressible in any interaction net system can be translated into an equivalent computation using only interaction combinators.5 This demonstrates that the fundamental computational dynamics of interaction nets can be captured by a minimal set of local graph transformations.
* Rules: The core rules governing ICs typically involve:
   * Annihilation: When two agents of the same type interact via their principal ports, they are deleted, and their corresponding auxiliary ports are connected directly.25
   * Commutation/Duplication: When two agents of different types interact, both agents are duplicated, and their auxiliary ports are rewired in a specific pattern (often involving a "crossing" or swapping of connections).25 These simple local rules of annihilation and commutation/duplication form the bedrock of computation in this model.25
* Relevance: Interaction combinators serve as the foundational "assembly language" for many interaction net implementations. Runtimes like HVM 63 and the IVM (via Ivy) 87 are essentially efficient evaluators for nets composed of interaction combinators. High-level languages like Bend and Vine are compiled down to this combinator level for execution.
The existence and universality of interaction combinators suggest that the core computational power of interaction nets lies not in complex agent behaviors but in the fundamental graph topology manipulations defined by these simple, local rules. This minimalism is theoretically elegant and provides a concrete target for efficient runtime implementation.
5.2 Interaction Calculus (IC)
The Interaction Calculus provides a textual syntax for describing interaction nets and their reduction, bridging the gap between the graphical model and symbolic manipulation.5
* Definition: IC represents nets as configurations, typically involving an interface (a sequence of free names) and a multiset of equations.5 Terms are built from agent symbols and names (variables), where names represent the endpoints of wires. A crucial constraint is linearity: each name must appear exactly twice within a configuration, representing the two ends of a wire.5 Agent applications are often represented like function calls, e.g., α(x1​,...,xn​), where the principal port might be implicitly represented or explicitly named.6 Equations like t=u or x∼t represent connections.6
* Lambda Calculus Encoding: IC provides a direct way to encode lambda calculus terms and beta-reduction using interaction combinators.43 Lambda abstractions (λx.body) and applications (ts) are typically mapped to constructor nodes (γ), while variable usage (especially non-linear usage requiring copying) involves duplicator nodes (δ).43 This encoding allows interaction nets/calculus to serve as an implementation model for functional languages.
* Reduction: Computation proceeds via rules analogous to IN reduction:
   * Interaction: An equation between two agent terms representing an active pair ($ \alpha(...) = \beta(...) $) is replaced by the set of equations corresponding to the right-hand side of the interaction rule.5
   * Indirection/Substitution: An equation of the form x=t allows the substitution of t for the other occurrence of the name x in the configuration.5 The calculus inherits the strong confluence and deterministic properties of the underlying interaction nets.5 Notions like α-conversion also apply naturally.5
The Interaction Calculus is thus essential both for theoretical analysis (providing a formal language to reason about INs) and for practical implementation (serving as a basis for textual IN languages and compilation targets like Ivy or HVM IR).
5.3 Abstract Calculus
The Abstract Calculus is mentioned as a related formalism, particularly in the context of Victor Taelin's work (creator of HVM/Bend).52 It appears to be a term rewriting system heavily inspired by lambda calculus but incorporating key features from interaction nets/combinators:
* Affine Variables: Variables can be used at most once, similar to the linearity constraint in IC/INs.85
* Global Variables: Variables have a global scope, simplifying substitution.85
* Explicit Superposition & Duplication: Includes first-class constructs for superposition (&) and duplication (!), analogous to interaction combinator operations.85
This calculus aims to combine the expressiveness of lambda calculus (e.g., for higher-order functions) with the efficiency properties (e.g., optimal evaluation potential, GC-free nature) derived from the interaction net model.85 It serves as another bridge formalism, offering a different perspective on combining functional programming with interaction-based computation.
5.4 Other Graph Rewriting Systems
Interaction nets are a specific, constrained form of graph rewriting.17 General graph rewriting encompasses a wider range of rule types and application mechanisms, such as the Double-Pushout (DPO) or Single-Pushout (SPO) approaches.100 Other related systems include:
* Term Graph Rewriting: Focuses on rewriting graphs that represent terms, often incorporating sharing of subterms for efficiency.53 This is closely related to INs, which can be seen as a form of term graph rewriting with specific constraints.
* Hierarchical Graph Rewriting: Systems like LMNtal extend graph rewriting with concepts like membranes or boxes to represent hierarchical structures, also drawing connections to linear logic.54
* Bigraphs: Milner's Bigraphs offer a formalism for modeling systems with both connectivity (linking) and locality (nesting), which has inspired extensions to interaction nets (Bigraphical Nets) to handle limitations in expressing certain patterns, particularly in the context of the ρ-calculus.34
These other systems highlight the broader context of graph transformation techniques, where interaction nets occupy a specific niche characterized by their simplicity, strong theoretical properties (confluence, parallelism), and close ties to linear logic and functional computation.
6. Implementation Strategies for Interaction Net Languages
Implementing programming languages based on interaction nets involves translating the abstract model of agents, ports, and rules into executable code. Various strategies have been explored, targeting different levels of abstraction and performance goals.
6.1 Runtime Architectures
The execution environment for interaction net programs can take several forms:
* Virtual Machines (VMs): These provide a dedicated runtime environment specifically designed to evaluate interaction nets, often based on interaction combinators.
   * HVM/HVM2: Developed for Bend, HVM2 is a high-performance, massively parallel evaluator for interaction combinators. It acts as a compile target for Bend and potentially other languages, offering backends for C (parallel CPU) and CUDA (massively parallel GPU).62 Its design emphasizes automatic parallelism and GC-free memory management.70
   * IVM: The Interaction Virtual Machine is used by Vine to execute its compiled Ivy code (also based on interaction combinators).87 It provides a runtime tailored to Vine's specific features but currently appears limited to CPU execution.62
* Interpreters: These systems directly execute a textual or graphical representation of interaction nets and their rules.
   * Inpla: A notable example is Inpla, a multi-threaded interpreter for an interaction net language written primarily in C and Yacc.11 It uses the POSIX threads library for parallelism and includes features like built-in types and an interactive mode.30
   * Graphical Interpreters: Tools like INblobs 7 and in2 27 provide visual editors and step-by-step interpreters, emphasizing the graphical nature of INs for pedagogical or design purposes.
* Compilers: These translate interaction net languages or representations into other existing languages or machine code.
   * INETS Compiler: Compiles the INETS language (an extended textual syntax for INs) to C, leveraging C's portability and optimizing compilers.17
   * Pin Compiler: The Pin language was designed with compilation to an abstract machine in mind.13
   * Embeddings: Encoding interaction nets within existing functional languages like OCaml 11 or Concurrent Haskell.11 This approach leverages the host language's type system for correctness and its concurrency features for potential parallelism.
* Abstract Machines: These are theoretical models that define the operational semantics of interaction net reduction in a step-by-step manner, bridging the gap between the high-level graphical rules and concrete implementations. Examples include machines proposed by Sousa Pinto 27 and those based on chemical abstract machine (CHAM) formalisms.23
This spectrum of architectures reflects different design goals, trading off factors like performance, portability, ease of implementation, correctness guarantees, and level of abstraction.
6.2 Graph Reduction Algorithms
Regardless of the runtime architecture, the core computation involves graph reduction based on interaction rules.
* Core Operation: The fundamental step is identifying an active pair (two agents connected via principal ports) and replacing it with the right-hand side net specified by the corresponding interaction rule, correctly rewiring the auxiliary ports.5
* Reduction Strategies: Strong confluence means the final result is independent of the order of reductions.5 Implementations typically choose a strategy for selecting which active pair(s) to reduce next:
   * Sequential: Reduce one active pair at a time. A common approach uses a stack (LIFO) to manage active pairs: when a reduction creates new active pairs, they are pushed onto the stack, and the next reduction processes the pair at the top.17
   * Parallel: Identify all currently active pairs in the net and reduce them simultaneously within a single parallel step. This process is repeated until no active pairs remain.9 This strategy directly exploits the inherent parallelism of INs.
* Optimal Reduction: This concept, originating from lambda calculus research (specifically Lévy's work), aims to perform the minimum number of reduction steps necessary by sharing identical sub-computations.61 Interaction nets, particularly through encodings using interaction combinators, provide a mechanism for implementing optimal (or near-optimal) reduction strategies for the lambda calculus.5 Lamping's algorithm is a key example implemented via INs.6 Runtimes like HVM aim for beta-optimality, which is closely related.70 However, achieving true, provably optimal reduction for all cases is theoretically complex and may incur significant overhead.111 Practical implementations often prioritize efficient parallel reduction over strict Lévy optimality.
* Token-Passing Nets: This technique encodes specific evaluation strategies (e.g., call-by-value, call-by-need, optimal reduction) directly into the interaction net system itself.46 Special "token" agents traverse the net, enabling only specific active pairs according to the desired strategy. This allows implementing lazy evaluation or optimal reduction within the pure IN framework, potentially simplifying the runtime but adding complexity to the net structure and rules.109
6.3 Memory Management
A significant theoretical advantage claimed for interaction nets is the potential to avoid traditional garbage collection (GC).13 Conventional GC techniques like mark-and-sweep, generational collection, or reference counting typically involve runtime overhead, pauses, or complexity in managing object lifetimes globally.112 Interaction nets offer alternatives:
* Implicit Management via Linearity: Because interaction rules are local and preserve the interface (linearity), the agents involved in an interaction (the active pair) are consumed. The resources they occupied can often be immediately identified as free or reused for the agents created on the right-hand side of the rule.84 This localizes memory management decisions.
* Explicit Duplication/Erasure: When non-linear behaviour (copying or discarding data) is required, it must be encoded explicitly using dedicated duplicator (δ) and eraser (ϵ) agents and their associated interaction rules.25 Computation itself handles the allocation/deallocation logic via these explicit operations.
* Agent Reuse Optimization: A crucial implementation technique involves reusing the memory allocated for the agents in the active pair (left-hand side of a rule) to construct the agents required for the right-hand side.9 This significantly reduces the need for dynamic memory allocation and deallocation during reduction, improving efficiency. Compilers or runtimes can analyze rules to identify opportunities for reuse.
* Runtime-Specific Techniques:
   * HVM: Employs affine logic principles (variables used at most once) enforced by the runtime. Data duplication is handled potentially lazily or via specialized atomic operations (linkers) in the runtime, avoiding explicit GC passes.62
   * C Compilers: Implementations compiling to C often manage memory directly using a heap (e.g., a large array) and functions like mkAgent for allocation and potentially free for deallocation, combined with agent reuse strategies.17 Techniques common in manual memory management like bump allocation, free lists, or slab allocation could potentially be adapted depending on the allocation patterns.113
While INs offer the potential for GC-free execution, this doesn't mean memory management is entirely "free." It shifts the burden from a global, runtime GC mechanism to either explicit programmer encoding (using duplicators/erasers), sophisticated compiler analysis (for agent reuse), or specialized runtime mechanisms (like HVM's). Achieving efficiency requires careful implementation of these strategies.
6.4 Compilation Techniques
Translating high-level interaction net languages or abstract net descriptions into executable code involves various techniques:
* Targeting C/CUDA: A common approach for performance and portability. This involves:
   * Representing the net's graph structure using C data types (structs for agents and ports, arrays or pointers for connections, a global heap array).17
   * Compiling each interaction rule into a C function that takes the locations of the interacting agents as input and performs the necessary graph manipulations (allocating new agents, updating connections, freeing/reusing old agents).17
   * Implementing a runtime scheduler (often stack-based for sequential execution) that identifies active pairs and dispatches to the appropriate rule function.17
   * For CUDA targets (like HVM or ingpu 35), this involves mapping the net structure and reduction tasks onto the GPU's parallel architecture (kernels, shared memory, atomic operations), which presents challenges due to the dynamic and irregular nature of graph rewriting.35
* Targeting Functional Languages: Embedding INs into languages like OCaml or Haskell.11 This allows leveraging the host language's features:
   * Type Safety: Using the host's type system (e.g., OCaml's GADTs or Haskell's type classes) to enforce IN constraints like arity or even Lafont's port typing rules statically.11
   * Concurrency: Utilizing the host language's concurrency primitives (e.g., OCaml 5's domains and effect handlers, Concurrent Haskell's MVars/STM) to implement parallel reduction.11
* Low-Level Intermediate Representations (IRs): Defining specialized IRs that capture the essence of interaction net operations but are closer to a machine model. Examples include:
   * HVM IR: Textual syntax for wiring interaction combinator nets.74
   * Ivy: Vine's low-level interaction-combinator language.87
   * LL0: A low-level language with instructions for allocating agents/names and manipulating connections, designed for studying implementation costs.9 Compilation then involves translating the source language to this IR, which is subsequently interpreted or compiled further.
* Flattening: A common preliminary step is to convert the graphical net structure into a linear, textual representation. This typically involves assigning unique names (variables) to ports/wires and representing the net as a list of agents annotated with the names of the ports they connect to.6 An edge is represented by two occurrences of the same name. This flattened representation is often the input to interpreters or compilers.
7. Theoretical Analysis
Interaction nets offer a unique computational model with distinct theoretical advantages and disadvantages compared to established formalisms like the lambda calculus.
7.1 Advantages of Interaction Nets
The core theoretical strengths of INs stem directly from their constrained definition:
* Inherent Parallelism: The model is fundamentally parallel. Locality and strong confluence ensure that multiple reductions can occur simultaneously and asynchronously without interfering or requiring synchronization, making INs a natural fit for parallel hardware architectures.5
* Strong Confluence: Guarantees deterministic outcomes and uniqueness of normal forms, simplifying correctness proofs and eliminating concerns about evaluation order (lazy vs. strict debates become less relevant at the net level).5 It also implies that all reduction paths to normal form have the same length.17
* Locality and Linearity: Reductions are simple, local graph operations that can be performed in constant time, simplifying implementation and analysis.5
* Absence of Implicit Garbage Collection: The local nature of reduction and explicit handling of duplication/erasure avoids the need for complex, global garbage collection mechanisms, potentially leading to more predictable performance and lower overhead.13
* Foundation for Optimal Reduction: INs provide a framework for implementing optimal reduction strategies for the lambda calculus, minimizing redundant computations.5
* Explicit Computational Steps: All operations, including structural ones like copying or discarding data, are represented explicitly within the net and rule formalism.13 This makes INs suitable for developing fine-grained cost models.13
7.2 Disadvantages and Challenges
The very constraints that provide the advantages also lead to theoretical and practical challenges:
* Primitive Pattern Matching: The restriction that rules only apply to active pairs (binary interaction on principal ports) makes expressing computations involving matching on multiple arguments or nested structures cumbersome.16 Programmers must often introduce auxiliary agents and rules to decompose complex matches into sequences of binary interactions.18 This can obscure the intended logic and increase the complexity of IN programs. Extensions like macros 18 or generic rules 12 have been proposed to alleviate this, but they add complexity to the core model.
* Expressiveness Limitations: While Turing complete 11, expressing certain common programming constructs can be unnatural or inefficient in pure INs.
   * Non-determinism: Pure INs are deterministic and cannot directly model non-deterministic choice or concurrency patterns found in process calculi without extensions (e.g., agents with multiple principal ports, amb-like operators).5
   * Higher-Order Functions: While INs can implement lambda calculus, directly representing higher-order functions as first-class citizens within the IN model itself requires careful encoding or extensions like generic rules.12
   * Side Effects: Modeling state, I/O, or exceptions requires extensions, such as adapting monadic structures, which again complicates the pure model.33
* Data Sharing Inefficiency: The core reduction mechanism consumes the interacting agents. While this aids memory management, it can be inefficient for algorithms that rely heavily on sharing complex data structures, as structures might need to be explicitly reconstructed or copied via duplicator agents.16 Implementations of lambda calculus often rely on specific graph structures (sharing graphs) to manage this.10
* Need for Abstraction: Programming directly at the level of agents and wires is akin to assembly programming – powerful but tedious and error-prone.16 Higher-level languages (like Bend, Vine, Pin) are necessary to provide abstractions, syntactic sugar, modules, and other features expected by programmers.13
7.3 Comparison with Lambda Calculus
Interaction nets are often compared to the lambda calculus (λ-calculus), particularly as INs are frequently used for its implementation.5
* Expressiveness: Both models are Turing complete.11 However, they represent computation differently. Lambda calculus is based on function abstraction and application (substitution), while INs are based on local graph interaction.43 Encoding one in the other is possible but not always direct or efficient. INs can express graph structures not directly representable as lambda terms.110 The ρ-calculus, which generalizes lambda calculus with pattern matching, highlights differences, as its implementation in INs reveals implicit parallelism not easily captured in standard lambda calculus semantics.34
* Evaluation Strategy: Lambda calculus has various reduction strategies (call-by-name, call-by-value, call-by-need, optimal) with different performance characteristics and termination properties. Interaction nets, due to strong confluence, have a more uniform behaviour – the order doesn't matter for the result, making the lazy vs. strict distinction less fundamental at the net level.110 Specific strategies can be encoded using techniques like token-passing nets.46
* Sharing and Copying: Lambda calculus substitution can implicitly duplicate subterms, leading to redundant computations if not implemented carefully (e.g., via graph reduction). Interaction nets handle sharing more explicitly: active pairs (redexes) cannot be duplicated 20, and copying requires explicit duplicator agents.25 This forms the basis for optimal reduction implementations.110
* Parallelism: Lambda calculus is inherently sequential. While parallel evaluation strategies exist, parallelism is not intrinsic to the model. Interaction nets are inherently parallel due to locality and confluence.5
* Theoretical Foundation: Lambda calculus is foundational for functional programming. Interaction nets are rooted in linear logic and proof nets, offering a different logical perspective focused on resource consumption and interaction.5
In essence, interaction nets provide a lower-level, more explicit, and inherently parallel model of computation compared to the lambda calculus. While lambda calculus offers powerful abstraction through function application and substitution, interaction nets focus on the dynamics of local interactions and resource management, making them well-suited for analyzing and implementing efficient, parallel evaluation strategies, particularly for functional languages.
8. Practical Evaluation
While theoretically promising, the practical viability of interaction net-based languages like Bend and Vine depends on factors like performance, ease of programming, and the availability of development tools.
8.1 Performance
Performance remains a critical aspect and a significant challenge for current interaction net languages.
* Bend:
   * Scaling vs. Absolute Speed: Bend's primary performance claim is scalability with core count, particularly on GPUs, due to the HVM2 runtime.1 Benchmarks provided by the developers for algorithms like bitonic sort show significant speedups when moving from single-thread CPU to multi-thread CPU (e.g., 12.15s to 0.96s on M3 Max) and further speedups on GPU (0.21s on RTX 4090).2 A parallel sum example also demonstrated scaling.2
   * Current Limitations: However, the developers acknowledge that single-core performance is currently low, and the code generation ("codegen") is immature ("embarrassingly bad", "abysmal").2 Independent benchmarks on tasks like matrix determinant calculation showed Bend performing significantly worse than single-threaded Go for larger inputs, even with multi-core execution, potentially due to the overhead of its linked-list based data structures and interaction combinator encoding.4 Comparisons on a simple recursive sum also showed Bend to be much slower than Python or PyPy, attributed by the developers to high allocation overhead for numeric operations in the current HVM2 implementation for that specific case.72 Performance is expected to improve as code generation and optimizations mature.2 The limited 24-bit numeric types also pose a practical performance (and correctness) constraint.4
* Vine:
   * Status: Vine is highly experimental, and performance is not yet a primary focus.62 The language currently runs on its own CPU-based virtual machine (IVM) without GPU support.62
   * Metrics: The Vine runtime (IVM) reports performance metrics like total interactions, memory usage, execution time, and Interactions Per Second (IPS).96 Early examples show variable IPS, suggesting performance is highly dependent on the specific net structure and workload.96
   * Outlook: The creator acknowledges that achieving performance comparable to optimized runtimes on existing CPUs will require significant effort, but sees potential for GPUs or custom hardware in the future.62 The focus seems to be more on exploring the model's capabilities (like the inverse operator) than on immediate speed.62
* Inpla:
   * Benchmarks: The Inpla interpreter has been benchmarked against languages like Haskell, OCaml, SML, and Python.30 Results show that multi-threaded Inpla (Inpla8) can outperform these other implementations (often sequential) on certain tasks, particularly those amenable to parallelism like quick sort or merge sort on large inputs. A speed-up graph demonstrates scaling with the number of threads for parallelizable algorithms.30 However, for inherently sequential tasks or those with less potential for parallelism (like insertion sort or Ackermann function in some implementations), Inpla might not show significant advantages or could be slower than compiled languages like OCaml or Haskell.30 Performance also depends on specific Inpla features like reuse annotations (Inpla8r).30
Overall, while the parallelism of interaction nets offers significant potential for performance scaling, current practical implementations like Bend and Vine are still in early stages. Absolute performance often lags behind mature, highly optimized languages and runtimes, especially on single-core or sequential tasks. Achieving competitive performance requires substantial effort in compiler optimization, runtime development, and potentially hardware co-design.
8.2 Ease of Programming / Usability / Developer Experience (DevEx)
The developer experience of using interaction net languages is shaped by the tension between the low-level nature of the underlying model and the abstractions provided by the high-level language.
* Abstraction vs. Low-Level Details:
   * Interaction nets themselves are considered a low-level model, akin to assembly language.13 Programming directly with agents and wires, managing linearity, and handling pattern matching via auxiliary agents is complex and error-prone.18
   * Languages like Bend and Vine aim to provide higher-level abstractions (functions, datatypes, pattern matching, familiar syntax) to hide this complexity.13 Bend explicitly aims for Python-like ease of use 1, while Vine aims for functional/imperative interop with Rust influences.87
   * However, the underlying model still leaks through. Bend's reliance on linked lists, immutable data, and specific constructs like fold/bend requires a different mindset than typical imperative programming.4 Vine's unique concepts like inverse types also present a learning curve.62 The "Python-like" claim for Bend has been questioned due to its static typing and functional nature.4
* Tooling and Ecosystem:
   * Developer experience heavily relies on good tooling (IDEs, debuggers, linters, build systems, package managers) and a supportive ecosystem (libraries, documentation, community).124
   * Bend has a language server providing semantic highlighting and diagnostics for VSCode 128 and syntax highlighting support for highlight.js.129 It uses cargo for installation.2 Documentation includes a guide (GUIDE.md) and feature list.2 A community Discord exists.2
   * Vine also has language server protocol (LSP) support mentioned in its repository structure (lsp directory) 88 and documentation available online.87 It uses cargo for building and has a Discord server.88
   * Inpla provides an interactive Read-Eval-Print Loop (REPL) and batch execution mode, using standard build tools like make, gcc, flex, bison.30 It has sample files and some documentation on its GitHub page.30
   * Overall, the tooling for IN-based languages is still nascent compared to mature language ecosystems. Features like advanced IDE support, robust package management, extensive standard libraries, and graphical debuggers are largely underdeveloped.
8.3 Debugging Support
Debugging programs based on graph rewriting presents unique challenges.
* Challenges: The state of the computation is a potentially large, complex graph. Reduction steps are local and can happen in parallel, making it difficult to follow a single thread of execution or predict the exact state after a parallel step. Identifying the source of errors (e.g., incorrect wiring leading to deadlocks or unexpected normal forms) can be non-trivial. Traditional debugging techniques focused on sequential execution and call stacks may not directly apply.
* Current Tools:
   * Bend: Debugging support seems limited. The primary way to inspect execution appears to be through runtime flags (-s) that provide statistics like reduction counts and timing.2 There is no mention of a dedicated debugger in the main repositories or guides. Error messages might indicate issues like type mismatches or overflows.75
   * Vine: The creator mentioned that developing a debugger is a future plan.62 The compiler provides error messages pointing to line numbers for syntax or type errors.97 Runtime output includes interaction counts and memory usage, which might offer some insight.96
   * Other: Some research implementations mention step-by-step debuggers or visualizers (like Inpla's interactive mode or INblobs) which allow observing the graph reduction process incrementally.7 Tracing interaction paths is also a potential debugging/analysis technique inherent in the model.132
Debugging remains a significant practical hurdle for interaction net programming. The lack of mature, dedicated debugging tools makes finding and fixing errors more challenging than in mainstream languages. Visualizers and step-by-step interpreters can help, but scaling these approaches to complex, parallel reductions is difficult.
9. Research Landscape & Future Directions
The field of interaction net-based programming is an active area of research, characterized by ongoing development, significant challenges, and promising future directions.
9.1 Current State
* Active Development: Interaction nets continue to be explored both as a theoretical model and as a basis for practical programming languages. Languages like Bend 62 and Vine 62 represent recent efforts to build high-level languages on this foundation. Implementations like Inpla 14 and the HVM runtime 74 are under active development, focusing on performance and parallelism. Research continues into implementation techniques 27, extensions to the model 12, and theoretical aspects like type systems.10
* Niche Applications: The primary application area historically has been the efficient and optimal implementation of the lambda calculus.5 More recent projects like Bend aim for broader applicability in general-purpose parallel computing.1 Other potential applications mentioned include modeling distributed systems 42, process calculi 46, and potentially even areas like zero-knowledge proofs.38 However, widespread adoption outside academic research is still limited.
9.2 Key Challenges
Several significant challenges hinder the broader adoption and practical success of interaction net-based programming:
* Performance Optimization: While theoretically promising for parallelism, achieving competitive absolute performance, especially single-core speed, against highly optimized compilers and runtimes for traditional languages remains a major hurdle.4 Efficient compilation, runtime optimization, and effective memory management (including minimizing overhead from explicit duplication/erasure or agent reuse mechanisms) are critical.27 GPU implementation faces specific challenges related to communication bottlenecks and efficient graph representation.35
* High-Level Abstraction: Bridging the gap between the low-level, explicit nature of interaction nets and the abstractions required for productive high-level programming is crucial.13 This involves designing languages with intuitive syntax, powerful features (like expressive pattern matching, modules, rich data types), and standard libraries, while still enabling the underlying runtime to exploit parallelism effectively. Finding the right balance between abstraction and exposing the model's power (as seen in the differing philosophies of Bend and Vine) is key.
* Tooling and Debugging: The lack of mature development tools, particularly sophisticated debuggers, profilers, and package managers, significantly impacts developer productivity and the ability to build and maintain complex applications.62 Debugging parallel graph reductions is inherently difficult.
* Handling Side Effects and Impurity: Pure interaction nets are inherently functional. Integrating necessary impure features like I/O, state management, and exception handling in a way that is both practical for programmers and compatible with the core IN properties (especially confluence) is an ongoing challenge.33 Approaches often involve monadic techniques or specific language extensions.33
* Scalability and Granularity Control: While INs offer fine-grained parallelism, automatically managing this parallelism efficiently across diverse hardware and problem sizes is complex. Determining the optimal granularity for parallel tasks and managing scheduling overhead are important considerations, as naive fine-grained parallelism is not always fastest.64
9.3 Future Directions
Research and development in interaction nets are progressing along several fronts:
* Hardware Acceleration (GPUs and Custom Hardware): A major focus is on efficiently implementing IN runtimes on parallel hardware. This includes improving GPU implementations (like HVM's CUDA backend or exploring alternatives 35) and even investigating the potential for custom hardware architectures specifically designed for interaction net reduction.62
* Language Design and Abstraction: Continued development of higher-level languages like Bend and Vine, focusing on improving usability, expressiveness (e.g., better pattern matching 18, richer type systems), standard libraries, and integrating imperative or stateful features cleanly.2 Exploring different language paradigms built on INs remains an open area.
* Compiler and Runtime Optimization: Improving the performance of existing runtimes (like HVM, IVM, Inpla) through better code generation, more sophisticated memory management (e.g., optimizing agent reuse 27), efficient parallel scheduling algorithms, and reduced overhead for core operations.2 Developing better cost models to guide optimization.13
* Type Systems: Developing more expressive type systems for interaction nets, potentially based on linear logic or dependent types, to enhance correctness guarantees, enable static analysis, and potentially prove termination or complexity bounds.10
* Tooling Development: Creating better IDE support, debuggers specifically designed for parallel graph reduction, profilers, and package ecosystems to improve the practical usability of IN-based languages.62
* Broader Applications: Exploring the application of interaction nets beyond lambda calculus implementation, such as in distributed systems, bioinformatics, AI (potentially via graph representations), or other areas where inherent parallelism and local interaction are beneficial.42
The future of interaction net programming hinges on successfully addressing the challenges of performance, usability, and tooling, while leveraging the model's unique theoretical strengths for parallel computation.
10. Conclusion
Interaction nets, born from the theoretical depths of linear logic, present a fascinating and fundamentally different model of computation based on local graph rewriting. Their core properties – locality, linearity, and strong confluence – inherently support massive parallelism and deterministic execution, offering a compelling theoretical alternative to traditional sequential models like the Turing machine and lambda calculus, especially in the context of modern parallel hardware.
Languages like Bend and Vine represent concrete attempts to harness this potential, albeit with differing philosophies. Bend prioritizes automatic parallelism and ease of use, aiming to abstract the underlying net mechanics behind a Python-like facade, powered by the HVM runtime targeting multi-core CPUs and GPUs. Vine, conversely, embraces its experimental nature, seeking to expose the power of the interaction net model more directly through concepts like inverse types and functional/imperative interoperability, running on its own IVM infrastructure. Other systems like Interaction Combinators distil the model to its universal essence, while the Interaction Calculus provides a crucial textual bridge to symbolic reasoning.
However, the journey from theoretical elegance to practical utility is fraught with challenges. The very constraints that guarantee confluence and parallelism also limit expressiveness, often requiring complex encodings for common programming patterns. Achieving performance that rivals mature, optimized languages remains a significant hurdle, demanding sophisticated compiler, runtime, and memory management techniques – the promise of "GC-free" computation requires careful, non-trivial implementation. Furthermore, the developer experience is hampered by nascent tooling, particularly in debugging and ecosystem support.
Despite these obstacles, the field is vibrant with research activity. Future directions focus intensely on optimizing performance through better compilation and runtime strategies, exploring hardware acceleration via GPUs and custom architectures, designing more expressive and usable high-level languages and type systems, and building the necessary development tools.
In conclusion, interaction nets offer a powerful, parallelism-native computational foundation. While languages like Bend and Vine demonstrate the potential to build practical systems upon this foundation, significant research and engineering effort is still required to overcome performance limitations and improve developer experience. The success of this paradigm will likely depend on continued innovation in runtime systems, compiler technology, language design, and potentially specialized hardware, ultimately determining whether interaction nets can transition from a theoretically compelling model to a mainstream programming paradigm for the parallel future.
Works cited
1. Higher Order Company, accessed on April 20, 2025, https://higherorderco.com/
2. HigherOrderCO/Bend: A massively parallel, high-level programming language - GitHub, accessed on April 20, 2025, https://github.com/HigherOrderCO/Bend
3. Introducing Bend: The Revolutionary Programming Language for Parallel Computing in 2024 - SkillsFoster, accessed on April 20, 2025, https://skillsfoster.com/bend-programming-language-for-parallel-computing/
4. Breaking Bend: Benchmarking the HVM, accessed on April 20, 2025, https://blog.speedfox.co.uk/
5. Interaction nets - Wikipedia, accessed on April 20, 2025, https://en.wikipedia.org/wiki/Interaction_nets
6. A Calculus for Interaction Nets - CiteSeerX, accessed on April 20, 2025, https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e0b35e17d680da34afef2f324c0f3e8b4519639f
7. A Tool for Programming with Interaction Nets - Universidade do Minho, accessed on April 20, 2025, https://webarchive.di.uminho.pt/wiki.di.uminho.pt/twiki/pub/Research/PURe/PURePublications/PUReTR060401.pdf
8. From proof nets to interaction nets - Advances in Linear Logic - Cambridge University Press, accessed on April 20, 2025, https://www.cambridge.org/core/books/advances-in-linear-logic/from-proof-nets-to-interaction-nets/8C2C748AFE97D1E5B7310AB855CCB51B
9. Design and implementation of a low-level language for interaction nets, accessed on April 20, 2025, https://sussex.figshare.com/articles/thesis/Design_and_implementation_of_a_low-level_language_for_interaction_nets/23417312
10. drops.dagstuhl.de, accessed on April 20, 2025, https://drops.dagstuhl.de/storage/00lipics/lipics-vol023-csl2013/LIPIcs.CSL.2013.316/LIPIcs.CSL.2013.316.pdf
11. An Encoding of Interaction Nets in OCaml, accessed on April 20, 2025, https://joerg.endrullis.de/downloads/gcm2024/STAF_2024_paper_72.pdf
12. Extending the Interaction Nets Calculus by Generic Rules - CSE CGI Server, accessed on April 20, 2025, https://cgi.cse.unsw.edu.au/~eptcs/paper.cgi?LINEARITY2012.2.pdf
13. Interaction nets: programming language design and implementation, accessed on April 20, 2025, https://www.user.tu-berlin.de/o.runge/tfs/workshops/gtvmt08/Program/paper_38.pdf
14. An Encoding of Interaction Nets in OCaml - arXiv, accessed on April 20, 2025, https://arxiv.org/pdf/2503.20463
15. An Encoding of Interaction Nets in OCaml - ResearchGate, accessed on April 20, 2025, https://www.researchgate.net/publication/390213798_An_Encoding_of_Interaction_Nets_in_OCaml
16. Interaction nets: programming language design and implementation - CiteSeerX, accessed on April 20, 2025, https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=fc2598c9d7ad0a1f2e5777bcfcbc00366c3d579e
17. core.ac.uk, accessed on April 20, 2025, https://core.ac.uk/download/pdf/82756233.pdf
18. Macros for Interaction Nets, accessed on April 20, 2025, http://www.lsv.fr/Publis/PAPERS/PDF/SM-tg04.pdf
19. Interaction nets: programming language design and implementation - ResearchGate, accessed on April 20, 2025, https://www.researchgate.net/publication/220054026_Interaction_nets_programming_language_design_and_implementation
20. (PDF) Towards a Programming Language for Interaction Nets - ResearchGate, accessed on April 20, 2025, https://www.researchgate.net/publication/222918925_Towards_a_Programming_Language_for_Interaction_Nets
21. (PDF) Compilation of Interaction Nets - ResearchGate, accessed on April 20, 2025, https://www.researchgate.net/publication/222033040_Compilation_of_Interaction_Nets
22. Theory and Applications of Interaction Nets - Computational Logic, accessed on April 20, 2025, http://cl-informatik.uibk.ac.at/isr-2008/pdfs/B/mackie/slides.pdf
23. A Calculus for Interaction Nets Based on the Linear Chemical Abstract Machine - CORE, accessed on April 20, 2025, https://core.ac.uk/download/pdf/82045326.pdf
24. A context semantics for interaction nets, accessed on April 20, 2025, https://dice14.tcs.ifi.lmu.de/abstracts/Perrinel.pdf
25. XXIIVV — interaction nets, accessed on April 20, 2025, https://wiki.xxiivv.com/site/interaction_nets.html
26. Interaction Nets - School of Computing Science, accessed on April 20, 2025, https://www.dcs.gla.ac.uk/~simon/publications/diploma.pdf
27. arxiv.org, accessed on April 20, 2025, https://arxiv.org/pdf/1505.07164
28. Parallel evaluation of interaction nets: some observations and examples - CEUR-WS.org, accessed on April 20, 2025, https://ceur-ws.org/Vol-1403/paper5.pdf
29. core.ac.uk, accessed on April 20, 2025, https://core.ac.uk/download/pdf/81113716.pdf
30. inpla/inpla: Inpla: Interaction nets as a programming ... - GitHub, accessed on April 20, 2025, https://github.com/inpla/inpla
31. Explanation of a simple example of how an interaction net computes?, accessed on April 20, 2025, https://cs.stackexchange.com/questions/160537/explanation-of-a-simple-example-of-how-an-interaction-net-computes
32. Conditional Nested Pattern Matching in Interaction Nets - CSE CGI Server, accessed on April 20, 2025, https://cgi.cse.unsw.edu.au/~eptcs/paper.cgi?DCM2023.6.pdf
33. Realizing Monads in Interaction Nets via Generic Typed Rules - Theory and Logic Group - TU Wien, accessed on April 20, 2025, https://www.logic.tuwien.ac.at/people/gramlich/papers/techrep-e1852-2011-01.pdf
34. (PDF) Interaction Nets vs. the ρ-calculus: Introducing Bigraphical Nets - ResearchGate, accessed on April 20, 2025, https://www.researchgate.net/publication/222823685_Interaction_Nets_vs_the_r-calculus_Introducing_Bigraphical_Nets
35. arxiv.org, accessed on April 20, 2025, https://arxiv.org/pdf/1404.0076
36. What are the ports used for in a practical sense in the Interaction Net paradigm? - Computer Science Stack Exchange, accessed on April 20, 2025, https://cs.stackexchange.com/questions/160555/what-are-the-ports-used-for-in-a-practical-sense-in-the-interaction-net-paradigm
37. A study of two graph rewriting formalisms: Interaction Nets and, accessed on April 20, 2025, https://scispace.com/pdf/a-study-of-two-graph-rewriting-formalisms-interaction-nets-29yp3s79wr.pdf
38. Arithmetization of Functional Program Execution via Interaction Nets in Halo 2, accessed on April 20, 2025, https://eprint.iacr.org/2022/1211.pdf
39. type theory - Is there a "lambda cube" for interaction nets?, accessed on April 20, 2025, https://cstheory.stackexchange.com/questions/33719/is-there-a-lambda-cube-for-interaction-nets
40. inpla/inpla0.02 - Interaction nets as a programming language - GitHub, accessed on April 20, 2025, https://github.com/inpla/inpla0.02
41. An Implementation of Nested Pattern Matching in Interaction Nets - arXiv, accessed on April 20, 2025, https://arxiv.org/pdf/1003.4562
42. A programming model based on Interaction Nets - IEOM, accessed on April 20, 2025, http://ieomsociety.org/dc2018/papers/200.pdf
43. Interaction Nets, Combinators, and Calculus, accessed on April 20, 2025, https://zicklag.github.io/blog/interaction-nets-combinators-calculus/
44. OBSERVATIONAL EQUIVALENCE AND FULL ABSTRACTION IN THE SYMMETRIC INTERACTION COMBINATORS - Logical Methods in Computer Science, accessed on April 20, 2025, https://lmcs.episciences.org/1150/pdf
45. [2503.20463] An Encoding of Interaction Nets in OCaml - arXiv, accessed on April 20, 2025, https://arxiv.org/abs/2503.20463
46. (PDF) Non-deterministic Interaction Nets - ResearchGate, accessed on April 20, 2025, https://www.researchgate.net/publication/238799226_Non-deterministic_Interaction_Nets
47. (PDF) Interaction Nets with McCarthy's amb - ResearchGate, accessed on April 20, 2025, https://www.researchgate.net/publication/220369522_Interaction_Nets_with_McCarthy's_amb
48. Live Science | Introduction to Interaction Nets - YouTube, accessed on April 20, 2025, https://www.youtube.com/watch?v=zCcAg-vcpys
49. AN EXPLICIT FRAMEWORK FOR INTERACTION NETS - Logical Methods in Computer Science, accessed on April 20, 2025, https://lmcs.episciences.org/1108/pdf
50. (PDF) Combinators For Interaction Nets - ResearchGate, accessed on April 20, 2025, https://www.researchgate.net/publication/2498584_Combinators_For_Interaction_Nets
51. From Proof-Nets to Interaction Nets 1 Introduction 2 Multiplicatives - CiteSeerX, accessed on April 20, 2025, https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e18ef7b6e9094e4bfa4514ef31379889f105ecea
52. The Abstract Calculus : r/haskell - Reddit, accessed on April 20, 2025, https://www.reddit.com/r/haskell/comments/99s1pv/the_abstract_calculus/
53. LINEAR BEHAVIOUR OF TERM GRAPH REWRITING PROGRAMS - CiteSeerX, accessed on April 20, 2025, https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=cc34d3f582303fa7b8d7eee4c3934daa50553890
54. [2411.14802] Enhancing a Hierarchical Graph Rewriting Language based on MELL Cut Elimination - arXiv, accessed on April 20, 2025, https://arxiv.org/abs/2411.14802
55. Linear logic and deep learning [pdf] - Hacker News, accessed on April 20, 2025, https://news.ycombinator.com/item?id=16255612
56. AN OVERVIEW OF LINEAR LOGIC PROGRAMMING §1. Introduction. It is now common place to recognize the important role of logic in th, accessed on April 20, 2025, https://www.cs.uoregon.edu/research/summerschool/summer02/lectures/llp.pdf
57. GPT-4 explains linear logic, proof nets, and the geometry of interaction - GitHub Gist, accessed on April 20, 2025, https://gist.github.com/stuhlmueller/1ad19f854287995a673c37ca21f05dec
58. Linear Logic - Stanford Encyclopedia of Philosophy, accessed on April 20, 2025, https://plato.stanford.edu/entries/logic-linear/
59. How should I think about proof nets? - Theoretical Computer Science Stack Exchange, accessed on April 20, 2025, https://cstheory.stackexchange.com/questions/16159/how-should-i-think-about-proof-nets
60. Parallel Implementation Models for the -calculus Using the Geometry of Interaction (Extended Abstract) - Universidade do Minho, accessed on April 20, 2025, https://repositorium.sdum.uminho.pt/bitstream/1822/774/1/parimlcugi-ea.pdf
61. Interaction Systems I: The Theory of Optimal Reductions. - ResearchGate, accessed on April 20, 2025, https://www.researchgate.net/publication/220173276_Interaction_Systems_I_The_Theory_of_Optimal_Reductions
62. Two new graph-based functional programming languages [LWN.net], accessed on April 20, 2025, https://lwn.net/Articles/1011803/
63. r/programming - Bend - a high-level language that runs on GPUs, powered by HVM2, accessed on April 20, 2025, https://www.reddit.com/r/programming/comments/1cu2ymo/bend_a_highlevel_language_that_runs_on_gpus/
64. Bend: a new GPU-native language - Offtopic - Julia Discourse, accessed on April 20, 2025, https://discourse.julialang.org/t/bend-a-new-gpu-native-language/114440
65. Bend/GUIDE.md at main · HigherOrderCO/Bend · GitHub, accessed on April 20, 2025, https://github.com/HigherOrderCO/bend/blob/main/GUIDE.md
66. edusporto/bend - GitHub, accessed on April 20, 2025, https://github.com/edusporto/bend
67. Bend It Like Python, Scale It Like CUDA - Analytics India Magazine, accessed on April 20, 2025, https://analyticsindiamag.com/deep-tech/bend-it-like-python-scale-it-like-cuda/
68. What is Bend Programming Language? - Taazaa, accessed on April 20, 2025, https://www.taazaa.com/glossary/bend-programming-language/
69. I Tried Bend - Sambhav Gupta, accessed on April 20, 2025, https://sambhavg.github.io/blog/bend
70. Higher-Order Virtual Machine (HVM) - Hacker News, accessed on April 20, 2025, https://news.ycombinator.com/item?id=35336113
71. Higher Order Company : r/haskell - Reddit, accessed on April 20, 2025, https://www.reddit.com/r/haskell/comments/10cv2to/higher_order_company/
72. Bend: a high-level language that runs on GPUs (via HVM2) | Hacker News, accessed on April 20, 2025, https://news.ycombinator.com/item?id=40390287
73. HVM-Core: Compiling Python and Haskell to Run Anything on GPUs - E2E Networks, accessed on April 20, 2025, https://www.e2enetworks.com/blog/hvm-core-compiling-python-and-haskell-to-run-anything-on-gpus
74. HigherOrderCO/HVM: A massively parallel, optimal functional runtime in Rust - GitHub, accessed on April 20, 2025, https://github.com/HigherOrderCO/HVM
75. Why does this recursive Function in Bend Hangs with list size > 11? - Stack Overflow, accessed on April 20, 2025, https://stackoverflow.com/questions/78710914/why-does-this-recursive-function-in-bend-hangs-with-list-size-11
76. High-order Virtual Machine (HVM): Massively parallel, optimal functional runtime, accessed on April 20, 2025, https://news.ycombinator.com/item?id=30219452
77. Massimult: A Novel Parallel CPU Architecture Based on Combinator Reduction - arXiv, accessed on April 20, 2025, https://arxiv.org/html/2412.02765v1
78. Massimult: A Novel Parallel CPU Architecture Based on Combinator Reduction - arXiv, accessed on April 20, 2025, https://www.arxiv.org/pdf/2412.02765
79. Multiverse: Easy Conversion of Runtime Systems into OS Kernels via Automatic Hybridization - arXiv, accessed on April 20, 2025, https://arxiv.org/pdf/1901.06360
80. Platform-Agnostic Steal-Time Measurement in a Guest Operating System - arXiv, accessed on April 20, 2025, https://arxiv.org/pdf/1810.01139
81. Kindelia/ROADMAP.md at master - GitHub, accessed on April 20, 2025, https://github.com/HigherOrderCO/Kindelia/blob/master/ROADMAP.md
82. HVM: a next-gen massively parallel, beta-optimal functional runtime is 50x faster than its predecessors : r/haskell - Reddit, accessed on April 20, 2025, https://www.reddit.com/r/haskell/comments/shewq7/hvm_a_nextgen_massively_parallel_betaoptimal/
83. HVM2, a parallel runtime written in Rust, is now production ready, and runs on GPUs! Bend is a Pythonish language that compiles to it. - Reddit, accessed on April 20, 2025, https://www.reddit.com/r/rust/comments/1cu558j/hvm2_a_parallel_runtime_written_in_rust_is_now/
84. The Abstract Calculus now has a simpler specification and an accompanying implementation : r/haskell - Reddit, accessed on April 20, 2025, https://www.reddit.com/r/haskell/comments/9b3yks/the_abstract_calculus_now_has_a_simpler/
85. VictorTaelin/Interaction-Calculus: A programming language and model of computation that matches the optimal λ-calculus reduction algorithm perfectly. - GitHub, accessed on April 20, 2025, https://github.com/VictorTaelin/Interaction-Calculus
86. [Fireship] Mind-bending new programming language for GPUs just dropped... - Reddit, accessed on April 20, 2025, https://www.reddit.com/r/programming/comments/1cuh4pg/fireship_mindbending_new_programming_language_for/
87. New Programming Language Vine Based on Interaction Nets - InfoQ, accessed on April 20, 2025, https://www.infoq.com/news/2025/03/new-programming-language-vine/
88. VineLang/vine: an experimental new programming language based on interaction nets, accessed on April 20, 2025, https://github.com/VineLang/vine
89. Vine - Programming language - PLDB, accessed on April 20, 2025, http://pldb.io/concepts/vine.html
90. New Programming Language Vine Based on Interaction Nets | daily, accessed on April 20, 2025, https://app.daily.dev/posts/new-programming-language-vine-based-on-interaction-nets-gpolvukej
91. vine/README.md at main · VineLang/vine - GitHub, accessed on April 20, 2025, https://github.com/VineLang/vine/blob/main/README.md
92. Trombecher/vine: The Vine Programming Language - GitHub, accessed on April 20, 2025, https://github.com/Trombecher/vine
93. The Vine Programming Language - GitHub, accessed on April 20, 2025, https://github.com/VineLang
94. The Vine Programming Language, accessed on April 20, 2025, https://vine.dev/
95. Introduction - The Vine Programming Language, accessed on April 20, 2025, https://vine.dev/docs/
96. Vine: A programming language based on Interaction Nets | Hacker News, accessed on April 20, 2025, https://news.ycombinator.com/item?id=43144040
97. xdanger/try-vine - GitHub, accessed on April 20, 2025, https://github.com/xdanger/try-vine
98. Ivy and the IVM - The Vine Programming Language, accessed on April 20, 2025, https://vine.dev/docs/ivy/
99. Interaction combinators - GitHub Gist, accessed on April 20, 2025, https://gist.github.com/cheery/805d7a9f09cca592f4d310defe1ff8e8
100. Graph rewriting - Wikipedia, accessed on April 20, 2025, https://en.wikipedia.org/wiki/Graph_rewriting
101. Adventures in testing my conceptual term graph rewriting system - Reddit, accessed on April 20, 2025, https://www.reddit.com/r/ProgrammingLanguages/comments/1deb74u/adventures_in_testing_my_conceptual_term_graph/
102. [microblog] Reasoner.js: a functional-logic framework for automated reasoning - Reddit, accessed on April 20, 2025, https://www.reddit.com/r/ProgrammingLanguages/comments/17wq0v3/microblog_reasonerjs_a_functionallogic_framework/
103. Interaction Nets vs. the ρ-calculus: Introducing Bigraphical Nets - CiteSeerX, accessed on April 20, 2025, https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=0e08860671b9809439dbaa3b81c509f78ad56d47
104. Efficient λ Evaluation with Interaction Nets | Request PDF - ResearchGate, accessed on April 20, 2025, https://www.researchgate.net/publication/251314634_Efficient_l_Evaluation_with_Interaction_Nets
105. [2409.11015] Introducing Quantification into a Hierarchical Graph Rewriting Language, accessed on April 20, 2025, https://arxiv.org/abs/2409.11015
106. Inpla - GitHub, accessed on April 20, 2025, https://github.com/inpla
107. (PDF) A Tool for Programming with Interaction Nets - ResearchGate, accessed on April 20, 2025, https://www.researchgate.net/publication/220369597_A_Tool_for_Programming_with_Interaction_Nets
108. (PDF) An Implementation Model for Interaction Nets - ResearchGate, accessed on April 20, 2025, https://www.researchgate.net/publication/277291269_An_Implementation_Model_for_Interaction_Nets
109. A token-passing net implementation of optimal reduction with embedded read-back - arXiv, accessed on April 20, 2025, https://arxiv.org/pdf/1512.02995
110. Why the overall lack of interest in Interaction Combinators? : r/haskell - Reddit, accessed on April 20, 2025, https://www.reddit.com/r/haskell/comments/568gtk/why_the_overall_lack_of_interest_in_interaction/
111. Why isn't anyone talking about optimal lambda calculus implementations? - Reddit, accessed on April 20, 2025, https://www.reddit.com/r/haskell/comments/2zqtfk/why_isnt_anyone_talking_about_optimal_lambda/
112. Understanding Garbage Collection in .NET: How to Optimize Memory Management, accessed on April 20, 2025, https://dev.to/leandroveiga/understanding-garbage-collection-in-net-how-to-optimize-memory-management-3cj2
113. Understanding garbage collection: A developer's guide to memory management - Aerospike, accessed on April 20, 2025, https://aerospike.com/blog/understanding-garbage-collection/
114. Fundamentals of garbage collection - .NET | Microsoft Learn, accessed on April 20, 2025, https://learn.microsoft.com/en-us/dotnet/standard/garbage-collection/fundamentals
115. Garbage Collection: Automatic Memory Management in the Microsoft .NET Framework, accessed on April 20, 2025, https://learn.microsoft.com/en-us/archive/msdn-magazine/2000/november/garbage-collection-automatic-memory-management-in-the-microsoft-net-framework
116. Advanced Memory Management Techniques in C# - TechOnDiapers, accessed on April 20, 2025, https://tech-on-diapers.hashnode.dev/advanced-memory-management-techniques-in-c
117. Memory management and resource management in .NET - Alter Solutions, accessed on April 20, 2025, https://www.alter-solutions.com/articles/memory-resource-management-net
118. Memory management : r/csharp - Reddit, accessed on April 20, 2025, https://www.reddit.com/r/csharp/comments/1bglvx4/memory_management/
119. [1505.07164] An Implementation Model for Interaction Nets - arXiv, accessed on April 20, 2025, https://arxiv.org/abs/1505.07164
120. String diagrams for the lambda-calculus? - Robin Piedeleu, accessed on April 20, 2025, https://piedeleu.com/posts/diagrammatic-lambda-calculus/
121. Is there an efficient algorithm to check for duplicator-invariant equivalence on symmetric interaction combinators? - Theoretical Computer Science Stack Exchange, accessed on April 20, 2025, https://cstheory.stackexchange.com/questions/52873/is-there-an-efficient-algorithm-to-check-for-duplicator-invariant-equivalence-on
122. Lambda calculus - Wikipedia, accessed on April 20, 2025, https://en.wikipedia.org/wiki/Lambda_calculus
123. 2 Lambda Calculus: History and Syntax - Diderot, accessed on April 20, 2025, https://www.diderot.one/courses/22/books/78/chapter/759
124. Interaction Developer Job Description | Celarity, accessed on April 20, 2025, https://www.celarity.com/job-description/interaction-developer/
125. Developer Experience: Importance, Metrics, And 6 Ways To Improve - Octopus Deploy, accessed on April 20, 2025, https://octopus.com/devops/developer-experience/
126. Developer Experience Infrastructure (DXI) - Kenneth Auchenberg, accessed on April 20, 2025, https://kenneth.io/post/developer-experience-infrastructure-dxi
127. Web Developer Tools Employers Want to See on Your Resume | Rasmussen University, accessed on April 20, 2025, https://www.rasmussen.edu/degrees/technology/blog/web-developer-tools-employers-want-to-see-on-your-resume/
128. HigherOrderCO/bend-language-server - GitHub, accessed on April 20, 2025, https://github.com/HigherOrderCO/bend-language-server
129. Highlight.js grammar for Bend - GitHub, accessed on April 20, 2025, https://github.com/highlightjs/highlightjs-bend
130. Debugging Symbols with .NET - Intergraph Smart 3D - Update 2 - Troubleshooting, accessed on April 20, 2025, https://docs.hexagonppm.com/r/en-US/Intergraph-Smart-3D-Troubleshooting-Reference/14/37151
131. Language, interpreter and compiler for interaction nets - GitHub, accessed on April 20, 2025, https://github.com/szeiger/interact
132. Show HN: iNet – A programming language for interaction nets - Hacker News, accessed on April 20, 2025, https://news.ycombinator.com/item?id=37406742
133. Interaction Net as a Representation Model of a Programming Language | Request PDF, accessed on April 20, 2025, https://www.researchgate.net/publication/330446536_Interaction_Net_as_a_Representation_Model_of_a_Programming_Language
134. Bend - a high-level language that runs on GPUs (powered by HVM2) : r/ProgrammingLanguages - Reddit, accessed on April 20, 2025, https://www.reddit.com/r/ProgrammingLanguages/comments/1cu45ks/bend_a_highlevel_language_that_runs_on_gpus/
135. Linearity/TLLA Program - EasyChair, accessed on April 20, 2025, https://easychair.org/smart-program/FLoC2018/LinearityTLLA-program.html