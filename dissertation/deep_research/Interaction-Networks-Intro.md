Interaction Network Reduction: A Computational Paradigm for Parallelism and Optimality
1. Introduction to Interaction Network Reduction
Interaction networks represent a distinct paradigm in the landscape of computational models, offering a graphical approach based on localized graph rewriting.1 Introduced by Yves Lafont in 1990 1, they emerged initially as a computational interpretation and generalization of the proof structures found in Jean-Yves Girard's Linear Logic.5 Unlike traditional models such as Turing Machines or the Lambda Calculus, interaction nets define computation through the interaction of autonomous entities called agents, governed by simple, local rules.
This model operates at a fundamental level, often considered a low-level computational formalism 2 that can serve multiple purposes. It can function as an object language for compiling higher-level programming languages, particularly functional ones based on the lambda calculus.3 Alternatively, interaction nets themselves, augmented with appropriate structures, can form the basis of novel programming languages.2 Despite their structural simplicity, interaction nets possess the power of universal computation, meaning they are Turing complete.1
The driving motivations behind the development and continued research into interaction nets stem from their inherent potential to address long-standing challenges in computation:
1. Inherent Parallelism: The computational steps (reductions) in interaction nets are strictly local, involving only pairs of interacting agents and their immediate connections. This locality, combined with strong confluence properties, means that reductions occurring in different parts of the net do not interfere with each other.1 Consequently, interaction nets offer a natural model for massive parallelism, potentially allowing computations to be distributed across numerous processing units (like CPU cores or GPU threads) without complex synchronization mechanisms or explicit parallel programming constructs.5 This contrasts sharply with conventional models where achieving safe and efficient parallelism often requires significant programmer effort and sophisticated runtime support.8
2. Optimal Reduction: Interaction nets provide a framework for realizing optimal reduction strategies, particularly relevant for the evaluation of lambda calculus terms.1 Optimality, often discussed in the context of Lévy-optimality, refers to the avoidance of redundant computations by sharing the results of evaluated sub-expressions.4 Interaction nets, through their graphical nature and specific rule systems (like interaction combinators), can explicitly represent and manage this sharing, potentially leading to exponentially faster evaluation for certain classes of higher-order programs compared to traditional reduction strategies.21
3. Implicit Memory Management: The structure and dynamics of interaction nets, particularly systems based on interaction combinators, align closely with concepts from linear logic regarding resource usage. This often allows for memory management to be handled implicitly through the reduction rules themselves, specifically via annihilation and erasure rules that consume agents.5 This potentially obviates the need for traditional garbage collection mechanisms, which can introduce unpredictable pauses and overhead in high-performance systems.5
This report provides a comprehensive analysis of interaction network reduction systems. It begins by detailing the fundamental components and operational principles of interaction nets. It then delves into their theoretical underpinnings, including their relationship to other computational models and their key formal properties. Core operational techniques, such as parallel execution strategies, memory management approaches, optimality properties, and the role of universal rule sets like interaction combinators, are examined. The report presents specific programming languages and runtime systems based on this paradigm, notably HVM, Inpla, Bend, and Vine, offering a comparative analysis. Compilation techniques for translating higher-level code are investigated, followed by an exploration of current and potential application domains. Finally, the report summarizes the current state of research, identifies outstanding challenges, and discusses future directions in the field.
2. Fundamentals of Interaction Nets
Interaction nets provide a graphical language for computation built upon a small set of core concepts: agents, ports, wires, and interaction rules that govern the dynamic rewriting of the net structure.
2.1. Agents, Ports, and Wires: The Static Structure
The basic building blocks of an interaction net are agents.1 Agents can be visualized as nodes in a graph. Each agent belongs to a specific type, identified by a symbol (e.g., α, β, γ) drawn from a predefined set Σ, often referred to as the signature of the interaction system.25
Agents connect to each other via ports.1 Each agent possesses exactly one distinguished principal port, often depicted graphically with an arrow pointing towards or away from the agent.1 This port is crucial as it dictates where interactions can occur. In addition to the principal port, an agent has a fixed number of auxiliary ports, determined by the agent's symbol. The number of auxiliary ports is called the agent's arity, denoted ar(α) for an agent of type α.1 An agent with arity n has n auxiliary ports (and one principal port, for a total of n+1 ports).
Connections between ports are established by wires, which correspond to the edges of the graph.1 Each wire connects exactly two ports. A fundamental rule is that any given port can be connected to at most one wire. Ports that are not connected to any wire are termed free ports. The collection of all free ports in a net constitutes its interface, representing the points where the net can be connected to other nets or interact with an external environment.1 A net might consist solely of wiring (no agents) or even be empty.13
The strict distinction between the single principal port, used for triggering interactions, and the auxiliary ports, which primarily serve as pass-through connection points during rewrites, is a cornerstone of the interaction net model. This design choice fundamentally enables the locality and determinism that characterize the reduction process. The principal port acts as the control point for computation, while auxiliary ports manage the data flow or structural connections without initiating interaction themselves.1
2.2. Active Pairs, Interaction Rules, and Reduction: The Dynamic Process
Computation in interaction nets occurs through a process called reduction, which involves rewriting specific patterns within the net.1 The trigger for reduction is an active pair: a pair of agents connected to each other via their principal ports.1 An active pair is the interaction net analogue of a redex (reducible expression) in term rewriting or lambda calculus.
The transformation that occurs when an active pair is reduced is defined by a set of interaction rules specific to the interaction system.1 A rule typically takes the form ((α,β)→N) or α⋈β⇒N, indicating that an active pair consisting of agents with symbols α and β is replaced by a new, predefined net N.
These rules are subject to strict constraints that ensure the well-behaved nature of the computation:
1. Interface Preservation: The net N that replaces the active pair (α,β) must have a set of free ports that exactly matches the set of auxiliary ports of the original agents α and β. When the replacement occurs, the wires originally connected to the auxiliary ports of α and β are reconnected to the corresponding free ports of N. No new free ports can be created in this process.1
2. Uniqueness: For any pair of symbols (α,β), there can be at most one interaction rule defined in the system.1 This eliminates non-determinism in rule choice.
3. Symmetry: If a rule exists for an agent interacting with itself ((α,α)→N), the resulting net N must possess a corresponding symmetry.1
The reduction process involves repeatedly finding any active pair within the net and applying its corresponding interaction rule. This continues until no more active pairs exist in the net. A net containing no active pairs is said to be in normal form.5
The interface preservation constraint is particularly critical. It guarantees that each reduction step is a purely local modification that seamlessly integrates the replacement net N into the surrounding context without disrupting the overall graph structure.1 This property ensures that reduction steps are composable and maintain the integrity of the net throughout the computation, preventing the reduction process from creating dangling wires or invalid configurations.
2.3. Interaction Nets as Constrained Graph Rewriting
Fundamentally, interaction nets constitute a specific and highly constrained instance of graph rewriting systems.1 The net itself is the graph, agents are the nodes, wires are the edges, and interaction rules define the graph transformations.
However, interaction nets differ significantly from general graph rewriting due to the stringent constraints imposed on the rules and structure:
* Interaction is strictly limited to pairs of agents connected via their unique principal ports.
* Rules must preserve the interface (the set of external connections via auxiliary ports).
* There is at most one rule per pair of interacting agent types.
These constraints are not arbitrary limitations but are carefully designed to yield desirable computational properties, most notably confluence and locality, which are often difficult to guarantee in more general graph rewriting frameworks.1 By restricting the rewriting mechanism in this way, interaction nets provide a framework where complex computations can emerge from simple, local, and inherently parallelizable interactions.
3. Theoretical Foundations and Formal Properties
Interaction nets are not merely a practical implementation technique but are grounded in deep theoretical concepts, primarily stemming from linear logic and possessing significant formal properties that underpin their computational behavior.
3.1. Relationship to Linear Logic
The genesis of interaction nets is deeply intertwined with Girard's Linear Logic.5 Lafont developed them as a concrete computational model derived from the proof nets of multiplicative linear logic.12 In this correspondence, agents can be seen as representing inference rules of the logic, while wires correspond to formulas or propositions being passed between rules.17 The connections and interactions between agents mirror the structure and dynamics of proof construction and cut elimination in linear logic.
Linear logic's sensitivity to resource usage (propositions cannot be arbitrarily duplicated or discarded) finds a natural echo in the structure of interaction nets. Wires represent specific connections, and agents are explicitly consumed and produced according to the interaction rules. Operations like copying or discarding data require specific agents (like the duplication and erasure combinators, δ and ε) rather than being implicit assumptions of the model. This explicit handling of resources is a key aspect that distinguishes interaction nets (and linear logic) from classical computational models.
3.2. Encoding Lambda Calculus
One of the most significant applications and areas of study for interaction nets has been their use as an implementation substrate for the lambda calculus, the foundational model for functional programming.1 Various schemes have been developed to translate lambda terms into interaction nets, where beta-reduction (the core computational step in lambda calculus) is simulated by sequences of interaction rule applications.26
A major focus of this work has been on achieving optimal reduction.1 Optimal reduction, in the sense defined by Jean-Jacques Lévy, aims to avoid redundant computations by sharing the results of evaluating common sub-expressions, even when those sub-expressions are created dynamically during reduction (e.g., inside lambda abstractions). Standard lambda calculus evaluators often struggle with this, potentially performing exponential amounts of redundant work for certain terms. Interaction nets, with their graphical representation that makes sharing explicit, provide a natural framework for implementing optimal reduction strategies, such as those based on Lamping's work.27 Systems like the Bologna Optimal Higher-Order Machine (BOOHM) 5 and, more recently, HVM 21 are implementations pursuing this goal. HVM, in particular, claims to achieve beta-optimality, suggesting it can evaluate certain higher-order functions exponentially faster than conventional runtimes like GHC.21
Different strategies exist for encoding lambda calculus, including those based on explicit substitution 15 and token-passing mechanisms.25 However, challenges remain. Fully capturing all aspects of lambda calculus semantics (like handling different evaluation strategies or potential non-termination issues) within the interaction net framework can be complex, and achieving practical, efficient optimal reduction remains an active area of research and engineering.22 Some implementations might impose restrictions or exhibit different performance characteristics depending on the encoding choices.36
The pursuit of optimal lambda calculus reduction via interaction nets highlights a fundamental alignment. The graphical nature of interaction nets directly addresses the challenge of representing and managing shared computations, a task that is often awkward or inefficient in purely textual representations like the lambda calculus itself.22 The explicit graph structure, combined with agents like duplicators (δ), allows interaction net systems to directly manipulate shared subgraphs, implementing sharing mechanisms that are central to optimal reduction strategies.4 Implementations aiming for optimality leverage this inherent capability 1, attempting to realize the theoretical efficiency gains promised by Lévy's work in a practical runtime.21
3.3. Key Formal Properties
Interaction nets possess several crucial formal properties that stem directly from their constrained definition and underpin their computational advantages:
* Strong Confluence (Church-Rosser / Diamond Property): Interaction net reduction is strongly confluent, often referred to as satisfying the one-step diamond property.1 This means that if a net N can be reduced in one step to two different nets P and Q (by applying rules to two different active pairs), then both P and Q can be reduced (potentially in multiple steps, though often also in one step) to a common net R. This property is typically guaranteed by the strict constraints on interaction rules.9 The primary consequence of confluence is determinism: the final result (normal form) of a computation is independent of the order in which the reduction steps are applied. This holds even if reductions are performed in parallel.1
* Locality: As previously mentioned, interaction rules only affect the active pair being reduced and the immediate wiring of their auxiliary ports.1 Reductions occurring in disjoint parts of the net are completely independent and cannot interfere with each other.
* Linearity (Constant-Time Rules): Each application of an interaction rule involves replacing the active pair with the predefined net N and performing the necessary rewiring. This operation typically takes an amount of time and resources that depends only on the size of the rule (α,β)→N, not on the size of the overall net.5 This property is crucial for predictable performance and allows for the development of meaningful cost models for computation based purely on the number of interactions.
The synergy between strong confluence and locality forms the theoretical foundation for the massive parallelism inherent in interaction nets. Confluence guarantees that concurrently executing reductions, regardless of their order or interleaving, will always yield the same deterministic result.1 Locality ensures that these concurrent reductions do not interfere with each other's execution unless they are competing for the exact same agents.1 This combination means that any available active pair in the net can be reduced in parallel without requiring complex locking, synchronization, or scheduling logic that burdens traditional parallel programming models. This potential for automatic and safe parallelism is one of the most compelling aspects of the interaction net paradigm.5
4. The Interaction Calculus: A Textual Formalism
While the graphical representation of interaction nets provides intuition, a textual formalism is essential for rigorous analysis, implementation, and specification. The Interaction Calculus serves this purpose, providing an algebraic way to represent nets and their reduction dynamics.4
4.1. Representing Nets Textually
The Interaction Calculus represents nets using terms, names, equations, and configurations:
* Terms (t,s,u): Terms represent the building blocks within the net. A term can be either:
   * An agent application: α(t1​,...,tn​), where α is an agent symbol from the signature Σ, n is the arity of α, and t1​,...,tn​ are the terms connected to its auxiliary ports.4
   * A name: x,y,z, representing a port.4
* Names (x,y,z): Names correspond to the ports or the ends of wires in the graphical representation.4 A crucial syntactic constraint is that in any valid configuration, every name must occur exactly twice.4 One occurrence might be as a standalone name (representing a free port in the interface or a variable binder), while the other might be within a term (representing a connection to an auxiliary port) or in an equation. This rule directly enforces the point-to-point wiring discipline of the graphical nets. Configurations are considered equivalent up to α-conversion, the consistent renaming of pairs of name occurrences.5
* Equations (t=s): Equations represent the wires connecting ports.4 An equation t=s typically signifies a connection between the principal ports of the agents represented by terms t and s, or a connection between a principal port and an auxiliary port/interface name. Equations are symmetric (t=s is equivalent to s=t).
* Configurations: A configuration represents the entire state of an interaction net at a given time. It consists of two parts:
   * An interface: An ordered sequence of terms or names (t~) representing the free ports of the net.34
   * A multiset of equations: Δ,Θ, representing the internal connections and active pairs within the net.4 A configuration is typically written as ⟨t~∣Δ⟩.4
The "exactly two occurrences" rule for names is fundamental. It mirrors the graphical constraint that every wire connects precisely two ports.1 This enforces linearity or affinity at the structural level 32: connections are inherently point-to-point. Implicit fan-out (copying a value to multiple destinations) or fan-in (merging multiple connections) is disallowed unless explicitly mediated by agents defined for those purposes, such as the δ (duplicator) and ε (eraser) interaction combinators.5 This explicit resource management is a key characteristic inherited from linear logic.
4.2. Reduction Semantics
The dynamics of the Interaction Calculus are defined by reduction rules that transform configurations. The two primary types of rules are interaction and indirection/substitution:
* Interaction Rule (int): This rule models the application of a graphical interaction rule. If the system has a rule defined for the active pair (α,β), often written using Lafont's notation as α[x1​,...,xm​]⋈β[y1​,...,yn​]⇒Θ (where Θ is the set of equations representing the right-hand side net N) 5, then a configuration containing the corresponding equation can be reduced:
⟨t~∣α(t1​,...,tm​)=β(u1​,...,un​),Δ⟩int​⟨t~∣Θ′,Δ⟩
where Θ′ is obtained from Θ by substituting the terms t1​,...,tm​ for the names x1​,...,xm​ and u1​,...,un​ for y1​,...,yn​ (after appropriate renaming of bound variables in Θ).4 This step replaces the equation representing the active pair with the set of equations defining the structure of the replacement net.
* Indirection/Substitution Rule(s): These rules handle equations of the form x=t, where x is a name and t is a term. They propagate connections by substituting t for the other occurrence of the name x within the configuration.4 Different presentations of the calculus might use slightly different sets of substitution rules (e.g., the "lightweight calculus" in 4 uses communication, substitution, and collect rules). A general form is:
⟨...s[x]...∣x=t,Δ⟩sub​⟨...s[t/x]...∣Δ⟩
or when substituting into another equation:
⟨t~∣x=t,s[x]=u,Δ⟩sub​⟨t~∣s[t/x]=u,Δ⟩
where s[x] denotes a term s containing an occurrence of the name x, and s[t/x] denotes the result of substituting t for that occurrence. This process effectively "follows the wire" represented by x=t.
Together, these rules define the reduction relation → on configurations. A configuration reduces to its normal form c′ (containing no equations corresponding to active pairs) is often denoted c↓c′.5 Some interaction systems might also include extensions, such as the non-deterministic choice agent amb 5 or specialized calculi like the directed interaction calculus.27
The Interaction Calculus serves as an essential bridge between the intuitive graphical model and the needs of formal analysis and practical implementation. While graphs are visually appealing 1, direct manipulation can be cumbersome. The calculus provides a symbolic framework 5 suitable for defining precise operational semantics 15, proving properties, and serving as a target for compilers or the basis for interpreters written in conventional programming languages.1 It makes the two phases of reduction—rewriting an active pair (interaction) and propagating the resulting connections (indirection)—explicit and distinct.4
5. Core Techniques and Operational Features
Interaction network runtimes and languages employ several core techniques stemming from the model's fundamental properties to achieve parallelism, efficient memory management, and potentially optimal evaluation.
5.1. Exploiting Inherent Parallelism
The combination of locality and strong confluence is the key enabler of parallelism in interaction nets.1 Since reductions are local and the final result is independent of the reduction order, any active pairs present in the net can, in principle, be reduced simultaneously without risk of data races or incorrect results. This allows for automatic parallelism – the runtime system can potentially parallelize execution without explicit directives from the programmer.
Practical runtime systems exploit this by identifying active pairs and distributing the work of reducing them across available processing units. Strategies may include:
   * Maintaining a pool of active pairs and assigning them to worker threads or cores.37
   * Prioritizing reductions based on their position in the net; HVM, for instance, reportedly prioritizes reducing active pairs (redexes) that are closer to the root of the program's normal form structure.31
   * Targeting massively parallel hardware like Graphics Processing Units (GPUs) using frameworks like CUDA.8 This approach aims to leverage the thousands of cores available on modern GPUs to achieve significant speedups for computations with sufficient inherent parallelism. HVM, in particular, has reported substantial performance gains (measured in billions of rewrites per second) by moving from CPU to GPU execution.20
However, realizing the theoretical potential of automatic parallelism in practice presents significant engineering challenges. The runtime must efficiently discover active pairs across potentially large and complex net structures, manage the overhead associated with distributing work and communicating results between parallel units, and ensure effective load balancing.31 The performance gains are ultimately dependent on the inherent parallelism within the computation itself and the runtime's ability to effectively map this onto the specific hardware architecture.8 Historical difficulties in achieving practical speedups 38 underscore the complexity involved, while modern efforts like HVM demonstrate that significant progress is being made through careful runtime design and hardware targeting.20
5.2. Memory Management: Linearity and Garbage-Free Execution
A notable feature often associated with interaction nets, especially those based on interaction combinators, is the potential for garbage-free execution, meaning computation can proceed without relying on a traditional garbage collector (GC) to reclaim unused memory.9
This property arises from the explicit resource handling embedded within the interaction rules themselves, reflecting the linearity principles of the underlying logical foundations. Agents and connections are not implicitly duplicated; operations like copying or discarding require specific agents:
   * Duplication: The δ (delta) combinator explicitly duplicates a connection, allowing a value or substructure to be used in multiple places.5
   * Erasure: The ε (epsilon) combinator explicitly discards a connection and the substructure attached to it.5
Furthermore, annihilation rules, where two interacting agents destroy each other (e.g., γ⋈γ→wiring, or ε⋈α→wiring/εs), effectively reclaim the memory occupied by those agents as part of the computational step itself.5 Memory management becomes an integral part of the reduction process, handled locally by the rules, rather than a separate, global GC phase that periodically scans memory.
This approach offers potential advantages in performance and predictability by avoiding GC pauses.30 HVM explicitly promotes this feature, drawing parallels to Rust's ownership system which also achieves memory safety without a GC.30 However, this "garbage-free" property is not automatic in all interaction net systems. It relies on the specific set of rules being used (particularly the presence and behavior of erasure and annihilation rules) and the guarantee that computations eventually reduce structures containing garbage into active pairs involving erasure or annihilation. Poorly designed rule sets could potentially lead to memory leaks if unreachable cycles are created that contain no active pairs. Thus, the burden shifts from a runtime GC to the careful design of the interaction system itself.
5.3. Achieving Optimal Reduction
Interaction nets provide a promising framework for implementing optimal reduction strategies for the lambda calculus.1 The goal of optimal reduction is to perform the minimal number of computational steps required to reach the normal form, primarily by ensuring that identical sub-computations are performed only once, sharing the result wherever it's needed.4
Traditional lambda calculus evaluators often duplicate work, especially when substituting expressions containing redexes into multiple locations or when dealing with computations under lambda abstractions.22 Interaction nets, through their explicit graph structure, can naturally represent shared sub-structures. Rules involving the duplication agent (δ) allow controlled sharing, and the reduction process inherently propagates results through the graph structure. This allows interaction net implementations to more directly encode optimal evaluation strategies like Lamping's abstract algorithm, which relies heavily on managing shared graph structures (control nodes, fan nodes).15
HVM explicitly claims to be beta-optimal 21, suggesting it implements such a strategy. This optimality can lead to significant, even exponential, performance improvements for certain classes of programs, particularly those involving higher-order functions or complex recursion patterns where traditional methods would perform redundant computations.21 For instance, techniques like "deforestation" (eliminating intermediate data structures like lists) can sometimes emerge automatically from the optimal reduction process in interaction nets, without requiring specific compiler pragmas as in Haskell.22
5.4. Interaction Combinators: A Universal Base System
A specific, minimal interaction system known as Interaction Combinators plays a crucial role, particularly in modern implementations like HVM.18 This system typically consists of only three agent types (though variations exist) 5:
   1. Constructor (γ or ()): Often used to build data structures or represent lambda abstractions/applications. Typically has arity 2.
   2. Duplicator (δ or {}): Used to explicitly duplicate connections, enabling sharing. Typically has arity 2.
   3. Eraser (ε or *): Used to explicitly discard connections and associated sub-nets. Typically has arity 0.
The interactions between these agents are defined by a small set of rules 5:
   * Annihilation: When identical agents interact, they destroy each other, potentially simplifying the net or just creating wiring (e.g., γ⋈γ→wiring, δ⋈δ→wiring, ε⋈ε→empty).
   * Commutation: When different agents interact, they typically "pass through" each other, potentially duplicating or erasing parts of the structure according to their function (e.g., δ duplicates the structure connected to the other agent, ε erases it). Specific rules define how γ, δ, and ε interact with each other.
Despite their simplicity, interaction combinators are universal, meaning any computation that can be performed by any interaction net system (and thus any Turing-computable function) can be encoded using only these three agents and their interaction rules.7
Interaction combinators serve as a foundational layer, analogous to a reduced instruction set computing (RISC) architecture for interaction nets. They provide a minimal, universal set of operations onto which more complex agent behaviors or higher-level language constructs can be compiled.19 This simplifies the design and optimization of the core runtime engine (like HVM's core evaluator 39), as it only needs to implement a small, fixed set of interaction rules very efficiently. The complexity of handling diverse language features is then shifted to the compiler that translates those features into the appropriate patterns of interaction combinators.
6. Interaction Network Languages and Runtimes
While interaction nets were initially conceived as a theoretical model 2, significant effort has gone into developing practical implementations, ranging from interpreters and visual tools to high-performance runtimes and dedicated programming languages.
6.1. Overview of Key Systems and Implementations
Lafont's original vision included interaction nets as a potential programming language paradigm.2 Over the years, various implementations have emerged, though many early attempts faced challenges with execution speed or lacked features expected of modern programming environments.3
Notable implementations and tools mentioned in the literature include:
   * Interpreters and Visualizers: Tools like INblobs 5, InTwo 30, GraphPaper 28, and a JavaScript engine 5 provide environments for creating, visualizing, and executing interaction nets, often for educational or research purposes.
   * Optimal Evaluators: The Bologna Optimal Higher-Order Machine (BOOHM) 5 was a significant research project focused on implementing optimal lambda calculus evaluation using interaction nets.
   * Embeddings: Early experiments involved embedding interaction net concepts within existing functional languages like Concurrent Haskell or OCaml.1
   * Compilers to C: Techniques for compiling interaction nets to C for improved performance and portability were developed.3
More recently, several dedicated runtimes and languages have gained prominence, aiming to harness the parallelism and efficiency potential of interaction nets more effectively.
6.2. Case Study: HVM (Higher-order Virtual Machine)
HVM stands for Higher-order Virtual Machine and represents a major contemporary effort in the field, developed and supported by the Higher Order Company.18
   * Goals: HVM aims to be a massively parallel runtime environment primarily for functional languages, leveraging the principles of interaction nets to achieve optimal reduction (specifically beta-optimality for lambda calculus encodings) and near-ideal speedups on multi-core and GPU hardware.7
   * Architecture: HVM (specifically HVM2, the successor to an earlier prototype HVM1 19) is built upon an extended version of Interaction Combinators.19 The core runtime is implemented in Rust 19, providing a low-level intermediate representation (IR) for interaction nets.19
   * Parallel Execution: A key focus of HVM is exploiting parallelism. It compiles its IR to C for efficient execution on multi-core CPUs and, significantly, to CUDA for execution on NVIDIA GPUs.8 HVM employs specific strategies for distributing reduction tasks across available cores/threads, reportedly achieving very high interaction rates (billions of rewrites per second) on GPUs.18
   * Memory Management: HVM is designed to operate without a traditional garbage collector, relying on the implicit memory management inherent in the interaction combinator rules (annihilation/erasure).21
   * Target Languages: While providing a low-level IR, HVM is intended as a compilation target. Bend is the primary high-level language designed specifically for HVM.8 HVM has also been demonstrated or proposed as a target for subsets of Haskell, Python 19, and Agda (via the agda2hvm compiler).43 An intermediate, higher-level abstract syntax tree representation (HVM-Lang) has also been mentioned as a potential target for language implementers.20
6.3. Case Study: Inpla
Inpla is another active project focused on providing a practical environment for programming directly with interaction nets.11
   * Description: Inpla functions as a multi-threaded parallel interpreter for general interaction nets.37 Its primary goal is to allow users to define and execute interaction systems directly, rather than serving solely as a backend for other languages.
   * Features: It supports both interactive and batch execution modes. Parallelism is achieved using POSIX threads, allowing execution to scale across available CPU cores.37 The language includes features to simplify programming, such as syntax for defining rules and nets, built-in types (like integers), and abbreviation notations (<<) to make defining result-passing connections more concise.37 Example programs provided include common algorithms like GCD and sorting.37
   * Implementation: Inpla is implemented using C, with Yacc and Flex for parsing.37
6.4. Emerging Languages: Bend and Vine
Alongside the runtimes, new high-level programming languages are being developed specifically to leverage the interaction net paradigm:
   * Bend: Developed by the Higher Order Company, Bend is designed as the primary user-facing language for the HVM runtime.8 Its syntax draws inspiration from Python but includes static typing and functional programming features reminiscent of Haskell.8 The core design goal is to provide a language that allows programmers to write code that automatically scales across potentially thousands of parallel threads on CPUs and GPUs via HVM.8 While still under active development and optimization 8, Bend represents a significant effort to make the power of HVM accessible through a more conventional programming interface.
   * Vine: A more recent language project, Vine is also explicitly based on interaction nets.8 It features a syntax influenced by Rust.44 Unlike Bend, which targets HVM, Vine currently uses its own virtual machine for execution, with an initial focus on CPU parallelism.8 Vine explores interesting language features potentially enabled by the interaction net model, such as "inverse operators" designed to optimize certain computational patterns that seem inherently sequential.44
6.5. Comparative Analysis
The landscape of interaction net systems presents diverse approaches. HVM focuses on extreme performance and parallelism via interaction combinators and GPU compilation, serving as a backend. Inpla offers direct interaction net programming with CPU-based parallelism. Bend and Vine provide higher-level language abstractions built on the interaction net philosophy, targeting HVM and a custom VM, respectively. Implementations based directly on the Interaction Calculus often serve research and theoretical exploration purposes. The following table summarizes key characteristics:


Feature
	HVM (HVM2)
	Inpla
	Bend
	Vine
	Interaction Calculus (Impl.)
	Model
	Ext. Interaction Combinators
	General Interaction Nets
	Targets HVM (Int. Combinators)
	General Interaction Nets
	Interaction Calculus (affine vars)
	Implementation
	Rust (core), C/CUDA (targets)
	C, Yacc, Flex
	Python-like syntax, targets HVM
	Rust-like syntax, own VM (Rust?)
	C, Haskell (ref)
	Execution
	Compiled (C/CUDA), Interpreted
	Interpreted (multi-threaded)
	Compiled via HVM
	Interpreted (VM)
	Interpreted
	Parallelism
	Massive (CPU/GPU focus)
	Multi-threaded CPU
	Via HVM (CPU/GPU)
	Via VM (CPU focus currently)
	Concurrent possible (atomic swap)
	Memory Mgmt
	Implicit (No GC claimed)
	Implicit (Assumed)
	Via HVM (No GC claimed)
	Implicit (Assumed)
	Implicit (Affine vars/Subst map)
	Optimality
	Beta-Optimality claimed
	Standard Reduction
	Via HVM (Beta-Optimality)
	Standard Reduction
	Optimal-like (affine)
	Maturity/Status
	Actively Developed (HOC)
	Active Development
	Actively Developed (HOC)
	Early Development
	Research Implementation
	Target Use Case
	High-perf parallel functional
	Direct IN programming/research
	Parallel programming (GPU/CPU)
	Parallel programming (CPU)
	Theoretical exploration
	This comparison highlights a significant trend: the field is evolving from theoretical models and basic interpreters towards sophisticated, high-performance runtimes like HVM, designed to serve as backends for new, user-friendly languages like Bend and Vine.3 This push aims to make the benefits of interaction nets – particularly parallelism and potential optimality – accessible for practical software development.
However, this evolution also reveals a potential design tension. Systems like HVM leverage the simplicity and universality of Interaction Combinators 5 to build a streamlined, optimizable core runtime.19 The cost is that programming directly in combinators is intricate, necessitating compilation from higher-level languages.19 Conversely, systems like Inpla or Vine, which allow programming with more general interaction nets 37, might offer more direct or intuitive ways to express certain algorithms.11 This reflects a classic trade-off in system design between the elegance and efficiency of a minimal core versus the expressiveness and convenience of higher-level abstractions.
7. Compilation and Execution Strategies
Bridging the gap between high-level programming languages or abstract interaction net specifications and efficient execution on modern hardware requires sophisticated compilation and runtime strategies.
7.1. From High-Level Languages to Interaction Nets
A crucial step is the compilation of programs written in languages like Haskell, Python, Bend, or Agda into the interaction net formalism.8 This translation involves mapping the source language's constructs and semantics onto the graphical structure of agents, wires, and interaction rules.
Challenges in this process include:
   * Representing Language Features: Standard features like functions, data types (algebraic data types, records), pattern matching, and recursion must be encoded using interaction net agents and rules.2 This often involves defining specific agents to represent data constructors (like S and Z for unary numbers 3), function application (@), lambda abstraction (λ) 26, or control flow structures.
   * Encoding Recursion: Implementing recursion, a cornerstone of functional programming, requires specific techniques within interaction nets, such as using fixpoint combinators or specialized recursion operators.12 Macros have also been proposed as a way to manage complex patterns.14
   * Semantic Mapping: The compiler must correctly translate the source language's evaluation semantics (e.g., lazy vs. strict evaluation) into the reduction dynamics of the interaction net target. This mapping can significantly impact performance and correctness.
Specific compiler efforts illustrate this process. The agda2hvm compiler translates programs from the dependently typed language Agda to HVM's interaction combinator representation.43 Bend programs are compiled via HVM's toolchain.8 Earlier work proposed the Pin language as an intermediate layer for interaction net programming, intended to be generated by graphical tools or compilers.9
Efficient compilation requires more than a simple syntactic translation. It must effectively map the source language's semantics onto the target interaction net model to leverage its strengths (like parallelism and sharing) while respecting its constraints (like linearity). For example, translating Haskell's lazy evaluation mechanism, which relies on thunks and garbage collection 22, to a potentially strict, GC-free interaction net runtime like HVM requires careful consideration of how sharing and demand-driven computation are represented. HVM's reported shift from a lazy strategy in HVM1 to a stricter one in HVM2, driven by better performance on GPUs 20, exemplifies how target hardware and runtime characteristics influence these crucial semantic mapping decisions during compiler design.
7.2. Compilation Targets and Execution Methods
Once an interaction net representation is obtained (either directly specified or compiled from a higher-level language), various strategies exist for its execution:
   * Compilation to C/CUDA: This is a prominent modern approach, exemplified by HVM 19 and earlier research.3 The interaction net runtime logic (active pair detection, rule application, memory management) or the net structure itself is compiled into C code for execution on CPUs or CUDA code for execution on NVIDIA GPUs.8 This strategy leverages the highly optimized compilers available for C and CUDA, allowing the interaction net runtime to benefit from mature code generation and direct access to hardware capabilities, particularly the massive parallelism of GPUs.20
   * Compilation to Abstract Machines: Some research has explored defining abstract machines specifically designed to execute interaction nets.9 Interaction nets are compiled into a lower-level instruction set for this abstract machine, which is then interpreted or further compiled. This approach offers more control over the execution model but requires the development and optimization of the abstract machine itself.
   * Direct Interpretation: Interpreters provide a straightforward way to execute interaction nets directly from their definition. Systems like Inpla 37, HVM's Rust-based reference interpreter 19, and various research tools 1 fall into this category. Interpretation is generally simpler to implement but typically incurs significant performance overhead compared to compiled approaches.8 It is often suitable for development, debugging, education, or executing smaller nets.
The choice of execution strategy involves trade-offs. Compilation to C/CUDA currently appears to offer the highest performance potential, especially for parallel execution, by leveraging existing compiler technology and hardware features.19 However, it may limit portability and increase build complexity. Custom abstract machines offer tailored execution environments but require substantial development effort. Interpretation provides simplicity and portability at the cost of speed. The trend towards C/CUDA compilation in high-performance systems like HVM suggests that maximizing execution speed on contemporary hardware is a primary driver in current development.19
7.3. Optimization Techniques
Optimizing the execution of interaction nets is crucial for realizing their performance potential. Optimization can occur at multiple levels:
   * Runtime System Optimization:
   * Active Pair Detection: Efficiently finding active pairs within the potentially large graph structure is critical, especially in parallel environments.
   * Memory Layout: Optimizing the memory representation of the net to improve data locality and reduce cache misses during traversal and reduction can significantly impact performance. HVM's developers attribute major speedups to breakthroughs in memory layout design.21
   * Rule Application: Implementing the core interaction rule application (rewriting and rewiring) as efficiently as possible.
   * Parallel Scheduling: Developing effective strategies for distributing active pairs to parallel workers, managing communication, and ensuring load balancing.31
   * Interaction Net Level Optimization:
   * Rule Simplification: Analyzing the interaction rules themselves to identify potential simplifications or equivalent but more efficient formulations.
   * Macro Systems: Using macro-like extensions to define complex interaction patterns more concisely, potentially allowing for optimized expansion or direct implementation.14
   * Target Code Optimization: When compiling to C or CUDA, leveraging the optimization capabilities of the underlying C/CUDA compilers is essential.
8. Application Domains
The unique properties of interaction nets—inherent parallelism, potential for optimal reduction, and graphical nature—suggest applicability across several domains, moving beyond their theoretical origins.
8.1. High-Performance Functional Programming Runtimes
Perhaps the most active area of application development is the use of interaction nets as a backend or runtime environment for functional programming languages.9 Systems like HVM aim to provide an alternative to established runtimes like GHC (for Haskell), with the goal of achieving superior performance through automatic parallelization and optimal evaluation.21 By compiling functional code to interaction nets (specifically interaction combinators in HVM's case), the hope is to unlock performance on multi-core CPUs and GPUs that is difficult to achieve with traditional runtime architectures, potentially overcoming limitations related to garbage collection, laziness overhead, or suboptimal sharing.21
8.2. Massively Parallel Computation Frameworks
The inherent parallelism and the demonstrated targeting of GPUs make interaction nets a candidate model for general-purpose massive parallelism.5 Languages like Bend, built on HVM, explicitly aim to allow programmers to write code that scales automatically across many cores.8 This suggests potential applications in domains requiring high computational throughput, such as scientific computing, large-scale data processing, simulations, and other areas where exploiting parallel hardware is critical, particularly if the interaction net model simplifies the expression of parallel algorithms compared to traditional imperative or message-passing approaches. The primary driver for current interest appears focused here: using interaction nets to unlock parallel hardware capabilities for declarative programming styles.8
8.3. Symbolic Computation and Formal Systems
Given their origins in logic and their nature as a rewriting system, interaction nets remain relevant for symbolic computation tasks.12 They provide a formal framework for studying computational dynamics, implementing logical systems (like Linear Logic proof reduction), and potentially performing other symbolic manipulations. Research using context semantics to analyze program evaluation via paths within interaction nets is one example of their use in formal studies.12
8.4. Explorations in Novel Hardware Architectures
A more speculative but intriguing potential application lies in the design of novel computer architectures.16 The extreme locality and simplicity of interaction net rules (especially interaction combinators) have led to proposals that they might be suitable for direct implementation in hardware. Such an architecture could potentially consist of a large array of simple processing elements, each capable of performing local rewrites in parallel, thereby circumventing the sequential instruction fetch and centralized memory bottlenecks of traditional Von Neumann architectures.24 The Higher Order Company explicitly includes hardware development as a long-term goal 30, suggesting a belief that the interaction net model's fundamental properties might align better with future massively parallel, distributed hardware paradigms than current software simulations on conventional machines. While highly challenging and speculative, this represents a long-term vision for the computational model.
9. Research Landscape: State, Challenges, and Future Directions
The field of interaction network reduction is experiencing a resurgence of interest, driven by recent performance breakthroughs and the development of new languages and runtimes. However, significant research questions and engineering challenges remain.
9.1. Current Research Frontiers
Active research and development are concentrated in several key areas:
   * High-Performance Implementations: Optimizing runtime performance remains paramount. This includes refining algorithms for active pair detection and scheduling on parallel architectures (especially GPUs), improving memory management and data layout, and minimizing the overhead of rule application.19
   * Language Design and Type Systems: Developing expressive, ergonomic high-level languages (like Bend and Vine) that effectively abstract the underlying interaction net model is crucial for broader adoption.8 Research into suitable type systems for interaction nets, potentially leveraging ideas from linear types or substructural types, is also ongoing to ensure safety and aid optimization.1
   * Compiler Technology: Building robust and optimizing compilers that can efficiently translate various high-level language features and evaluation strategies into efficient interaction net code is a major undertaking.3
   * Theoretical Extensions: Exploring theoretical aspects continues, including extending the model to handle non-determinism 5, developing more sophisticated type systems and program logics 1, formal verification of implementations, and refining computational cost models.
9.2. Open Problems and Implementation Challenges
Despite progress, several significant hurdles need to be overcome:
   * Performance Consistency and Scalability: While impressive peak performance figures have been reported (especially for HVM on GPUs 20), achieving consistently high performance across a wide range of applications and input sizes remains a challenge. Pathological cases leading to unexpected slowdowns (like the quadratic behavior noted in HVM issue #60 36 or potential exponential time complexities in certain abstract machine designs 22) need to be understood and mitigated. Efficiently managing communication and memory access patterns on highly parallel hardware is complex.8
   * Tooling and Debugging: The highly parallel and fine-grained nature of interaction net execution makes debugging difficult. Developing effective tools for visualization, tracing, performance profiling, and debugging in this paradigm is essential for practical development but currently lags behind the mature ecosystems of established languages.9
   * Expressiveness and Practical Features: Integrating interaction nets seamlessly with the requirements of real-world programming remains an ongoing task. Efficiently handling input/output (I/O), foreign function interfaces (FFI) to interact with existing libraries, managing mutable state where necessary, and representing complex data structures pose challenges for the pure, graph-based model.3 Techniques like using monads to model impure computations are being explored 28, but robust solutions are needed. Bridging the gap between the pure model and impure practicalities is a key future direction.
   * Correctness: Ensuring the correctness of complex parallel runtimes (like HVM) and the compilers targeting them is non-trivial. Subtle semantic mismatches, such as potential limitations in HVM's encoding of the full lambda calculus 36, require careful verification and testing.
9.3. Future Potential and Outlook
Interaction nets offer a compelling alternative model of computation with the potential to significantly impact parallel programming, particularly within the functional paradigm. The promise of automatic parallelism and optimal reduction, combined with advancements in runtime performance (especially on GPUs), suggests a bright future if the current engineering challenges can be surmounted.
The field appears to be in a dynamic phase. The performance demonstrated by systems like HVM has generated considerable excitement 20, spurring the development of new languages 8 and attracting investment.30 However, translating this potential into widespread practical adoption requires overcoming the substantial hurdles related to performance consistency, tooling maturity, practical feature integration, and verified correctness.9
Success hinges on continued progress in runtime optimization, compiler technology, and language design. If these challenges are met, interaction nets could become a valuable tool for harnessing the power of parallel hardware, offering a path to high performance for declarative programming styles. The long-term vision of specialized hardware 24, while speculative, further underscores the perceived fundamental advantages of the model.
10. Conclusion
10.1. Summary of Interaction Network Reduction
Interaction network reduction presents a unique computational model rooted in graph rewriting and linear logic. Defined by agents interacting locally via principal ports according to strict rules, computation proceeds through deterministic, confluent reduction steps. Key properties emerge directly from this structure: locality of interaction, strong confluence ensuring result independence from reduction order, and linearity enabling constant-time rule application. These properties form the basis for the model's most compelling features: inherent, automatic parallelism, and the potential for optimal reduction strategies that avoid redundant work. The Interaction Calculus provides a necessary textual formalism, while Interaction Combinators offer a universal, minimal foundation. Recent years have seen a shift from purely theoretical exploration towards practical implementations, exemplified by the high-performance, parallel runtime HVM, the direct interaction net interpreter Inpla, and emerging high-level languages like Bend and Vine designed to leverage the paradigm.
10.2. Significance and Future Outlook
The primary significance of interaction nets today lies in their potential to unlock efficient, automatic parallelism for high-level, particularly functional, programming languages on modern multi-core and GPU architectures. By offering a computation model where parallelism is inherent rather than explicitly managed, and where optimal evaluation strategies can be naturally expressed, interaction nets promise a path towards harnessing massively parallel hardware without sacrificing declarative programming styles or requiring complex manual parallelization. The development of runtimes like HVM, achieving substantial performance gains, signals that this potential is beginning to be realized.
However, the field is still developing. Significant engineering challenges remain in achieving robust and predictable performance across diverse applications, developing mature development tools and debugging environments, seamlessly integrating practical programming features like I/O, and ensuring the correctness of complex compilers and runtimes. The success of interaction nets as a mainstream paradigm will depend on the research community's ability to overcome these hurdles. If successful, interaction network reduction could offer a powerful new foundation for parallel software development, potentially influencing language design, runtime systems, and perhaps even future hardware architectures. The journey from Lafont's theoretical insights to practical, high-performance systems is well underway, making interaction nets a field of considerable interest and future potential in computer science.
Works cited
   1. joerg.endrullis.de, accessed on April 17, 2025, https://joerg.endrullis.de/downloads/gcm2024/STAF_2024_paper_72.pdf
   2. A Functional Programming Language for Interaction Nets – Abstract, accessed on April 17, 2025, https://plrg-bristol.github.io/fir/assets/abstracts/thatcher.pdf
   3. Compilation of Interaction Nets - CORE, accessed on April 17, 2025, https://core.ac.uk/download/pdf/82756233.pdf
   4. Extending the Interaction Nets Calculus by Generic Rules - CSE CGI Server, accessed on April 17, 2025, https://cgi.cse.unsw.edu.au/~eptcs/paper.cgi?LINEARITY2012.2.pdf
   5. Interaction nets - Wikipedia, accessed on April 17, 2025, https://en.wikipedia.org/wiki/Interaction_nets
   6. Towards a GPU-based implementation of interaction nets - arXiv, accessed on April 17, 2025, https://arxiv.org/pdf/1404.0076
   7. Interaction Nets, Combinators, and Calculus, accessed on April 17, 2025, https://zicklag.github.io/blog/interaction-nets-combinators-calculus/
   8. Two new graph-based functional programming languages - LWN.net, accessed on April 17, 2025, https://lwn.net/Articles/1011803/
   9. Interaction nets: programming language design and implementation - CiteSeerX, accessed on April 17, 2025, https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=fc2598c9d7ad0a1f2e5777bcfcbc00366c3d579e
   10. Interaction nets: programming language design and implementation, accessed on April 17, 2025, https://www.user.tu-berlin.de/o.runge/tfs/workshops/gtvmt08/Program/paper_38.pdf
   11. Explanation of a simple example of how an interaction net computes?, accessed on April 17, 2025, https://cs.stackexchange.com/questions/160537/explanation-of-a-simple-example-of-how-an-interaction-net-computes
   12. A context semantics for interaction nets, accessed on April 17, 2025, https://dice14.tcs.ifi.lmu.de/abstracts/Perrinel.pdf
   13. A Calculus for Interaction Nets - CiteSeerX, accessed on April 17, 2025, https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e0b35e17d680da34afef2f324c0f3e8b4519639f
   14. Partial Evaluation of Interaction Nets., accessed on April 17, 2025, http://pagesperso.ls2n.fr/~bechet-d/Documents/LIENS-1992-Bec-WSA92-recompiled-2003-07-22-draft.pdf
   15. AN EXPLICIT FRAMEWORK FOR INTERACTION NETS - Logical Methods in Computer Science, accessed on April 17, 2025, https://lmcs.episciences.org/1108/pdf
   16. noughtmare/lafont90: The interaction net language proposed by Yves Lafont implemented in Rascal. - GitHub, accessed on April 17, 2025, https://github.com/noughtmare/lafont90
   17. From Proof-Nets to Interaction Nets 1 Introduction 2 Multiplicatives - CiteSeerX, accessed on April 17, 2025, https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e18ef7b6e9094e4bfa4514ef31379889f105ecea
   18. Higher Order Company, accessed on April 17, 2025, https://higherorderco.com/
   19. HigherOrderCO/HVM: A massively parallel, optimal ... - GitHub, accessed on April 17, 2025, https://github.com/HigherOrderCO/HVM
   20. Quick HVM updates: huge simplifications, *finally* runs on GPUs, 80x speedup on RTX 4090 : r/haskell - Reddit, accessed on April 17, 2025, https://www.reddit.com/r/haskell/comments/172hk22/quick_hvm_updates_huge_simplifications_finally/
   21. HVM: a next-gen massively parallel, beta-optimal functional runtime is 50x faster than its predecessors : r/haskell - Reddit, accessed on April 17, 2025, https://www.reddit.com/r/haskell/comments/shewq7/hvm_a_nextgen_massively_parallel_betaoptimal/
   22. High-order Virtual Machine (HVM) an optional GHC-like runtime for rust with many comparisons to GHC - Learn - Haskell Discourse, accessed on April 17, 2025, https://discourse.haskell.org/t/high-order-virtual-machine-hvm-an-optional-ghc-like-runtime-for-rust-with-many-comparisons-to-ghc/4118
   23. Macros for Interaction Nets, accessed on April 17, 2025, http://www.lsv.fr/Publis/PAPERS/PDF/SM-tg04.pdf
   24. Why the overall lack of interest in Interaction Combinators? : r/haskell - Reddit, accessed on April 17, 2025, https://www.reddit.com/r/haskell/comments/568gtk/why_the_overall_lack_of_interest_in_interaction/
   25. (PDF) Token-passing Optimal Reduction with Embedded Read-back - ResearchGate, accessed on April 17, 2025, https://www.researchgate.net/publication/308009092_Token-passing_Optimal_Reduction_with_Embedded_Read-back/download
   26. (PDF) Weak Reduction and Garbage Collection in Interaction Nets - ResearchGate, accessed on April 17, 2025, https://www.researchgate.net/publication/222519768_Weak_Reduction_and_Garbage_Collection_in_Interaction_Nets
   27. A compact encoding for λ-terms in interaction calculus - CiteSeerX, accessed on April 17, 2025, https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=a62d572f692f5687a75d441fd3c28489c7520d24
   28. (PDF) A Tool for Programming with Interaction Nets - ResearchGate, accessed on April 17, 2025, https://www.researchgate.net/publication/220369597_A_Tool_for_Programming_with_Interaction_Nets
   29. HigherOrderCO/HVM1: A massively parallel, optimal functional runtime in Rust - GitHub, accessed on April 17, 2025, https://github.com/HigherOrderCO/HVM1
   30. Higher-Order Virtual Machine (HVM) - Hacker News, accessed on April 17, 2025, https://news.ycombinator.com/item?id=35336113
   31. High-order Virtual Machine (HVM): Massively parallel, optimal functional runtime, accessed on April 17, 2025, https://news.ycombinator.com/item?id=30219452
   32. VictorTaelin/Interaction-Calculus: A programming language and model of computation that matches the optimal λ-calculus reduction algorithm perfectly. - GitHub, accessed on April 17, 2025, https://github.com/VictorTaelin/Interaction-Calculus
   33. XXIIVV — interaction nets, accessed on April 17, 2025, https://wiki.xxiivv.com/site/interaction_nets.html
   34. cgi.cse.unsw.edu.au, accessed on April 17, 2025, https://cgi.cse.unsw.edu.au/~eptcs/paper.cgi?TERMGRAPH2016.7.pdf
   35. arXiv:1806.07275v4 [cs.LO] 14 Oct 2018 Upward confluence in the interaction calculus, accessed on April 17, 2025, https://arxiv.org/pdf/1806.07275
   36. The HVM project is incredibly interesting and I applaud the great work Victor Ta... | Hacker News, accessed on April 17, 2025, https://news.ycombinator.com/item?id=35340714
   37. Inpla: Interaction nets as a programming language (the current version) - GitHub, accessed on April 17, 2025, https://github.com/inpla/inpla
   38. Interaction Nets, Combinators, and Calculus — HVM functional runtime in Rust - Reddit, accessed on April 17, 2025, https://www.reddit.com/r/rust/comments/123ri90/interaction_nets_combinators_and_calculus_hvm/
   39. HVM2: A Parallel Evaluator for Interaction Combinators - GitHub, accessed on April 17, 2025, https://raw.githubusercontent.com/HigherOrderCO/HVM/main/paper/HVM2.pdf
   40. hvm-core 0.1.4 - Docs.rs, accessed on April 17, 2025, https://docs.rs/crate/hvm-core/0.1.4/
   41. How Bend Works, accessed on April 17, 2025, https://docs.usebend.ai/introduction/how-bend-works
   42. Gadersd/ic: An interaction combinator runtime - GitHub, accessed on April 17, 2025, https://github.com/Gadersd/ic
   43. Code extraction from Agda to HVM | TU Delft Repository, accessed on April 17, 2025, https://repository.tudelft.nl/record/uuid:90e4331b-d684-4578-ac14-5e448411b536
   44. Vine: A programming language based on Interaction Nets | Hacker News, accessed on April 17, 2025, https://news.ycombinator.com/item?id=43144040