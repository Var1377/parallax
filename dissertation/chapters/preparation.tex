\chapter{Preparation}\label{chap:preparation}

\section{Introduction}\label{sec:prep_intro}

This chapter documents the \textit{preparation} phase of the Parallax project, bridging the gap between the initial proposal and the start of significant implementation work. This crucial phase involved refining the project's core goal, analysing detailed requirements (Section~\ref{sec:prep_requirements}), conducting focused background research (Section~\ref{sec:prep_research}) to inform key pre-implementation design decisions (Section~\ref{sec:prep_design_decisions}), establishing the software engineering approach (Section~\ref{sec:prep_software_eng}), and outlining the evaluation plan (Section~\ref{sec:prep_evaluation}).

The central aim guiding this preparation was to solidify the path towards achieving automatic parallelism for a functional language by using interaction nets as a compilation target and implementing a suitable parallel runtime system.

\section{Requirements Analysis}\label{sec:prep_requirements}
Based on the initial project proposal and subsequent refinement during the preparation phase, the requirements for Parallax were clarified. Guided by the core design philosophy of achieving \textbf{efficient and scalable reduction} with \textbf{minimal programmer effort} (as outlined in Chapter 1), these requirements were then prioritised using the MoSCoW method (Must have, Should have, Could have, Won't have).

\subsection{Language Features}

To fulfil the goal of minimal programmer effort and provide a familiar functional interface, the language features identified during preparation were categorized as follows:

\subsubsection*{Must-Have}
These features were deemed essential for the core concept and minimum viability:
\begin{itemize}
    \item \textbf{Primitive Types}: integers, floats, booleans.
    \item \textbf{Arithmetic}: addition, subtraction, multiplication, division.
    \item \textbf{Control flow}: if then else, pattern matching, recursion.
    \item \textbf{Algebraic Data Types}: structs and enums.
    \item \textbf{Higher-Order Functions}: map, filter, fold, etc.
\end{itemize}

\subsubsection*{Should-Have}
These features were strongly desired to make the language more practical and expressive, but were deferred from the \textit{Must-Have} category due to their implementation complexity or because they were not strictly essential for demonstrating the core parallel interaction net concept:
\begin{itemize}
    \item \textbf{Closures}: these are highly desirable for idiomatic functional programming but possibly more complex to implement.
    \item \textbf{Generics}: these greatly enhance expressiveness and code reuse but the core functionality doesn't require them.
    \item \textbf{Standard Library}: while essential for practical use, the initial focus was on the core language and runtime mechanics; a minimal set of intrinsics could suffice initially.
    \item \textbf{Modules}: these are crucial for code organization in larger projects, but not strictly necessary for evaluating the core parallel execution model with smaller examples.
    \item \textbf{IO}: while necessary for real-world interaction, implementing side effects within or alongside the pure interaction net model adds complexity that could be addressed after the core reduction engine was functional.
\end{itemize}

\subsubsection*{Could-Have}
These features add significant value but were deferred to focus initial efforts on core requirements:
\begin{itemize}
    \item \textbf{Core Data structures (Arrays, Strings)}: these are fundamental but deferred as their efficient implementation likely depends on garbage collection, adding significant complexity.
    \item \textbf{Traits}: these enhance polymorphism but implementing type class resolution and code generation is complex and secondary to core features.
    \item \textbf{Dependencies}: this is important for larger projects but a package manager is a substantial undertaking, not essential for the initial proof-of-concept.
    \item \textbf{Effects}: these provide structured side-effect management but significantly increase compiler and type system complexity.
    \item \textbf{Async Abstraction}: this would broaden applicability but represents a major extension to the runtime model, deferred until synchronous execution was stable.
\end{itemize}

\subsubsection*{Won't-Have}
These features were explicitly excluded to maintain focus and manage project scope effectively:
\begin{itemize}
    \item \textbf{Hashmaps/Sets}: these are useful collections but efficient implementation, potentially requiring custom hashing, adds complexity beyond core goals.
    \item \textbf{FFI (Foreign Function Interface)}: this enables interoperability but integrating with C libraries adds significant complexity (data representation, safety) distracting from the core runtime.
    \item \textbf{More advanced type system features}: These likely add substantial implementation complexity and deviation from the goal of a relatively simple, familiar functional language.
\end{itemize}
No other features like macros or complex module interactions were explicitly ruled out at this stage, but focus remained on the core requirements.

\subsection{Runtime and Compiler Requirements}
Beyond the language features, the requirements for the compiler and runtime system were also established during preparation and prioritised using MoSCoW:

\subsubsection*{Must-Have}
These capabilities were fundamental to the project's viability:
\begin{itemize}
    \item \textbf{Core Language Compilation}: The compiler must successfully parse, analyze, and generate a set of interaction nets for the "Must-Have" language features defined above.
    \item \textbf{Parallel Interaction Net Reduction}: The runtime must implement a strategy for executing the generated interaction nets in parallel across multiple CPU cores.
\end{itemize}

\subsubsection*{Should-Have}
These features were important for achieving the desired performance and usability goals but were not strictly essential for the initial \textit{Must-Have} runtime:
\begin{itemize}
    \item \textbf{Efficient Primitive Operations}: Augmenting the IN system with intrinsics or specialized nodes is vital for performance but adds complexity beyond the basic reduction mechanism.
    \item \textbf{Basic Optimizations}: These improve code quality but are secondary to generating correct code from the core language features first.
    \item \textbf{Incremental Compilation}: This greatly improves developer experience but is not essential for the core compiler/runtime functionality; the architecture should facilitate it later.
\end{itemize}

\subsubsection*{Could-Have}
These capabilities would significantly enhance the system but were considered secondary improvements or alternative exploration paths:
\begin{itemize}
    \item \textbf{Garbage Collection}: Crucial for supporting practical data structures (like arrays) efficiently, but deferred as integrating GC adds significant runtime complexity.
    \item \textbf{Native Code Backend}: Useful for performance comparison and exploring hybrid models, but adds significant development effort compared to the primary IN target.
    \item \textbf{Advanced Optimizations}: Further performance improvements are valuable but complex to implement correctly and secondary to core functionality.
    \item \textbf{Sophisticated Error Reporting}: Enhances usability greatly but is an orthogonal concern to the core compilation and reduction logic.
    \item \textbf{Memory Profiling/Benchmarking Tools}: Important for evaluation but can be integrated or developed after the core system is functional.
\end{itemize}

\subsubsection*{Won't-Have}
These features were explicitly excluded due to significant complexity, deviation from core goals, or being outside the scope of a single dissertation project:
\begin{itemize}
    \item \textbf{Guaranteed Optimal Reduction}: Theoretically interesting but practically very difficult.
    \item \textbf{Formal Verification}: Provides strong correctness guarantees but is a massive undertaking requiring specialized expertise, far beyond project scope.
    \item \textbf{Distributed Execution}: Extends parallelism beyond single-machine multi-core, adding significant complexity related to networking and state synchronization.
    \item \textbf{GPU Backend}: Offers potential for massive parallelism but is a major separate effort.
\end{itemize}

\section{Background Research and Problem Analysis}\label{sec:prep_research}

The preparation phase involved significant research into interaction nets, existing implementations, parallel execution strategies, and the challenges of supporting practical language features on these foundations.

\subsection{Choosing an Interaction Net Model}\label{sec:prep_research_in}

As established in the Introduction, interaction nets offer promising theoretical properties like confluence and locality for automatic parallelism \cite{lafont1990interactionnets}. During the preparation phase, research focused on selecting a concrete interaction net model suitable as a compilation target. An extension on the Symmetric Interaction Combinators (SICs), described by Mazza \cite{mazza}, emerged as the primary candidate due to their theoretical elegance, potential link to optimal reduction strategies for functional languages, and relative simplicity compared to more complex interaction net variants.

A central theme that emerged from this research phase, however, was the critical importance of \textbf{granularity control}: balancing the theoretical purity and fine-grained parallelism inherent in the core IN model against the practical necessity of efficient execution for common, low-level operations. A critical challenge identified was the inherent performance limitation of a purely combinator-based model. Representing fundamental operations like integer arithmetic or conditional branching solely through fine-grained combinator gadgets leads to unacceptable inefficiency, directly conflicting with the project's requirement for high single-threaded performance. This highlights the fundamental trade-off: while maximizing theoretical parallelism might suggest fine granularity, practical performance often demands coarser-grained execution for primitives. Our analysis suggested existing approaches may not strike an optimal balance. Analysis of existing systems like HVM, which augment combinators with specialized nodes for primitives \cite{HVMGithub}, confirmed the practical necessity of moving beyond a purely fine-grained model. Therefore, a key finding from the preparation research was that any viable implementation would need to carefully manage this granularity trade-off, extending the pure SIC model to handle certain operations efficiently. This directly informed the design decisions regarding intrinsics and specialized nodes detailed in Section~\ref{sec:prep_decision_in}.

\subsection{Analysis of Existing Parallel IN Runtimes}\label{sec:prep_research_runtimes}

A review of existing interaction net programming languages and runtimes provided crucial insights into practical implementation strategies and their trade-offs, particularly concerning the granularity control problem identified above.

\subsubsection{Higher-Order Virtual Machine (HVM)}

The Higher-Order Virtual Machine (HVM) \cite{HVMGithub}, the runtime for the \texttt{Bend} language, represents a significant effort in realizing parallel interaction combinator execution. Its design choices informed the initial analysis for Parallax.

\texttt{HVM}'s runtime employs a \textbf{global node buffer} (Figure~\ref{fig:prep_global_buffer}). It relies on fine-grained atomic operations (CAS/exchange) for synchronization across all threads attempting to modify nodes in the shared buffer, coupled with \textbf{work-stealing} --- a dynamic load balancing technique where idle worker threads proactively take pending tasks from the queues of busy threads --- for load balancing.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance=0.8cm and 1.5cm, auto, >=Latex,
        worker/.style={rectangle, draw, thick, minimum size=1cm, text centered, rounded corners},
        buffer/.style={rectangle, draw, thick, minimum width=8cm, minimum height=4cm, fill=blue!10, rounded corners}]
        
        % Global Buffer
        \node (buffer) [buffer] {Global Node Buffer / Heap};
        
        % Workers
        \node (w1) [worker, above left=of buffer.north] {Worker 1};
        \node (w2) [worker, above right=of buffer.north] {Worker 2};
        \node (w3) [worker, left=of buffer.west, xshift=-1cm] {Worker 3};
        \node (w4) [worker, right=of buffer.east, xshift=1cm] {Worker 4};
        
        % Arrows (representing access attempts)
        \draw [->, thick, red] (w1.south) -- (w1.south |- buffer.north);
        \draw [->, thick, red] (w2.south) -- (w2.south |- buffer.north);
        \draw [->, thick, red] (w3.east) -- (w3.east -| buffer.west);
        \draw [->, thick, red] (w4.west) -- (w4.west -| buffer.east);
        
    \end{tikzpicture}
    \caption{Conceptual Diagram: Global Node Buffer Runtime Strategy}
    \label{fig:prep_global_buffer}
\end{figure}

Analysis during preparation identified this global atomic strategy as a potential scalability bottleneck, particularly on NUMA architectures due to potential bus saturation and cache coherence traffic leading to partial serialization. This approach also restricted HVM to 24-bit integers due to the size restrictions of 64-bit atomics.

Preliminary benchmarking and analysis of Bend/HVM also revealed performance limitations related to its reliance on linked-list-like structures for data, impacting random access speed, and potential memory bandwidth constraints \cite{BreakingTheBend}. Observations by its authors also suggested HVM could be memory-bandwidth constrained, performing notably better on architectures with high memory bandwidth (like Apple Silicon) compared to typical x86 systems \cite{HVMGithub}.

\subsubsection{Inpla}

Another notable system is \texttt{Inpla} \cite{inpla}, which also focuses on efficient parallel reduction. Unlike HVM, \texttt{Inpla} requires the user to define the interaction system rules directly. It achieves high performance, partially by compiling the interaction system into specialized bytecode to accelerate runtime operations. While demonstrating strong performance potential, its programming model (direct IN definition) differs significantly from the goal of compiling a high-level language so it's tough to directly draw from.

\subsubsection{Vine}

\texttt{Vine} \cite{VineGithub}, another high-level language targeting interaction networks (specifically its own IVM), emerged later in the project's timeline and thus did not influence these initial preparation-phase analyses, though it serves as a relevant contemporary point of comparison.

\subsection{Challenges in Supporting Language Features}\label{sec:prep_research_features}

Bridging the gap between the pure, linear model of interaction nets and the requirements of a practical, performant programming language presents several fundamental theoretical challenges.

\subsubsection{Garbage Collection}

While some functional features map reasonably well to INs, efficient implementations of common data structures like arrays and strings necessitate memory management beyond simple node erasure. Representing these variable-length, random-access structures directly via IN copying is prohibitively inefficient. 

Research during the preparation phase concluded that the most viable approach involves adapting principles from Garbage Collection (GC) theory. This would entail managing the underlying data for these structures on a separate heap, representing them within the net via lightweight pointers (e.g., \texttt{(pointer, length)}), and requiring the GC mechanism to trace references originating from within the potentially distributed IN graph nodes.

Initial exploration of potential GC algorithms suitable for graph-based tracing in a concurrent environment identified \textbf{Mark-Sweep} principles as more appropriate than \textbf{Reference Counting} for this context, primarily due to the anticipated overhead of frequent atomic updates required by reference counting in a highly parallel reduction engine.

\subsubsection{Native Code Integration}

Integrating calls to native machine code (e.g., for performant primitive operations) introduces the significant theoretical hurdle of \textbf{reduction recognition}. Interaction net reduction is inherently asynchronous. Before calling a native function (e.g., integer addition), the runtime must ensure that the graph fragments representing the function's inputs have stabilized into the expected canonical forms (e.g., a constructor connected to two Numbers). Invoking the native function prematurely on unevaluated graph structures would lead to an inability to call the native function.

Devising an efficient and reliable mechanism for the runtime to detect this state of input stabilization without unduly interfering with the parallel reduction process was identified as a non-trivial challenge requiring a robust solution for any strategy relying on native intrinsics.

\section{Key Design Decisions}\label{sec:prep_design_decisions}

Based on the requirements analysis (Section~\ref{sec:prep_requirements}) and the background research and problem analysis (Section~\ref{sec:prep_research}), several key architectural and design decisions were made before starting the main implementation phase. These decisions aimed to balance the theoretical benefits of interaction nets with the practical requirements for performance and usability.

\subsection{Parallel Reduction Strategy}\label{sec:prep_decision_runtime}

\subsubsection*{Rationale from Research}
Research into existing parallel interaction net runtimes, particularly HVM (Section~\ref{sec:prep_research_runtimes}), revealed potential scalability bottlenecks with its global atomic approach. Reliance on fine-grained atomics across a shared buffer (Figure~\ref{fig:prep_global_buffer}) can lead to high contention, memory bus saturation, and cache coherence issues, especially on NUMA architectures, potentially limiting parallel speedup.

\subsubsection*{Design Decision: Partition Ownership and Slab Allocation}
To address these scalability concerns, the \textbf{Partition Ownership model} was selected as the core parallel reduction strategy for Parallax. As illustrated in Figure~\ref{fig:prep_partition_ownership}, this approach explicitly divides the interaction net graph into partitions.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance=0.8cm and 1cm, auto, >=Latex,
        worker/.style={rectangle, draw, thick, minimum size=0.8cm, text centered, rounded corners},
        partition/.style={rectangle, draw, thick, minimum width=3.5cm, minimum height=2.5cm, fill=green!10, rounded corners, text centered},
        op/.style={font=\scriptsize, midway, fill=white, inner sep=1pt, anchor=center},
        localop/.style={font=\scriptsize, below=0.1cm}]

        % Partitions (arranged in a grid)
        \node (p1) [partition] {Partition 1}; 
        \node (p2) [partition, right=1.5cm of p1] {Partition 2};
        \node (p3) [partition, below=1cm of p1] {Partition 3};
        \node (p4) [partition, right=1.5cm of p3] {Partition 4};

        % Workers and Ownership (Adjusted vertical positioning)
        \node (w1) [worker, above left=0.3cm of p1] {W1} edge [->, thick, blue, bend right=10] (p1.north west);
        \node (w2) [worker, above right=0.3cm of p2] {W2} edge [->, thick, blue, bend left=10] (p2.north east);
        \node (w3) [worker, below left=0.3cm of p3] {W3} edge [->, thick, blue, bend right=10] (p3.south west) edge [->, thick, blue, bend right=10] node[op, pos=0.7, sloped, below] {Owns} (p4.south west);
        
        % Local Operations (represented inside partitions) - Removed for brevity as requested previously
        
        % Cross-Partition Interaction Need (Sync/Msg)
        % Example: Interaction between node in P1 and node in P2 requires sync
        \draw [<->, thick, orange, dashed] (p3.north east) -- (p2.south west) node[op] {Sync/Msg Req.}; % Kept commented
  
        % Work Stealing (Example: W3 gives P4 to W2)
        \draw [->, thick, purple, dashed, bend right=20] (p4.east) to node[op] {Work Steal (Transfer P4)} (w2.south east);

    \end{tikzpicture}
    \caption{Conceptual Diagram: Partition Ownership Runtime Strategy}
    \label{fig:prep_partition_ownership}
\end{figure}

This strategy grants worker threads exclusive, non-atomic access to owned partitions. The hypothesis was that this approach would offer better scalability and single-threaded performance by reducing contention and improving cache utilization compared to the global atomic approach. While requiring a more complex runtime design (e.g., for managing ownership transfer and boundary interactions), the anticipated benefits in scalability and performance, particularly on NUMA systems, justified this choice.

For managing the memory of interaction net nodes within these partitions, a \textbf{slab allocation strategy} was chosen. This approach uses pre-allocated, type-specific chunks (slabs) to store nodes, favored for its potential for predictable memory access patterns, reduced fragmentation within a partition, and potentially simpler memory management logic compared to more dynamic allocation schemes.

\subsection{Interaction System and Primitives}\label{sec:prep_decision_in}

\subsubsection*{Rationale from Research}
Research indicated that while \textbf{symmetric interaction combinators} provide a theoretically elegant foundation with properties like optimal reduction (Section~\ref{sec:prep_research_in}), a purely combinator-based approach suffers severe practical performance limitations, especially for primitive operations like arithmetic and conditional branching. Representing these via combinator gadgets (e.g., Church numerals) is far too slow for the project's performance goals. Existing systems like HVM mitigate this by introducing specialized node types for primitives.

\subsubsection*{Design Decision: Extended Symmetric Combinators with Intrinsics}
Based on the need to retain the parallel benefits of interaction nets while achieving practical performance, the decision was made to build Parallax upon symmetric interaction combinators, but extended with specific node types and an \textbf{intrinsic function mechanism} to provide effective \textbf{granularity control}. The core SIC rules are illustrated in Figure~\ref{fig:prep_sic_rules_combined}.

\begin{figure}[h!]
    \centering
    % Annihilation Rules
    \begin{tikzpicture}[scale=0.8, transform shape, node distance=0.8cm and 1.5cm, >=Latex, wirestyle/.style={draw, line width=0.15ex}]
        % Alpha Annihilation
        \inetcell[arity=2](a1){$\alpha$}{0,0}[U]; % Pointing Up
        \inetcell[arity=2](a2){$\alpha$}{$(a1.pal)+(0,1.5)$}[D]; % Pointing Down, positioned relative to a1's principal port
        \inetshortwire(a1.pal)(a2.pal); % Connect principal ports

        % Free wires for LHS
        \inetwirefree(a1.pax 1);
        \inetwirefree(a1.pax 2);
        \inetwirefree(a2.pax 1);
        \inetwirefree(a2.pax 2);

        % Arrow
        \draw[->, thick] ($(a1.east)+(0.8,1)$) -- ++(1,0);

        % RHS for alpha (crossed wires)
        % Define coordinates relative to ports
        \coordinate (a1p1_rhs) at ($(a1.pax 1)+(3.5, 0)$);
        \coordinate (a1p2_rhs) at ($(a1.pax 2)+(3.5, 0)$);
        \coordinate (a2p1_rhs) at ($(a2.pax 1)+(3.5, 0)$);
        \coordinate (a2p2_rhs) at ($(a2.pax 2)+(3.5, 0)$);

        % Draw crossed wires using standard TikZ draw
        \draw[wirestyle] (a1p1_rhs) -- (a2p1_rhs); % a1.pax1 connects to where a2.pax1 ends
        \draw[wirestyle] (a1p2_rhs) -- (a2p2_rhs); % a1.pax2 connects to where a2.pax2 ends

        % Epsilon Annihilation (Shifted right)
        \begin{scope}[xshift=7cm]
            \inetzerocell(e1){$\epsilon$}{0,1.8}; % Positioned at origin of scope
            \inetzerocell(e2){$\epsilon$}{$(e1.s)+(0,-1)$}; % Positioned below e1
            \inetshortwire(e1.s)(e2.n); % Connect south of e1 to north of e2

            % Arrow
            \draw[->, thick] ($(e1.east)+(0.5,-0.7)$) -- ++(1,0);
            % RHS is empty (erasure)
        \end{scope}
    \end{tikzpicture}

    \vspace{1cm} % Add vertical space between diagrams

    % Commutation and Erasure Rules
    \begin{tikzpicture}[scale=0.8, transform shape, node distance=0.8cm and 1.5cm, >=Latex, wirestyle/.style={draw, line width=0.15ex}]
        % Zeta ~ Delta Commutation
        \inetcell[arity=2](z1){$\zeta$}{0,0}[U]; 
        \inetcell[arity=2](d1){$\delta$}{$(z1.pal)+(0,1.5)$}[D];
        \inetshortwire(z1.pal)(d1.pal);

        \inetwirefree(z1.pax 1); \node[above left=0.1ex and 0.1ex] at (z1.pax 1) {}; % Placeholder for labels if needed
        \inetwirefree(z1.pax 2); \node[above right=0.1ex and 0.1ex] at (z1.pax 2) {};
        \inetwirefree(d1.pax 1); \node[below left=0.1ex and 0.1ex] at (d1.pax 1) {};
        \inetwirefree(d1.pax 2); \node[below right=0.1ex and 0.1ex] at (d1.pax 2) {};

        % Arrow
        \draw[->, thick] ($(z1.east)+(0.8,1)$) -- ++(1,0);

        % RHS for Zeta ~ Delta
        \inetcell[arity=2](d_tl){$\delta$}{4,0}[D];
        \inetcell[arity=2](d_tr){$\delta$}{6,0}[D];
        \inetcell[arity=2](z_bl){$\zeta$}{$(d_tl.pal)+(0,2.2)$}[U];
        \inetcell[arity=2](z_br){$\zeta$}{$(d_tr.pal)+(0,2.2)$}[U];
        
        % Connect aux ports
        \inetwire(d_tl.pax 2)(z_bl.pax 1); % Straight
        \inetwire(d_tr.pax 1)(z_br.pax 2); % Straight
        \inetwire(d_tl.pax 1)(z_br.pax 1); % Cross
        \inetwire(d_tr.pax 2)(z_bl.pax 2); % Cross

        % Free principal ports for RHS
        \inetwirefree(d_tl.pal);
        \inetwirefree(d_tr.pal);
        \inetwirefree(z_bl.pal);
        \inetwirefree(z_br.pal);

        % Alpha ~ Epsilon Erasure (Shifted further right)
        \begin{scope}[xshift=10cm, yshift=0cm] % Adjust shift as needed
            \inetcell[arity=2](a_erase){$\alpha$}{0,0}[U];
            \inetzerocell(e_erase){$\epsilon$}{$(a_erase.pal)+(0,1.2)$}[D]; % Position below alpha
            \inetshortwire(a_erase.pal)(e_erase.s); % Connect alpha pal to epsilon north

            \inetwirefree(a_erase.pax 1);
            \inetwirefree(a_erase.pax 2);

            % Arrow
            \draw[->, thick] ($(a_erase.east)+(0.8,0.7)$) -- ++(1,0);

            % RHS: Two Epsilons
            \inetzerocell(e_rhs1){$\epsilon$}{3.5, 0.5};
            \inetzerocell(e_rhs2){$\epsilon$}{$(e_rhs1)+(1.2, 0)$};
            \inetwirefree(e_rhs1.n);
            \inetwirefree(e_rhs2.n);
        \end{scope}

    \end{tikzpicture}

    \caption{SIC Reduction Rules: Annihilation, Erasure and Commutation ($\alpha\in\{\delta,\zeta\}$).}
    \label{fig:prep_sic_rules_combined}
\end{figure}

Instead of solely relying on fine-grained combinator interactions for all operations, Parallax adopts a strategy inspired by HVM's handling of function calls \cite{HVMGithub} to manage execution granularity. In HVM, function calls are implemented using \textbf{\texttt{REF}} nodes, which act as pointers to globally stored function definitions (composed of interaction net fragments). When a \texttt{REF} node interacts with its argument(s), the runtime effectively replaces the interaction pair by instantiating a copy of the function body's net structure and connecting the argument ports appropriately. 

Parallax will employ a similar \textbf{reference node} mechanism for invoking user-defined functions. For handling primitive operations (intrinsics) like arithmetic, which are inefficient when represented purely as combinators, a comparable approach will be taken. This will likely involve either using the same reference node type pointing to special intrinsic definitions or introducing a distinct \textbf{intrinsic node} type. In either case, when such a node interacts with its arguments (packaged using standard constructor nodes), the runtime will bypass further graph reduction for that specific operation. Instead, it will invoke highly optimized, pre-compiled native machine code, assuming the necessary input stability is met (the reduction recognition requirement, see Section~\ref{sec:prep_decision_native}). This represents a deliberate choice to trade potential fine-grained parallelism \textit{within} the primitive for vastly superior single-threaded performance via native code execution, while retaining the interaction net model's inherent parallelism for the overall program structure.

While intrinsics addressed arithmetic, efficient conditional logic required a dedicated solution. Therefore, a specialized \textbf{\texttt{Switch}} node was included to provide a direct, performant mechanism for conditional branching (essential for \texttt{if/then/else} and pattern matching), avoiding slow combinator gadgets. A \textbf{\texttt{Number}} node was also included to directly hold primitive numeric values.

Thus, the interaction network system designed during preparation comprised the base symmetric combinators, structural nodes (\textbf{\texttt{Constructor}}, \textbf{\texttt{Duplicator}}), value/control nodes (\textbf{\texttt{Number}}, \textbf{\texttt{Switch}}), an erasure node (\textbf{\texttt{Eraser}}), and a new \textbf{\texttt{Static}} node used for references to function calls, intrinsics and other static data. This design aimed to harness IN parallelism while pragmatically addressing performance bottlenecks through controlled granularity and specialized nodes.

\subsection{Memory Management for Data Structures}\label{sec:prep_decision_mem}

\subsubsection*{Rationale from Research}
The background research phase identified that supporting practical data structures like arrays and strings efficiently necessitates memory management beyond simple node erasure within the interaction net model (Section~\ref{sec:prep_research_features}). Representing variable-length, random-access structures directly via IN copying is prohibitively inefficient. Our proposed solution involves managing the underlying data on a separate heap using \textbf{Garbage Collection (GC)}, representing the structures within the net via lightweight pointers. Research into GC algorithms suitable for potentially concurrent, cyclic graph tracing pointed towards \textbf{Mark-Sweep} principles as more appropriate than \textbf{Reference Counting} due to the anticipated overhead of frequent atomic updates in a highly parallel reduction engine.

Based on these findings, the decision was made that a \textbf{tracing garbage collector}, likely based on mark-sweep principles, was required to efficiently support essential data structures. Implementing a correct and efficient tracing GC suitable for the concurrent graph environment was identified as a significant implementation risk, potentially impacting both functionality (support for complex data structures) and overall runtime performance.

\subsection{Native Code Integration Strategy}\label{sec:prep_decision_native}

\subsubsection*{Rationale from Research}
The decision to use the intrinsic mechanism described in Section~\ref{sec:prep_decision_in} --- invoking native code when a specific reference or intrinsic node interacts with its arguments --- introduced the significant theoretical hurdle of \textbf{reduction recognition}. Interaction net reduction is inherently asynchronous. Before calling a native function (e.g., integer addition), the runtime must ensure that the graph fragments representing the function's inputs have stabilized into the expected canonical forms (e.g., a constructor connected to two \texttt{Number} nodes). Unevaluated graph structures as arguments would lead to an inability to execute the native function.

\subsubsection*{Design Decision: Requirement for Reduction Recognition}
Consistent with the chosen interaction system and intrinsic mechanism, it was decided that the runtime \textit{must} implement a robust mechanism for reduction recognition to ensure arguments are stable before calling native code. Devising an efficient and reliable mechanism compatible with asynchronous parallel reduction was identified as an essential requirement for the viability of the chosen intrinsic execution strategy. The specific implementation details of this mechanism were deferred to the implementation phase, but the \textit{need} for it was a key outcome of the preparation research. The successful implementation of such a mechanism, compatible with the asynchronous parallel runtime, was therefore identified as a key technical risk, essential for the viability of the planned native intrinsic performance strategy.

\subsection{Compiler Architecture}\label{sec:prep_decision_compiler}

To meet the requirement for eventual language server support via incremental compilation (Section~\ref{sec:prep_requirements}), the decision was made to use the \textbf{\texttt{Salsa}} incremental computation framework, inspired by its successful application in \texttt{rust-analyzer} \cite{RustAnalyzer}. Salsa allows the compiler to track dependencies between computation steps (like parsing, type checking, code generation) and only re-execute the necessary steps when source code or configuration changes, significantly speeding up recompilation.

Furthermore, to promote modularity, testability, and maintainability, a \textbf{multi-crate Cargo workspace structure} was chosen. This approach, mirroring best practices observed in large Rust projects like \texttt{rustc} and \texttt{rust-analyzer}, clearly separates the concerns of different compiler stages (e.g., syntax, resolution, type checking, intermediate representations, backends) into distinct libraries (crates). This enhances code organization and allows for independent testing and development of components.

\section{Evaluation Plan}\label{sec:prep_evaluation}

The initial evaluation plan outlined in the project proposal remained largely unchanged during the preparation phase, with a focus on memory efficiency (footprint and churn) and parallel performance. This will involve using standard algorithms like sorting, as well as \textbf{a suite of domain-specific benchmarks} designed to test various aspects of the language and runtime capabilities. 

The primary metrics identified for assessing Parallax against its goals were:
\begin{itemize}
    \item Execution Time (Sequential and Parallel)
    \item Parallel Speedup (Scalability with core count)
    \item Peak Memory Usage
    \item Memory Churn (Allocation rate / GC pressure, if applicable)
\end{itemize}
Crucially, the evaluation will compare Parallax's performance on these benchmarks not only against its own sequential execution but also against existing interaction net implementations (e.g., HVM/Bend) and established industry-standard programming languages (e.g., C, Rust, Haskell) to contextualize its strengths and weaknesses.

However, the selection of specific tools for memory profiling (e.g., \texttt{Valgrind/Massif}, OS-specific tools, Rust libraries like \texttt{dhat}) and the choice of a benchmarking framework were deferred until the implementation phase. This allowed flexibility to choose tools best suited to the specifics of the implemented runtime and compiler.

\section{Software Engineering Plan}\label{sec:prep_software_eng}

A structured approach was adopted for the development process.

\subsection{Methodology}
A hybrid software development model was adopted. Initial architectural planning followed a more structured, upfront design approach akin to the Waterfall model. This included planning the multi-crate workspace structure (Section~\ref{sec:prep_decision_compiler}) and defining core API definitions for inter-crate communication. Once this foundation was laid, development within individual components (compiler passes, runtime features) proceeded in a more iterative, Spiral fashion. This involved cycles of implementation, testing, and refinement based on emerging requirements or challenges encountered during development.

\subsection{Tools and Technologies}
The primary implementation language chosen was \texttt{Rust}. It was selected for its strong safety guarantees, excellent performance potential, rich concurrency features, and mature ecosystem, particularly for compiler development.

\subsection{Project Structure}\label{sec:prep_project_structure}
The decision to use a multi-crate Cargo workspace is detailed in Section~\ref{sec:prep_decision_compiler}. A multi-crate workspace is a feature of Rust's build system, Cargo, that allows multiple individual Rust packages (crates) to be managed together within a single top-level project. This structure enables crates to depend on each other, share a common lockfile for dependencies, and output build artifacts to a shared `target` directory, greatly facilitating the development and maintenance of modular systems like compilers. Version control was managed using \texttt{Git}, following standard practices.

\section{Starting Point}\label{sec:prep_starting_point} % Renumbered
The project codebase was started entirely from scratch, building upon prior academic knowledge and programming experience.

\subsection{Prior Knowledge and Learning Curve}
The project commenced leveraging foundational knowledge gained from undergraduate courses, specifically Part IB Compiler Construction, Part IB Concurrent and Distributed Systems, and Part II Optimising Compilers. Concepts such as parsing techniques (recursive descent, operator precedence), intermediate representations (AST, graph-based IRs), dataflow analysis, and standard optimization algorithms (like Dead Code Elimination and Inlining) were directly applicable during the planning and subsequent implementation phases.

Further significant insights were gained by referencing the architectures of large \texttt{Rust} projects like \texttt{rustc} and particularly \texttt{rust-analyzer}. Studying these provided valuable examples of:
\begin{itemize}
    \item \textbf{Codebase Organisation}: Effective use of multi-crate workspaces and modular design, reinforcing the decision described in Section~\ref{sec:prep_project_structure}.
    \item \textbf{Incremental Computation}: Understanding the practical application and benefits of the \texttt{Salsa} framework, which heavily influenced Parallax's design for efficient recompilation.
    \item \textbf{Language Feature Implementation}: Drawing inspiration for tackling complex features found in Rust and other contemporary languages, such as trait resolution and type inference.
\end{itemize}

The learning curve for Interaction Nets specifically was steep during the preparation phase. They represent a distinct and less mainstream model of computation compared to lambda calculus or Turing machines. Key resources included Lafont's foundational paper \cite{lafont1990interactionnets}, literature on interaction combinators and optimal reduction (e.g., \cite{mazza}), and studying the implementation details and accompanying materials for existing systems like \texttt{HVM} \cite{HVMGithub}. This study helped understand practical runtime construction techniques and challenges.
% TODO: Add citations for HVM paper/source if not already covered by HVMGithub.

\subsection{Applicability of Existing Skills}
Existing experience with \texttt{Rust} and its concurrency features proved directly applicable to implementation as well as reasoning about the parallel runtime design and memory architecture.
