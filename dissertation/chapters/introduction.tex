\chapter{Introduction}

\section{Principal Motivation}

\subsection{The Growing Challenge of Parallel Programming Complexity}
Modern computing hardware is overwhelmingly parallel. Since the plateauing of single-core clock speed improvements around the mid-2000s, performance gains have predominantly arisen from increasing the number of processing cores \cite{Asanovic2006TheLandscape}. Effectively harnessing this parallelism in software, however, remains challenging. Developing correct and efficient parallel programs manually is notoriously difficult, time-consuming, and expensive. Programmers must grapple with subtle concurrency issues like race conditions, deadlocks, and livelocks, significantly increasing the complexity of debugging and maintenance \cite{Lee2006TheProblem}.

\subsection{The Need for Effective Automatic Parallelization}
This complexity creates a notable productivity gap and often leads to the unnecessary underutilization of hardware. Many applications remain sequential simply because the cost, effort, expertise and risk of parallelisation is not worth it. There is, therefore, a strong and persistent need for programming models and tools that can automatically extract parallelism from high-level code, allowing developers to benefit from multi-core architectures without bearing the full burden of explicit concurrency management.

\subsection{Introducing Parallax: An Overview}
This dissertation introduces Parallax, a novel programming language and runtime designed to address this challenge. Parallax explores the viability of interaction networks \cite{lafont1990interactionnets}, a graphical model of computation with special properties well-suited to parallel reduction, as a practical compilation target. The central hypothesis is that by implementing interaction networks with a highly optimized, memory-efficient runtime, we can achieve automatic parallelization with performance competitive with traditional, manually parallelized approaches. Parallax aims to demonstrate a path towards "zero-cost" parallelism, i.e. parallelism that is so abstracted away that it is invisible to the programmer, allowing them to write parallel programs as naturally as sequential ones.

\section{Related Work}

\subsection{Existing Approaches to Automatic Parallelisation}
The challenge of automatically parallelizing sequential code has been approached through various techniques over several decades. Traditional **compiler-based methods** analyze sequential code (particularly loops in languages like Fortran and C/C++) using data dependency analysis to identify independent computations \cite{KennedyAllen2001OptimizingCompilers}. These compilers employ sophisticated loop transformations (e.g., privatization, reduction recognition, interchange) to restructure code and eliminate dependencies, often generating parallel code using explicit directive systems like OpenMP \cite{OpenMPARBP2018OpenMPSpecification}. However, the effectiveness of purely static analysis is often limited by ambiguous pointer aliasing and complex memory access patterns inherent in languages like C++ \cite{Allen1983DependenceAnalysis}.

To overcome static limitations, **speculative parallelization** techniques (like Thread-Level Speculation) optimistically execute code sections in parallel, relying on runtime mechanisms to detect and recover from any dependence violations, albeit at the cost of potential overhead \cite{Rauchwerger1995RunTime}. More formal approaches, such as the **polyhedral model**, provide a powerful algebraic framework for analyzing and transforming loop nests with regular (affine) access patterns, enabling sophisticated optimizations for specific domains like scientific computing \cite{Bondhugula2008AutomaticDistributedMemory}.

Language design also plays a crucial role. Features like Fortran's array operations, the purity of **functional programming languages** (e.g., Haskell) which makes dependencies explicit \cite{Hammond1996ParallelFunctional}, or the integrated parallelism and locality constructs of **Partitioned Global Address Space (PGAS) languages** (e.g., Chapel, X10) \cite{Yelick2007ProductivityParallel} inherently facilitate or simplify the expression and potential for automatic parallelization compared to imperative languages with complex state and aliasing.

More recently, **AI and Machine Learning** approaches have emerged, using models trained on vast codebases to identify parallelization opportunities and generate parallel constructs (e.g., OpenMP pragmas), potentially surpassing the limitations of purely analytical methods in some cases \cite{OMPar}. Additionally, **binary rewriting** offers a path to parallelize legacy executables when source code is unavailable, though challenged by the lack of high-level information \cite{Amaral2006AutomaticBinary}.

\subsection{Current State of Interaction Network Research}
Interaction networks (INs), introduced by Lafont as a graphical model of computation derived from linear logic proof nets \cite{lafont1990interactionnets}, offer deterministic parallelism by construction due to their properties of locality and strong confluence \cite{Fernandez2010InteractionNets}. Computation proceeds via local rewrite rules applied to connected pairs of agents (nodes), guaranteeing that the order of independent reductions does not affect the final result \cite{Mackie1995InteractionNets}.

Theoretical research established the model's foundations, including the development of universal **interaction combinators** (minimal agent sets like {γ, δ, ε}) capable of representing any IN computation \cite{Lafont1995InteractionCombinators}, and explored deep connections to semantic frameworks like Girard's **Geometry of Interaction (GoI)** \cite{Girard1989GeometryInteraction} and **Context Semantics** \cite{Coquand1992ContextSemantics}, which provide alternative views on computation flow and have inspired implementation techniques. Various **type systems** have been developed, evolving from basic safety checks to richer systems aimed at proving termination or analyzing computational complexity \cite{Pinto2010TerminationInteraction, Ghilezan2014SizedTypes}.

Recognizing the determinism of standard INs limits their direct application to concurrency, extensions like **Multiport Interaction Nets (MINs)** were introduced, allowing agents multiple principal ports to model non-deterministic choice and encode process calculi like the π-calculus \cite{Fernandez2006MultiportInteraction}. This expressiveness, however, comes at the cost of the strong confluence property.

Practical implementations often focus on INs as a compilation target, particularly for functional languages aiming for **optimal reduction** of the lambda calculus, a historically significant application \cite{Asperti1999OptimalReduction}. Techniques like **token-passing** evaluation (inspired by GoI) and **in-place rewriting** (to improve memory usage) are key implementation strategies \cite{Mackie2006TokenPassing, vanOostrom2008InPlace}. Higher-level languages like **Pin** \cite{Asperti2002Pin}, and more recently systems like **HVM** \cite{HVMGithub} or **Juvix** \cite{JuvixGithub}, aim to provide more usable programming abstractions over the core IN model. Despite theoretical elegance, a persistent challenge remains achieving **high practical performance** on conventional hardware due to the overhead associated with graph manipulation and memory management \cite{Pinto2014InteractionNetsReview}.

\subsection{Interaction Networks as a Compilation Target}

% TODO: Add a section on interaction networks as a compilation target

\section{Planned Novel Contributions}

% TODO: Add a section on the planned contributions of this dissertation

\section{Dissertation Outline}
This dissertation details the design, implementation, and evaluation of the Parallax system. 
\begin{itemize}
    \item \textbf{Chapter 2 (Preparation):} Describes the refinement of the project goals, requirements analysis, key design decisions for the language and runtime, the theoretical background researched, and the declared starting point for the project.
    \item \textbf{Chapter 3 (Implementation):} Provides a detailed account of the implementation of the Parallax compiler pipeline (frontend, intermediate representations, code generation) and the parallel runtime system (memory management, reduction engine, tooling).
    \item \textbf{Chapter 4 (Evaluation):} Presents the methodology used to evaluate Parallax, the results focusing on memory efficiency and parallel scaling, and an analysis of these results against the project's goals.
    \item \textbf{Chapter 5 (Conclusions):} Summarizes the contributions of this work, reflects on the lessons learned, and suggests directions for future research.
\end{itemize}
Appendices contain supplementary material, followed by the original project proposal. 