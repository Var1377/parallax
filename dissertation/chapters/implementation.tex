\chapter{Implementation}
% This chapter describes the tangible artifacts produced: the Parallax compiler, runtime, and tooling.
% It details the process and design strategies employed, highlighting professional practices.

\section{Overview of Implemented System Components} % 3.1

The Parallax system, developed entirely in Rust, comprises three primary parts: a multi-stage compiler responsible for translating `Parallax` source code into an executable format, a parallel runtime engine designed for efficient Interaction Network reduction, and a command-line interface providing user access to the system's functionality.

\subsection{The Parallax Compiler Pipeline} % 3.1.1
The compiler transforms `Parallax` source code through a sequence of stages, each implemented as a distinct crate within the workspace to promote modularity and maintainability. The pipeline begins with the `parallax-lang` crate, which parses the source text into an Abstract Syntax Tree (AST). This AST then undergoes name resolution and symbol binding in `parallax-resolve`, followed by type checking and inference performed by `parallax-typeck`. 

Subsequently, the type-annotated representation is lowered into a High-level Intermediate Representation (HIR) within the `parallax-hir` crate. This HIR serves as input to the `parallax-mir` crate, which further transforms the code into a Mid-level Intermediate Representation (MIR) suitable for various optimization passes, such as dead code elimination and function inlining. Finally, the `parallax-codegen` crate takes the optimized MIR and generates the target executable representation, primarily focusing on Interaction Networks for the custom runtime, but also incorporating integration with LLVM for generating optimized native code.

\subsection{The \texttt{parallax-net} Runtime} % 3.1.2
The core of the execution environment is the `parallax-net` runtime. This component is responsible for taking the Interaction Network representation generated by the compiler and performing the graph reduction process. Its key responsibilities include managing the memory for the network nodes and connections efficiently, implementing the specific interaction rules for the chosen combinator set, and orchestrating the parallel execution of reduction steps across multiple CPU cores using techniques like work-stealing to ensure load balancing. A significant focus during its implementation was the design of a compact memory layout and efficient garbage collection strategy to address the memory performance goals outlined in Chapter 2.

\subsection{The \texttt{parallax-cli} Tooling} % 3.1.3
User interaction with the compiler and runtime is facilitated by the `parallax-cli` crate. This command-line application serves as the primary interface, providing commands to build `Parallax` source files (invoking the compiler pipeline) and execute the compiled programs (launching the `parallax-net` runtime). It handles configuration, orchestrates the different system components, and presents diagnostic information, such as compilation errors or runtime statistics, to the user.

% Sections 3.2 onwards will detail these components further.

\section{Compiler Frontend Implementation (\texttt{parallax-lang})} % 3.2

The initial stage of compilation, transforming raw source text into a semantically meaningful Abstract Syntax Tree (AST), is handled by the `parallax-lang` crate. This frontend is designed not only to parse correct programs but also to provide robust error handling and diagnostics, laying the groundwork for later analysis stages and potential interactive tooling like a language server.

\subsection{Parsing Strategy: Tree-sitter and AST Generation} % 3.2.1
A key design decision was to leverage the **`tree-sitter`** parsing framework rather than implementing a lexer and parser manually. `tree-sitter` offers several compelling advantages for this project. Firstly, it allows the language grammar to be defined declaratively, automatically generating an efficient and robust parser. Secondly, the generated parser produces a Concrete Syntax Tree (CST) which retains full fidelity with the original source code, including whitespace and comments. Crucially, `tree-sitter` incorporates sophisticated error recovery algorithms, enabling it to produce a usable CST even in the presence of syntax errors. This capability is fundamental for providing good diagnostics and enabling further analysis passes to run despite localized errors.

The `parallax-lang` crate then processes this CST, translating the syntactic structure into the compiler's Abstract Syntax Tree (AST). This translation step focuses on extracting the essential semantic structure relevant for compilation, discarding purely syntactic details already handled by the CST.

\subsection{Abstract Syntax Tree (AST) Design Rationale} % 3.2.2
The AST serves as the central data structure connecting the frontend to subsequent analysis and code generation phases. Its design prioritizes semantic clarity and suitability for further processing. Key principles include using Rust's enums (`ExprKind`, `ItemKind`, etc.) for type-safe representation of different language constructs, employing recursion to naturally model code structure, and embedding source `Span` information within each significant node. This span information, derived from the original CST nodes, is critical for linking analysis results and error messages back to the precise locations in the user's source code.

\subsection{Error Handling, Diagnostics, and Incremental Computation} % 3.2.3
Providing accurate and helpful diagnostics, especially in the context of potentially interactive use (e.g., via a language server), requires more than basic parsing. Parallax integrates the frontend with the **`salsa`** incremental computation framework. This integration leverages `tree-sitter`'s error recovery and `salsa`'s query-based, demand-driven architecture.

When syntax errors occur, `tree-sitter` generates recoverable CSTs containing error markers. The `parallax-lang` CST-to-AST translation process, operating within the `salsa` framework, can identify these markers. Crucially, `salsa` allows subsequent analysis queries (like name resolution or type checking) to proceed where possible, even if parts of the AST are malformed due to upstream parsing errors. Queries depending on erroneous parts will fail gracefully, but the system can still provide analysis for unaffected code sections.

This combination enables **superior error reporting**. Diagnostics can be associated accurately with source locations via spans, and `salsa` helps provide context by tracking dependencies. Furthermore, the incremental nature of `salsa` means that small changes to the source code only trigger re-computation of affected parts of the AST and subsequent analysis, making the frontend highly suitable for responsive **language server implementations** that provide real-time feedback to the developer.

\section{Name Resolution and Binding} % 3.3
Following parsing, the compiler must understand what each identifier (variable name, function name, type name) refers to. This **name resolution** phase connects uses of names to their original definitions, building a semantic understanding that transcends the raw syntax. This is crucial for enabling subsequent analyses like type checking.

\subsection{Strategy: Multi-Pass Resolution with Symbol Table} % 3.3.1
To handle language features like forward references (where a name is used before its definition appears textually), a **multi-pass strategy** is employed. An initial pass traverses the Abstract Syntax Tree (AST) primarily to **collect declarations** of functions, types, variables, modules, etc. These declarations are registered as symbols in a central **`SymbolTable`**. A subsequent pass then revisits the AST, and for each identifier encountered in expressions or type annotations, it queries the `SymbolTable` to find the corresponding definition. This establishes the essential link between the use of a name and its declaration.

\subsection{Handling Scopes, Modules, and Namespaces} % 3.3.2 & 3.3.3
Correctly resolving names requires careful management of scopes and visibility. The `SymbolTable` maintains a **hierarchical scope structure**, mirroring the program's lexical nesting (modules contain functions, functions contain blocks, etc.). When looking up a name, the search starts in the current scope and proceeds outwards to parent scopes. This naturally implements lexical scoping rules.

Furthermore, Parallax supports **modules** as organizational units with distinct visibility rules (public vs. private items). The resolver enforces these rules, preventing access to private items outside their defining module. To allow constructs like structs and functions to share the same name, the resolver implements separate **namespaces** for types, values, and modules. The context of an identifier's use (e.g., in a type annotation vs. a function call) determines which namespace is searched during lookup.

\subsection{Import Management} % 3.3.4
The `use` keyword introduces symbols from other modules. The resolver interprets `use` declarations, including complex paths, aliases, and glob imports (`*`). For each successful import, it registers an `Import` symbol in the current scope, linking the imported name to the original symbol while respecting the source module's visibility rules. This makes external definitions available locally without requiring fully qualified paths everywhere.

\subsection{Integration with Incremental Compilation} % 3.3.5
This resolution process is designed to integrate with the `salsa` incremental computation framework. By structuring resolution as `salsa` queries dependent on the AST, changes to the source code only trigger re-resolution for affected parts of the program. This is vital for efficient recompilation and responsive developer tooling. Errors detected during resolution, such as undefined names or visibility violations, are reported with source locations derived from the AST's span information.

\section{Type Checking and Inference} % 3.4
With names resolved, the next phase verifies that the program is type-safe, ensuring that operations are performed on compatible data types. This **type checking** stage also performs **type inference**, deducing the types of expressions where explicit annotations are omitted.

\subsection{Algorithm: Constraint-Based Inference} % 3.4.1
Parallax uses a **constraint-based type inference** algorithm, similar in principle to Hindley-Milner. The process begins by traversing the resolved, name-aware representation of the program. During this traversal, the system generates **type constraints** based on how values are used. For instance, adding two expressions generates a constraint that both expressions must have the same numeric type. Function arguments must match parameter types, and variable assignments require type compatibility.

Where types are unknown (e.g., a `let` binding without an annotation), the system introduces **type variables**. After traversing the relevant code section (like a function body), a **unification** algorithm attempts to solve the collected constraints. It systematically finds a consistent assignment (a **substitution**) of concrete types or other type variables to the initial type variables. If a consistent substitution satisfying all constraints is found, the types are inferred successfully. If constraints conflict (e.g., trying to equate an `Int` with a `Bool`), a type error is reported. This approach provides both type safety and the flexibility of type inference.

\subsection{Handling Polymorphism with Traits} % 3.4.2
To support generic programming, Parallax utilizes a **trait system**. Traits define interfaces or sets of behaviours (like `Add` or `Display`). Generic functions or types can be defined to operate over any type `T` as long as `T` implements the required traits (specified via **trait bounds**).

During type checking, using a generic function or requiring a trait implementation generates **trait obligations**. A **trait solver** component is responsible for verifying these obligations. It checks if a specific type implements a required trait, consulting both built-in implementations (e.g., `Int` implements `Add`) and user-defined `impl` blocks. This ensures that generic code is only used with types that actually support the required operations.

\subsection{Type Representation and Error Reporting} % 3.4.3 & 3.4.5
Internally, types are represented by an enum (`Ty`) covering variables, concrete types (primitives, structs, enums), function types, tuple types, and generic parameters. An `Error` type variant helps prevent cascading errors.

The type checker collects any inconsistencies found during unification or trait solving as `TypeError` values. These errors are associated with the source code location (via spans inherited from the AST/HIR) and formatted into user-friendly diagnostics, explaining the nature of the type mismatch or unsatisfied trait bound.

\subsection{Output and Incremental Integration} % 3.4.4
The successful completion of this phase yields a fully type-annotated representation of the program, often termed the High-Level Intermediate Representation (HIR). This HIR associates every expression and definition with its verified type. Like other stages, type checking is integrated with `salsa`. Type checking operations are structured as queries, enabling incremental re-checking when source code changes, contributing to efficient compilation cycles.

\section{High-Level Intermediate Representation (HIR)} % 3.5
Following the crucial frontend analyses of name resolution and type checking, the compiler requires a representation that moves beyond the source code's syntactic structure towards a more semantically explicit format. The High-Level Intermediate Representation (HIR) fulfils this role, acting as a vital bridge between the initial parsing stages and the subsequent phases of optimization and code generation.

\subsection{Rationale: Stabilizing Semantics Post-Analysis} % 3.5.1
The primary justification for introducing the HIR is to **stabilize the program representation after frontend analysis**. While the Abstract Syntax Tree (AST) closely mirrors the user's code, the HIR aims to capture the *meaning* derived from resolving names and validating types. It provides a clean, well-defined interface for later compiler stages, abstracting away many details of the original syntax and the complexities of the analysis processes (like type inference). This architectural separation simplifies the design of subsequent phases, as they can operate on a representation where types are explicit and names are unambiguously resolved.

\subsection{Key Characteristics: Explicit Types and Desugaring} % 3.5.2
The defining characteristic of the HIR is its **explicit embedding of type information**. Every expression, definition, and pattern within the HIR is annotated with its precise type, as determined by the preceding type-checking phase. This contrasts sharply with the AST, where type information is often implicit or only partially available.

Furthermore, the transition to HIR presents an opportunity for initial **desugaring**. Complex language features or syntactic sugar present in the source language can be translated into simpler, more fundamental constructs within the HIR. This simplification streamlines the program representation, making it more uniform and easier for subsequent optimization and code generation passes to process effectively.

\subsection{Role in Incremental Compilation} % 3.5.3
Consistent with the compiler's overall design philosophy, the generation of the HIR is integrated into the **incremental compilation framework**. This is a strategic choice aimed at maintaining performance and responsiveness. By treating HIR generation as a step whose results can be cached and selectively invalidated based on source changes, the compiler avoids redundant work. This ensures that the efficiency benefits achieved during parsing and analysis are propagated, minimizing recompilation time and supporting potentially interactive use cases like real-time feedback in development tools. The HIR, therefore, not only serves as a semantic checkpoint but also upholds the performance goals of the incremental architecture.

\section{Mid-Level IR and Optimization (\texttt{parallax-mir})} % 3.6
After the High-Level IR (HIR) stabilizes the program's semantics with explicit type information, the compilation pipeline transitions to the Mid-Level Intermediate Representation (MIR). This stage, managed by the `parallax-mir` crate, is strategically crucial for enabling sophisticated optimizations and preparing the code for effective backend code generation, whether targeting Interaction Networks or LLVM.

\subsection{Rationale: Enabling Control-Flow and Data-Flow Optimizations} % 3.6.1
The primary motivation for introducing the MIR is to represent the program in a form explicitly suited for traditional compiler optimizations. While HIR retains much of the source language's structure, MIR adopts a **Control Flow Graph (CFG)** representation. This structure breaks functions down into **Basic Blocks** â€“ sequences of simple instructions ending in explicit **Terminators** (like jumps, conditional branches, or returns). This explicit representation of control flow is fundamental for a wide range of data-flow analyses and transformations that underpin many optimization techniques. Furthermore, MIR employs simpler, often three-address-code-like **Statements** and explicit **Places** representing memory locations, making data movement and usage more apparent for analysis.

\subsection{Lowering from HIR: Explicit Control and Flattened Expressions} % 3.6.2
The transformation from HIR to MIR involves significant structural changes. A key process is **expression flattening**, where complex, nested expressions from the HIR are broken down into sequences of simpler MIR statements, often introducing temporary variables to hold intermediate results. This makes the data flow explicit. Simultaneously, high-level control structures from the HIR (like `if`, `match`, loops) are lowered into the CFG structure using basic blocks and corresponding terminators (`Switch`, `Goto`). This **control flow structuring** makes all potential execution paths explicit, which is essential for analysis and optimization. Pattern matching is also desugared into sequences of comparisons and branches during this lowering phase.

\subsection{Monomorphization: Concretizing Generics} % 3.6.3
Generic programming, using type parameters and traits, offers powerful abstraction but must eventually be resolved into concrete code for execution. Parallax performs **monomorphization** at the MIR stage. This involves creating specialized versions of generic functions for each unique set of concrete type arguments used in the program. The rationale for performing this at the MIR stage is strategic: it occurs after frontend type checking has validated the generic code's correctness but before backend code generation, which typically requires fully concrete function definitions and type layouts. By monomorphizing at this point, subsequent optimization passes can operate on the specialized, non-generic function bodies, potentially uncovering further optimization opportunities specific to the concrete types involved.

\subsection{Optimization Strategy and Passes} % 3.6.4
The MIR's structure facilitates various optimization passes aimed at improving code efficiency before backend translation. The optimization strategy focuses on applying well-understood compiler optimizations that benefit from the explicit CFG and simplified statement structure. Key passes implemented include:
\begin{itemize}
    \item \textbf{Constant Folding and Propagation:} Evaluating expressions with known constant inputs at compile time and propagating these constant values through the code, reducing runtime computation.
    \item \textbf{Dead Code Elimination (DCE):} Identifying and removing code (basic blocks, statements, assignments) that is unreachable or whose results are never used. This cleans up the code generated by earlier stages and reduces final code size and execution time.
\end{itemize}
These passes are chosen because they are effective at cleaning up inefficiencies introduced during lowering and abstraction, and they lay the groundwork for more advanced backend optimizations. The specific passes enabled can be controlled via optimization level settings.

\subsection{Incremental Integration} % 3.6.6 (Reflecting db.rs)
As with previous stages, the generation and optimization of MIR are integrated with the `salsa` framework. Lowering, monomorphization requests, and optimization passes are structured as queries. This maintains the benefits of incremental compilation, ensuring that changes typically only require re-processing and re-optimizing the affected functions, rather than the entire program, leading to faster subsequent builds.

\section{Code Generation (\texttt{parallax-codegen})} % 3.7
% Content for this section will follow.

\section{Runtime Implementation (\texttt{parallax-net})} % 3.8
The execution of Interaction Networks generated by the compiler is managed by the `parallax-net` runtime. This component is central to the project's goal of achieving efficient parallel reduction, particularly concerning memory usage. Its design focuses on managing the dynamic graph structure, orchestrating parallel computation, and implementing the core interaction rules.

\subsection{Core Runtime Architecture: Workers and Partitions} % 3.8.1
The runtime employs a multi-threaded architecture designed for scalable parallelism on multi-core processors. The central `Runtime` structure orchestrates a pool of `Worker` threads. To manage the interaction network graph and distribute work effectively, the network is logically divided into `Partition`s.

The **partitioning strategy** is key to parallelism and memory management. Each `Partition` conceptually owns a subset of the network nodes. A `Worker` thread takes exclusive ownership of one or more `Partition`s at any given time. This ownership model is crucial: it allows a worker to freely manipulate the nodes within its owned partitions without requiring locks for most operations, significantly reducing synchronization overhead. Interaction (reduction) primarily occurs *within* a partition, but interactions spanning partitions necessitate careful coordination or node migration.

The `Worker` threads actively seek work. When a worker exhausts the reducible pairs (redexes) within its owned partitions, it enters a **work-stealing** phase. It attempts to acquire ownership of partitions currently held by other, potentially busy, workers. This dynamic load balancing mechanism aims to keep all processor cores utilized effectively. Communication regarding ownership requests and transfers is handled through concurrent queues associated with each `Partition` and `Worker`.

\subsection{Interaction Network Representation} % 3.8.1.1 & 3.8.1.2 (Node/Port focus)
Representing the interaction network efficiently is critical for performance. Nodes (`Constructor`, `Duplicator`, `Number`, etc.) are stored within their owning `Partition`. To optimize memory allocation and improve cache locality, each `Partition` uses `Slab` allocators for its node storage. Slabs provide efficient O(1) allocation and deallocation and tend to keep related nodes closer in memory.

Connections between nodes are represented by `Port` identifiers. A `Port` is a compact 64-bit value encoding not only the target node's index and type but also the specific port on that node (e.g., principal, auxiliary left/right) and the ID of the owning `Partition`. This encoding allows for fast redirection of connections during reduction steps and efficient checking of whether an interaction partner is local (within the same partition) or remote.

\subsection{Memory Management: Allocation and Garbage Collection} % 3.8.2
Minimizing memory footprint and churn is a primary goal. The use of `Slab` allocators helps reduce fragmentation and allocation overhead compared to frequent heap allocations.

Garbage collection is essential in graph reduction, as reduction steps consume nodes and create new ones. `parallax-net` employs a strategy based on **reference counting or a similar mechanism** (details might involve tracking active ports or using erase queues). When a node becomes unreachable (no active ports point to it), it is marked for deletion. The `erase_queue` within each `Partition` likely plays a role in deferring the actual memory reclamation, potentially processing erasures in batches to improve efficiency or handle cross-partition references safely. The design aims for efficient reclamation without requiring global stop-the-world pauses, fitting the parallel execution model.

\subsection{Parallel Reduction Engine and Interaction Rules} % 3.8.3 & 3.8.4
Each `Worker` thread executes a reduction loop. It dequeues reducible pairs (redexes) associated with its owned partitions. A `Redex` simply identifies two ports that are connected and represent an active interaction site.

The core reduction logic implements the specific interaction rules defined for the chosen Interaction Network combinator set (e.g., symmetric interaction combinators, potentially extended with specific rules for numbers or other agents). When a redex is processed, the `Worker` applies the corresponding rule:
\begin{enumerate}
    \item It potentially annihilates the two interacting nodes.
    \item It potentially creates new nodes.
    \item It rewires the connections according to the rule, connecting auxiliary ports of the interacting nodes to each other or to newly created nodes.
\end{enumerate}
If rewiring involves a port belonging to a node in a *different* partition, coordination mechanisms (potentially involving message passing or migrating the node) are required. Detecting and handling these cross-partition interactions efficiently is a key challenge. The implementation prioritizes fast paths for local interactions.

% Subsection 3.8.5 (Performance Optimizations) could be added if specific techniques like lock-free algorithms were detailed.
% Subsection 3.8.6 (Integration with Host Environment) could cover FFI/IO if implemented.

\section{Compiler Architecture}
% Discuss the architecture of the compiler
% - Lexer and parser
% - Type checker
% - Code generator
% - Optimizations

\subsection{Lexer and Parser}

\subsection{Type Checker}

\subsection{Code Generator}

\subsection{Optimizations}

\section{Runtime System}
% Discuss the runtime system
% - Memory management
% - Concurrency model
% - Scheduling
% - Error handling

\subsection{Memory Management}

\subsection{Concurrency Model}

\subsection{Scheduling}

\subsection{Error Handling}

\section{Interaction Network Implementation}
% Discuss the implementation of the interaction network
% - Network representation
% - Reduction strategies
% - Optimizations

\subsection{Network Representation}

\subsection{Reduction Strategies}

\subsection{Optimizations}

\section{Challenges and Solutions}
% Discuss the challenges faced during implementation and the solutions
% - Technical challenges
% - Design challenges
% - Performance challenges

\subsection{Technical Challenges}

\subsection{Design Challenges}

\subsection{Performance Challenges} 